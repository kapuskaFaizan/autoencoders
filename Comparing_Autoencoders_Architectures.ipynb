{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data-fashion/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data-fashion/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data-fashion/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data-fashion/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# magic output of plotting commands is displayed inline\n",
    "%matplotlib inline\n",
    "\n",
    "# import packages, framework and libraries \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import Zalando's MNIST-alike dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data-fashion/fashion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fed6cbd0198>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/R8FIR0dOvRG0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fed6f7fbda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(data.train.images))\n",
    "\n",
    "img = data.train.images[1]\n",
    "plt.imshow(img.reshape((28, 28)), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define placeholders, no size for batch defined so None, and images are MNIST alike 28x28 greyscale\n",
    "inputs = tf.placeholder(tf.float32, (None, 28,28, 1), name='inputs')\n",
    "targets = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "# Encoder (from 28x28x1 --> 4x4x8), the compressed represetation is going to be a pooling layer\n",
    "# we don't want to potentially loose information padding --> same\n",
    "# since we want a reduced representation of the image, we try a bigger \n",
    "# filter (feature map) initally and we gradually reduce it at each convolution\n",
    "\n",
    "### Encoder\n",
    "# Conv 1 + Maxpool\n",
    "conv1 = tf.layers.conv2d(inputs, 64, (2,2), padding='same', activation=tf.nn.relu) # 28x28x64\n",
    "# tuples for maxpool layer: pool_size and strides\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same') # 14x14x64\n",
    "# Conv 2 + Maxpool\n",
    "conv2 = tf.layers.conv2d(maxpool1, 32, (2,2), padding='same', activation=tf.nn.relu) # 14x14x32\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same') # 7x7x32\n",
    "# Conv 3 + Maxpool \n",
    "conv3 = tf.layers.conv2d(maxpool2, 16, (1,1), padding='same', activation=tf.nn.relu) # 7x7x16\n",
    "maxpool3 = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same') # 4x4x16\n",
    "# Conv 4 + Maxpool\n",
    "conv4 = tf.layers.conv2d(maxpool3, 8, (1,1), padding='same', activation=tf.nn.relu) # 4x4x8\n",
    "maxpool4 = tf.layers.max_pooling2d(conv4, (2,2), (2,2), padding='same') # 2x2x8\n",
    "# Conv\n",
    "conv5 = tf.layers.conv2d(maxpool4, 4, (1,1), padding='same', activation=tf.nn.relu) # 2x2x4\n",
    "encoded = tf.layers.max_pooling2d(conv5, (2,2), (1,1), padding='same') # 2x2x4\n",
    "print(encoded.shape) # (?, 2, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternative encoder \n",
    "\n",
    "from custom_convolution_model import *\n",
    "\n",
    "tf.reset_default_graph()\n",
    "logits = encoding_conv_net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 64)\n"
     ]
    }
   ],
   "source": [
    "### Decoder\n",
    "\n",
    "# Resize images to size using nearest neighbor interpolation\n",
    "upsample1 = tf.image.resize_nearest_neighbor(logits, (4,4)) # 4x4x4 \n",
    "deconv1 = tf.layers.conv2d(upsample1, 8, (2,2), padding='same', activation=tf.nn.relu) # 4x4x8\n",
    "upsample2 = tf.image.resize_nearest_neighbor(deconv1, (7,7)) # 7x7x8 \n",
    "deconv2 = tf.layers.conv2d(upsample2, 16, (2,2), padding='same', activation=tf.nn.relu) # 7x7x16\n",
    "upsample3 = tf.image.resize_nearest_neighbor(deconv2, (14,14)) # 14x14x8\n",
    "deconv3 = tf.layers.conv2d(upsample3, 32, (2,2), padding='same', activation=tf.nn.relu) # 14x14x32\n",
    "upsample4 = tf.image.resize_nearest_neighbor(deconv3, (28,28)) # 28x28x8\n",
    "deconv4 = tf.layers.conv2d(upsample4, 64, (3,3), padding='same', activation=tf.nn.relu) # 28x28x64\n",
    "print(deconv4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "lr = 0.001\n",
    " \n",
    "# output layer 28x28x1\n",
    "output = tf.layers.conv2d(deconv4, 1, (2,2), padding='same', activation=None) #logits \n",
    "decoded = tf.nn.sigmoid(output, name='decoded')\n",
    "\n",
    "# compute the distance output vs target\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=output)\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Training loss: 0.6932\n",
      "Epoch: 1/20... Training loss: 0.6919\n",
      "Epoch: 1/20... Training loss: 0.6904\n",
      "Epoch: 1/20... Training loss: 0.6891\n",
      "Epoch: 1/20... Training loss: 0.6873\n",
      "Epoch: 1/20... Training loss: 0.6851\n",
      "Epoch: 1/20... Training loss: 0.6817\n",
      "Epoch: 1/20... Training loss: 0.6799\n",
      "Epoch: 1/20... Training loss: 0.6769\n",
      "Epoch: 1/20... Training loss: 0.6725\n",
      "Epoch: 1/20... Training loss: 0.6679\n",
      "Epoch: 1/20... Training loss: 0.6662\n",
      "Epoch: 1/20... Training loss: 0.6622\n",
      "Epoch: 1/20... Training loss: 0.6517\n",
      "Epoch: 1/20... Training loss: 0.6453\n",
      "Epoch: 1/20... Training loss: 0.6390\n",
      "Epoch: 1/20... Training loss: 0.6310\n",
      "Epoch: 1/20... Training loss: 0.6370\n",
      "Epoch: 1/20... Training loss: 0.6275\n",
      "Epoch: 1/20... Training loss: 0.6311\n",
      "Epoch: 1/20... Training loss: 0.6377\n",
      "Epoch: 1/20... Training loss: 0.6372\n",
      "Epoch: 1/20... Training loss: 0.6216\n",
      "Epoch: 1/20... Training loss: 0.6210\n",
      "Epoch: 1/20... Training loss: 0.6301\n",
      "Epoch: 1/20... Training loss: 0.6209\n",
      "Epoch: 1/20... Training loss: 0.6095\n",
      "Epoch: 1/20... Training loss: 0.6176\n",
      "Epoch: 1/20... Training loss: 0.6203\n",
      "Epoch: 1/20... Training loss: 0.6124\n",
      "Epoch: 1/20... Training loss: 0.6150\n",
      "Epoch: 1/20... Training loss: 0.6095\n",
      "Epoch: 1/20... Training loss: 0.6012\n",
      "Epoch: 1/20... Training loss: 0.6093\n",
      "Epoch: 1/20... Training loss: 0.6058\n",
      "Epoch: 1/20... Training loss: 0.6091\n",
      "Epoch: 1/20... Training loss: 0.6102\n",
      "Epoch: 1/20... Training loss: 0.6113\n",
      "Epoch: 1/20... Training loss: 0.6101\n",
      "Epoch: 1/20... Training loss: 0.6060\n",
      "Epoch: 1/20... Training loss: 0.6018\n",
      "Epoch: 1/20... Training loss: 0.6049\n",
      "Epoch: 1/20... Training loss: 0.6079\n",
      "Epoch: 1/20... Training loss: 0.6015\n",
      "Epoch: 1/20... Training loss: 0.5979\n",
      "Epoch: 1/20... Training loss: 0.5996\n",
      "Epoch: 1/20... Training loss: 0.5888\n",
      "Epoch: 1/20... Training loss: 0.6052\n",
      "Epoch: 1/20... Training loss: 0.5979\n",
      "Epoch: 1/20... Training loss: 0.5890\n",
      "Epoch: 1/20... Training loss: 0.5840\n",
      "Epoch: 1/20... Training loss: 0.5860\n",
      "Epoch: 1/20... Training loss: 0.5915\n",
      "Epoch: 1/20... Training loss: 0.5844\n",
      "Epoch: 1/20... Training loss: 0.5844\n",
      "Epoch: 1/20... Training loss: 0.5853\n",
      "Epoch: 1/20... Training loss: 0.5825\n",
      "Epoch: 1/20... Training loss: 0.5830\n",
      "Epoch: 1/20... Training loss: 0.5788\n",
      "Epoch: 1/20... Training loss: 0.5827\n",
      "Epoch: 1/20... Training loss: 0.5787\n",
      "Epoch: 1/20... Training loss: 0.5734\n",
      "Epoch: 1/20... Training loss: 0.5727\n",
      "Epoch: 1/20... Training loss: 0.5715\n",
      "Epoch: 1/20... Training loss: 0.5648\n",
      "Epoch: 1/20... Training loss: 0.5735\n",
      "Epoch: 1/20... Training loss: 0.5613\n",
      "Epoch: 1/20... Training loss: 0.5507\n",
      "Epoch: 1/20... Training loss: 0.5641\n",
      "Epoch: 1/20... Training loss: 0.5593\n",
      "Epoch: 1/20... Training loss: 0.5478\n",
      "Epoch: 1/20... Training loss: 0.5353\n",
      "Epoch: 1/20... Training loss: 0.5421\n",
      "Epoch: 1/20... Training loss: 0.5497\n",
      "Epoch: 1/20... Training loss: 0.5446\n",
      "Epoch: 1/20... Training loss: 0.5509\n",
      "Epoch: 1/20... Training loss: 0.5361\n",
      "Epoch: 1/20... Training loss: 0.5339\n",
      "Epoch: 1/20... Training loss: 0.5400\n",
      "Epoch: 1/20... Training loss: 0.5384\n",
      "Epoch: 1/20... Training loss: 0.5341\n",
      "Epoch: 1/20... Training loss: 0.5445\n",
      "Epoch: 1/20... Training loss: 0.5370\n",
      "Epoch: 1/20... Training loss: 0.5324\n",
      "Epoch: 1/20... Training loss: 0.5299\n",
      "Epoch: 1/20... Training loss: 0.5280\n",
      "Epoch: 1/20... Training loss: 0.5355\n",
      "Epoch: 1/20... Training loss: 0.5440\n",
      "Epoch: 1/20... Training loss: 0.5430\n",
      "Epoch: 1/20... Training loss: 0.5301\n",
      "Epoch: 1/20... Training loss: 0.5397\n",
      "Epoch: 1/20... Training loss: 0.5299\n",
      "Epoch: 1/20... Training loss: 0.5176\n",
      "Epoch: 1/20... Training loss: 0.5173\n",
      "Epoch: 1/20... Training loss: 0.5252\n",
      "Epoch: 1/20... Training loss: 0.5374\n",
      "Epoch: 1/20... Training loss: 0.5226\n",
      "Epoch: 1/20... Training loss: 0.5247\n",
      "Epoch: 1/20... Training loss: 0.5219\n",
      "Epoch: 1/20... Training loss: 0.5264\n",
      "Epoch: 1/20... Training loss: 0.5217\n",
      "Epoch: 1/20... Training loss: 0.5181\n",
      "Epoch: 1/20... Training loss: 0.5235\n",
      "Epoch: 1/20... Training loss: 0.5153\n",
      "Epoch: 1/20... Training loss: 0.5223\n",
      "Epoch: 1/20... Training loss: 0.5216\n",
      "Epoch: 1/20... Training loss: 0.5075\n",
      "Epoch: 1/20... Training loss: 0.5087\n",
      "Epoch: 1/20... Training loss: 0.5198\n",
      "Epoch: 1/20... Training loss: 0.5157\n",
      "Epoch: 1/20... Training loss: 0.5086\n",
      "Epoch: 1/20... Training loss: 0.5184\n",
      "Epoch: 1/20... Training loss: 0.5139\n",
      "Epoch: 1/20... Training loss: 0.5138\n",
      "Epoch: 1/20... Training loss: 0.5123\n",
      "Epoch: 1/20... Training loss: 0.5145\n",
      "Epoch: 1/20... Training loss: 0.5103\n",
      "Epoch: 1/20... Training loss: 0.5041\n",
      "Epoch: 1/20... Training loss: 0.5083\n",
      "Epoch: 1/20... Training loss: 0.5139\n",
      "Epoch: 1/20... Training loss: 0.5124\n",
      "Epoch: 1/20... Training loss: 0.5092\n",
      "Epoch: 1/20... Training loss: 0.5090\n",
      "Epoch: 1/20... Training loss: 0.5052\n",
      "Epoch: 1/20... Training loss: 0.5048\n",
      "Epoch: 1/20... Training loss: 0.5086\n",
      "Epoch: 1/20... Training loss: 0.5030\n",
      "Epoch: 1/20... Training loss: 0.5017\n",
      "Epoch: 1/20... Training loss: 0.5051\n",
      "Epoch: 1/20... Training loss: 0.4918\n",
      "Epoch: 1/20... Training loss: 0.5006\n",
      "Epoch: 1/20... Training loss: 0.5064\n",
      "Epoch: 1/20... Training loss: 0.5058\n",
      "Epoch: 1/20... Training loss: 0.5062\n",
      "Epoch: 1/20... Training loss: 0.5077\n",
      "Epoch: 1/20... Training loss: 0.5075\n",
      "Epoch: 1/20... Training loss: 0.4984\n",
      "Epoch: 1/20... Training loss: 0.5023\n",
      "Epoch: 1/20... Training loss: 0.5007\n",
      "Epoch: 1/20... Training loss: 0.4966\n",
      "Epoch: 1/20... Training loss: 0.4953\n",
      "Epoch: 1/20... Training loss: 0.4997\n",
      "Epoch: 1/20... Training loss: 0.4997\n",
      "Epoch: 1/20... Training loss: 0.4970\n",
      "Epoch: 1/20... Training loss: 0.5005\n",
      "Epoch: 1/20... Training loss: 0.4978\n",
      "Epoch: 1/20... Training loss: 0.4980\n",
      "Epoch: 1/20... Training loss: 0.4984\n",
      "Epoch: 1/20... Training loss: 0.4918\n",
      "Epoch: 1/20... Training loss: 0.5021\n",
      "Epoch: 1/20... Training loss: 0.4933\n",
      "Epoch: 1/20... Training loss: 0.4906\n",
      "Epoch: 1/20... Training loss: 0.5001\n",
      "Epoch: 1/20... Training loss: 0.4921\n",
      "Epoch: 1/20... Training loss: 0.5029\n",
      "Epoch: 1/20... Training loss: 0.4861\n",
      "Epoch: 1/20... Training loss: 0.4955\n",
      "Epoch: 1/20... Training loss: 0.4963\n",
      "Epoch: 1/20... Training loss: 0.4916\n",
      "Epoch: 1/20... Training loss: 0.4837\n",
      "Epoch: 1/20... Training loss: 0.4892\n",
      "Epoch: 1/20... Training loss: 0.4955\n",
      "Epoch: 1/20... Training loss: 0.4974\n",
      "Epoch: 1/20... Training loss: 0.4942\n",
      "Epoch: 1/20... Training loss: 0.4953\n",
      "Epoch: 1/20... Training loss: 0.4833\n",
      "Epoch: 1/20... Training loss: 0.4906\n",
      "Epoch: 1/20... Training loss: 0.4849\n",
      "Epoch: 1/20... Training loss: 0.4867\n",
      "Epoch: 1/20... Training loss: 0.4828\n",
      "Epoch: 1/20... Training loss: 0.4871\n",
      "Epoch: 1/20... Training loss: 0.4796\n",
      "Epoch: 1/20... Training loss: 0.4840\n",
      "Epoch: 1/20... Training loss: 0.4919\n",
      "Epoch: 1/20... Training loss: 0.4872\n",
      "Epoch: 1/20... Training loss: 0.4693\n",
      "Epoch: 1/20... Training loss: 0.4814\n",
      "Epoch: 1/20... Training loss: 0.4757\n",
      "Epoch: 1/20... Training loss: 0.4820\n",
      "Epoch: 1/20... Training loss: 0.4814\n",
      "Epoch: 1/20... Training loss: 0.4896\n",
      "Epoch: 1/20... Training loss: 0.4734\n",
      "Epoch: 1/20... Training loss: 0.4845\n",
      "Epoch: 2/20... Training loss: 0.4805\n",
      "Epoch: 2/20... Training loss: 0.4782\n",
      "Epoch: 2/20... Training loss: 0.4674\n",
      "Epoch: 2/20... Training loss: 0.4842\n",
      "Epoch: 2/20... Training loss: 0.4791\n",
      "Epoch: 2/20... Training loss: 0.4738\n",
      "Epoch: 2/20... Training loss: 0.4697\n",
      "Epoch: 2/20... Training loss: 0.4807\n",
      "Epoch: 2/20... Training loss: 0.4775\n",
      "Epoch: 2/20... Training loss: 0.4711\n",
      "Epoch: 2/20... Training loss: 0.4720\n",
      "Epoch: 2/20... Training loss: 0.4683\n",
      "Epoch: 2/20... Training loss: 0.4650\n",
      "Epoch: 2/20... Training loss: 0.4694\n",
      "Epoch: 2/20... Training loss: 0.4695\n",
      "Epoch: 2/20... Training loss: 0.4752\n",
      "Epoch: 2/20... Training loss: 0.4807\n",
      "Epoch: 2/20... Training loss: 0.4687\n",
      "Epoch: 2/20... Training loss: 0.4622\n",
      "Epoch: 2/20... Training loss: 0.4740\n",
      "Epoch: 2/20... Training loss: 0.4830\n",
      "Epoch: 2/20... Training loss: 0.4804\n",
      "Epoch: 2/20... Training loss: 0.4752\n",
      "Epoch: 2/20... Training loss: 0.4716\n",
      "Epoch: 2/20... Training loss: 0.4660\n",
      "Epoch: 2/20... Training loss: 0.4681\n",
      "Epoch: 2/20... Training loss: 0.4765\n",
      "Epoch: 2/20... Training loss: 0.4764\n",
      "Epoch: 2/20... Training loss: 0.4794\n",
      "Epoch: 2/20... Training loss: 0.4735\n",
      "Epoch: 2/20... Training loss: 0.4808\n",
      "Epoch: 2/20... Training loss: 0.4680\n",
      "Epoch: 2/20... Training loss: 0.4759\n",
      "Epoch: 2/20... Training loss: 0.4763\n",
      "Epoch: 2/20... Training loss: 0.4690\n",
      "Epoch: 2/20... Training loss: 0.4657\n",
      "Epoch: 2/20... Training loss: 0.4676\n",
      "Epoch: 2/20... Training loss: 0.4683\n",
      "Epoch: 2/20... Training loss: 0.4696\n",
      "Epoch: 2/20... Training loss: 0.4707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20... Training loss: 0.4602\n",
      "Epoch: 2/20... Training loss: 0.4643\n",
      "Epoch: 2/20... Training loss: 0.4668\n",
      "Epoch: 2/20... Training loss: 0.4612\n",
      "Epoch: 2/20... Training loss: 0.4644\n",
      "Epoch: 2/20... Training loss: 0.4735\n",
      "Epoch: 2/20... Training loss: 0.4721\n",
      "Epoch: 2/20... Training loss: 0.4639\n",
      "Epoch: 2/20... Training loss: 0.4639\n",
      "Epoch: 2/20... Training loss: 0.4633\n",
      "Epoch: 2/20... Training loss: 0.4743\n",
      "Epoch: 2/20... Training loss: 0.4753\n",
      "Epoch: 2/20... Training loss: 0.4705\n",
      "Epoch: 2/20... Training loss: 0.4553\n",
      "Epoch: 2/20... Training loss: 0.4734\n",
      "Epoch: 2/20... Training loss: 0.4743\n",
      "Epoch: 2/20... Training loss: 0.4780\n",
      "Epoch: 2/20... Training loss: 0.4640\n",
      "Epoch: 2/20... Training loss: 0.4625\n",
      "Epoch: 2/20... Training loss: 0.4652\n",
      "Epoch: 2/20... Training loss: 0.4729\n",
      "Epoch: 2/20... Training loss: 0.4627\n",
      "Epoch: 2/20... Training loss: 0.4768\n",
      "Epoch: 2/20... Training loss: 0.4721\n",
      "Epoch: 2/20... Training loss: 0.4679\n",
      "Epoch: 2/20... Training loss: 0.4675\n",
      "Epoch: 2/20... Training loss: 0.4760\n",
      "Epoch: 2/20... Training loss: 0.4672\n",
      "Epoch: 2/20... Training loss: 0.4690\n",
      "Epoch: 2/20... Training loss: 0.4709\n",
      "Epoch: 2/20... Training loss: 0.4686\n",
      "Epoch: 2/20... Training loss: 0.4621\n",
      "Epoch: 2/20... Training loss: 0.4775\n",
      "Epoch: 2/20... Training loss: 0.4602\n",
      "Epoch: 2/20... Training loss: 0.4676\n",
      "Epoch: 2/20... Training loss: 0.4727\n",
      "Epoch: 2/20... Training loss: 0.4681\n",
      "Epoch: 2/20... Training loss: 0.4718\n",
      "Epoch: 2/20... Training loss: 0.4665\n",
      "Epoch: 2/20... Training loss: 0.4644\n",
      "Epoch: 2/20... Training loss: 0.4688\n",
      "Epoch: 2/20... Training loss: 0.4605\n",
      "Epoch: 2/20... Training loss: 0.4614\n",
      "Epoch: 2/20... Training loss: 0.4559\n",
      "Epoch: 2/20... Training loss: 0.4725\n",
      "Epoch: 2/20... Training loss: 0.4657\n",
      "Epoch: 2/20... Training loss: 0.4655\n",
      "Epoch: 2/20... Training loss: 0.4623\n",
      "Epoch: 2/20... Training loss: 0.4701\n",
      "Epoch: 2/20... Training loss: 0.4595\n",
      "Epoch: 2/20... Training loss: 0.4680\n",
      "Epoch: 2/20... Training loss: 0.4671\n",
      "Epoch: 2/20... Training loss: 0.4601\n",
      "Epoch: 2/20... Training loss: 0.4699\n",
      "Epoch: 2/20... Training loss: 0.4637\n",
      "Epoch: 2/20... Training loss: 0.4707\n",
      "Epoch: 2/20... Training loss: 0.4588\n",
      "Epoch: 2/20... Training loss: 0.4722\n",
      "Epoch: 2/20... Training loss: 0.4651\n",
      "Epoch: 2/20... Training loss: 0.4706\n",
      "Epoch: 2/20... Training loss: 0.4570\n",
      "Epoch: 2/20... Training loss: 0.4644\n",
      "Epoch: 2/20... Training loss: 0.4617\n",
      "Epoch: 2/20... Training loss: 0.4652\n",
      "Epoch: 2/20... Training loss: 0.4596\n",
      "Epoch: 2/20... Training loss: 0.4724\n",
      "Epoch: 2/20... Training loss: 0.4770\n",
      "Epoch: 2/20... Training loss: 0.4785\n",
      "Epoch: 2/20... Training loss: 0.4550\n",
      "Epoch: 2/20... Training loss: 0.4611\n",
      "Epoch: 2/20... Training loss: 0.4546\n",
      "Epoch: 2/20... Training loss: 0.4727\n",
      "Epoch: 2/20... Training loss: 0.4683\n",
      "Epoch: 2/20... Training loss: 0.4673\n",
      "Epoch: 2/20... Training loss: 0.4715\n",
      "Epoch: 2/20... Training loss: 0.4551\n",
      "Epoch: 2/20... Training loss: 0.4632\n",
      "Epoch: 2/20... Training loss: 0.4512\n",
      "Epoch: 2/20... Training loss: 0.4619\n",
      "Epoch: 2/20... Training loss: 0.4794\n",
      "Epoch: 2/20... Training loss: 0.4625\n",
      "Epoch: 2/20... Training loss: 0.4706\n",
      "Epoch: 2/20... Training loss: 0.4607\n",
      "Epoch: 2/20... Training loss: 0.4676\n",
      "Epoch: 2/20... Training loss: 0.4648\n",
      "Epoch: 2/20... Training loss: 0.4658\n",
      "Epoch: 2/20... Training loss: 0.4707\n",
      "Epoch: 2/20... Training loss: 0.4468\n",
      "Epoch: 2/20... Training loss: 0.4656\n",
      "Epoch: 2/20... Training loss: 0.4682\n",
      "Epoch: 2/20... Training loss: 0.4734\n",
      "Epoch: 2/20... Training loss: 0.4690\n",
      "Epoch: 2/20... Training loss: 0.4581\n",
      "Epoch: 2/20... Training loss: 0.4602\n",
      "Epoch: 2/20... Training loss: 0.4611\n",
      "Epoch: 2/20... Training loss: 0.4585\n",
      "Epoch: 2/20... Training loss: 0.4574\n",
      "Epoch: 2/20... Training loss: 0.4672\n",
      "Epoch: 2/20... Training loss: 0.4596\n",
      "Epoch: 2/20... Training loss: 0.4692\n",
      "Epoch: 2/20... Training loss: 0.4683\n",
      "Epoch: 2/20... Training loss: 0.4756\n",
      "Epoch: 2/20... Training loss: 0.4702\n",
      "Epoch: 2/20... Training loss: 0.4809\n",
      "Epoch: 2/20... Training loss: 0.4703\n",
      "Epoch: 2/20... Training loss: 0.4708\n",
      "Epoch: 2/20... Training loss: 0.4700\n",
      "Epoch: 2/20... Training loss: 0.4657\n",
      "Epoch: 2/20... Training loss: 0.4632\n",
      "Epoch: 2/20... Training loss: 0.4682\n",
      "Epoch: 2/20... Training loss: 0.4562\n",
      "Epoch: 2/20... Training loss: 0.4659\n",
      "Epoch: 2/20... Training loss: 0.4606\n",
      "Epoch: 2/20... Training loss: 0.4684\n",
      "Epoch: 2/20... Training loss: 0.4641\n",
      "Epoch: 2/20... Training loss: 0.4640\n",
      "Epoch: 2/20... Training loss: 0.4634\n",
      "Epoch: 2/20... Training loss: 0.4645\n",
      "Epoch: 2/20... Training loss: 0.4566\n",
      "Epoch: 2/20... Training loss: 0.4552\n",
      "Epoch: 2/20... Training loss: 0.4615\n",
      "Epoch: 2/20... Training loss: 0.4567\n",
      "Epoch: 2/20... Training loss: 0.4589\n",
      "Epoch: 2/20... Training loss: 0.4538\n",
      "Epoch: 2/20... Training loss: 0.4621\n",
      "Epoch: 2/20... Training loss: 0.4550\n",
      "Epoch: 2/20... Training loss: 0.4658\n",
      "Epoch: 2/20... Training loss: 0.4733\n",
      "Epoch: 2/20... Training loss: 0.4557\n",
      "Epoch: 2/20... Training loss: 0.4560\n",
      "Epoch: 2/20... Training loss: 0.4693\n",
      "Epoch: 2/20... Training loss: 0.4662\n",
      "Epoch: 2/20... Training loss: 0.4741\n",
      "Epoch: 2/20... Training loss: 0.4519\n",
      "Epoch: 2/20... Training loss: 0.4568\n",
      "Epoch: 2/20... Training loss: 0.4678\n",
      "Epoch: 2/20... Training loss: 0.4512\n",
      "Epoch: 2/20... Training loss: 0.4601\n",
      "Epoch: 2/20... Training loss: 0.4547\n",
      "Epoch: 2/20... Training loss: 0.4614\n",
      "Epoch: 2/20... Training loss: 0.4561\n",
      "Epoch: 2/20... Training loss: 0.4592\n",
      "Epoch: 2/20... Training loss: 0.4593\n",
      "Epoch: 3/20... Training loss: 0.4624\n",
      "Epoch: 3/20... Training loss: 0.4640\n",
      "Epoch: 3/20... Training loss: 0.4596\n",
      "Epoch: 3/20... Training loss: 0.4578\n",
      "Epoch: 3/20... Training loss: 0.4579\n",
      "Epoch: 3/20... Training loss: 0.4568\n",
      "Epoch: 3/20... Training loss: 0.4557\n",
      "Epoch: 3/20... Training loss: 0.4564\n",
      "Epoch: 3/20... Training loss: 0.4594\n",
      "Epoch: 3/20... Training loss: 0.4584\n",
      "Epoch: 3/20... Training loss: 0.4577\n",
      "Epoch: 3/20... Training loss: 0.4654\n",
      "Epoch: 3/20... Training loss: 0.4535\n",
      "Epoch: 3/20... Training loss: 0.4515\n",
      "Epoch: 3/20... Training loss: 0.4573\n",
      "Epoch: 3/20... Training loss: 0.4496\n",
      "Epoch: 3/20... Training loss: 0.4591\n",
      "Epoch: 3/20... Training loss: 0.4644\n",
      "Epoch: 3/20... Training loss: 0.4562\n",
      "Epoch: 3/20... Training loss: 0.4578\n",
      "Epoch: 3/20... Training loss: 0.4629\n",
      "Epoch: 3/20... Training loss: 0.4570\n",
      "Epoch: 3/20... Training loss: 0.4651\n",
      "Epoch: 3/20... Training loss: 0.4617\n",
      "Epoch: 3/20... Training loss: 0.4652\n",
      "Epoch: 3/20... Training loss: 0.4654\n",
      "Epoch: 3/20... Training loss: 0.4553\n",
      "Epoch: 3/20... Training loss: 0.4609\n",
      "Epoch: 3/20... Training loss: 0.4642\n",
      "Epoch: 3/20... Training loss: 0.4534\n",
      "Epoch: 3/20... Training loss: 0.4556\n",
      "Epoch: 3/20... Training loss: 0.4607\n",
      "Epoch: 3/20... Training loss: 0.4623\n",
      "Epoch: 3/20... Training loss: 0.4669\n",
      "Epoch: 3/20... Training loss: 0.4675\n",
      "Epoch: 3/20... Training loss: 0.4549\n",
      "Epoch: 3/20... Training loss: 0.4611\n",
      "Epoch: 3/20... Training loss: 0.4611\n",
      "Epoch: 3/20... Training loss: 0.4639\n",
      "Epoch: 3/20... Training loss: 0.4541\n",
      "Epoch: 3/20... Training loss: 0.4665\n",
      "Epoch: 3/20... Training loss: 0.4615\n",
      "Epoch: 3/20... Training loss: 0.4534\n",
      "Epoch: 3/20... Training loss: 0.4633\n",
      "Epoch: 3/20... Training loss: 0.4606\n",
      "Epoch: 3/20... Training loss: 0.4614\n",
      "Epoch: 3/20... Training loss: 0.4432\n",
      "Epoch: 3/20... Training loss: 0.4612\n",
      "Epoch: 3/20... Training loss: 0.4571\n",
      "Epoch: 3/20... Training loss: 0.4550\n",
      "Epoch: 3/20... Training loss: 0.4556\n",
      "Epoch: 3/20... Training loss: 0.4507\n",
      "Epoch: 3/20... Training loss: 0.4460\n",
      "Epoch: 3/20... Training loss: 0.4609\n",
      "Epoch: 3/20... Training loss: 0.4630\n",
      "Epoch: 3/20... Training loss: 0.4605\n",
      "Epoch: 3/20... Training loss: 0.4582\n",
      "Epoch: 3/20... Training loss: 0.4549\n",
      "Epoch: 3/20... Training loss: 0.4657\n",
      "Epoch: 3/20... Training loss: 0.4599\n",
      "Epoch: 3/20... Training loss: 0.4630\n",
      "Epoch: 3/20... Training loss: 0.4605\n",
      "Epoch: 3/20... Training loss: 0.4659\n",
      "Epoch: 3/20... Training loss: 0.4578\n",
      "Epoch: 3/20... Training loss: 0.4558\n",
      "Epoch: 3/20... Training loss: 0.4600\n",
      "Epoch: 3/20... Training loss: 0.4495\n",
      "Epoch: 3/20... Training loss: 0.4568\n",
      "Epoch: 3/20... Training loss: 0.4570\n",
      "Epoch: 3/20... Training loss: 0.4444\n",
      "Epoch: 3/20... Training loss: 0.4629\n",
      "Epoch: 3/20... Training loss: 0.4591\n",
      "Epoch: 3/20... Training loss: 0.4560\n",
      "Epoch: 3/20... Training loss: 0.4463\n",
      "Epoch: 3/20... Training loss: 0.4608\n",
      "Epoch: 3/20... Training loss: 0.4570\n",
      "Epoch: 3/20... Training loss: 0.4618\n",
      "Epoch: 3/20... Training loss: 0.4607\n",
      "Epoch: 3/20... Training loss: 0.4561\n",
      "Epoch: 3/20... Training loss: 0.4613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20... Training loss: 0.4543\n",
      "Epoch: 3/20... Training loss: 0.4602\n",
      "Epoch: 3/20... Training loss: 0.4639\n",
      "Epoch: 3/20... Training loss: 0.4483\n",
      "Epoch: 3/20... Training loss: 0.4461\n",
      "Epoch: 3/20... Training loss: 0.4522\n",
      "Epoch: 3/20... Training loss: 0.4544\n",
      "Epoch: 3/20... Training loss: 0.4657\n",
      "Epoch: 3/20... Training loss: 0.4550\n",
      "Epoch: 3/20... Training loss: 0.4502\n",
      "Epoch: 3/20... Training loss: 0.4493\n",
      "Epoch: 3/20... Training loss: 0.4505\n",
      "Epoch: 3/20... Training loss: 0.4609\n",
      "Epoch: 3/20... Training loss: 0.4576\n",
      "Epoch: 3/20... Training loss: 0.4672\n",
      "Epoch: 3/20... Training loss: 0.4573\n",
      "Epoch: 3/20... Training loss: 0.4543\n",
      "Epoch: 3/20... Training loss: 0.4522\n",
      "Epoch: 3/20... Training loss: 0.4592\n",
      "Epoch: 3/20... Training loss: 0.4578\n",
      "Epoch: 3/20... Training loss: 0.4564\n",
      "Epoch: 3/20... Training loss: 0.4367\n",
      "Epoch: 3/20... Training loss: 0.4607\n",
      "Epoch: 3/20... Training loss: 0.4598\n",
      "Epoch: 3/20... Training loss: 0.4504\n",
      "Epoch: 3/20... Training loss: 0.4522\n",
      "Epoch: 3/20... Training loss: 0.4503\n",
      "Epoch: 3/20... Training loss: 0.4578\n",
      "Epoch: 3/20... Training loss: 0.4558\n",
      "Epoch: 3/20... Training loss: 0.4475\n",
      "Epoch: 3/20... Training loss: 0.4617\n",
      "Epoch: 3/20... Training loss: 0.4504\n",
      "Epoch: 3/20... Training loss: 0.4603\n",
      "Epoch: 3/20... Training loss: 0.4595\n",
      "Epoch: 3/20... Training loss: 0.4556\n",
      "Epoch: 3/20... Training loss: 0.4522\n",
      "Epoch: 3/20... Training loss: 0.4625\n",
      "Epoch: 3/20... Training loss: 0.4557\n",
      "Epoch: 3/20... Training loss: 0.4584\n",
      "Epoch: 3/20... Training loss: 0.4489\n",
      "Epoch: 3/20... Training loss: 0.4452\n",
      "Epoch: 3/20... Training loss: 0.4483\n",
      "Epoch: 3/20... Training loss: 0.4493\n",
      "Epoch: 3/20... Training loss: 0.4483\n",
      "Epoch: 3/20... Training loss: 0.4556\n",
      "Epoch: 3/20... Training loss: 0.4577\n",
      "Epoch: 3/20... Training loss: 0.4499\n",
      "Epoch: 3/20... Training loss: 0.4481\n",
      "Epoch: 3/20... Training loss: 0.4569\n",
      "Epoch: 3/20... Training loss: 0.4443\n",
      "Epoch: 3/20... Training loss: 0.4390\n",
      "Epoch: 3/20... Training loss: 0.4551\n",
      "Epoch: 3/20... Training loss: 0.4540\n",
      "Epoch: 3/20... Training loss: 0.4615\n",
      "Epoch: 3/20... Training loss: 0.4583\n",
      "Epoch: 3/20... Training loss: 0.4513\n",
      "Epoch: 3/20... Training loss: 0.4522\n",
      "Epoch: 3/20... Training loss: 0.4563\n",
      "Epoch: 3/20... Training loss: 0.4617\n",
      "Epoch: 3/20... Training loss: 0.4502\n",
      "Epoch: 3/20... Training loss: 0.4627\n",
      "Epoch: 3/20... Training loss: 0.4649\n",
      "Epoch: 3/20... Training loss: 0.4501\n",
      "Epoch: 3/20... Training loss: 0.4681\n",
      "Epoch: 3/20... Training loss: 0.4607\n",
      "Epoch: 3/20... Training loss: 0.4564\n",
      "Epoch: 3/20... Training loss: 0.4515\n",
      "Epoch: 3/20... Training loss: 0.4579\n",
      "Epoch: 3/20... Training loss: 0.4553\n",
      "Epoch: 3/20... Training loss: 0.4547\n",
      "Epoch: 3/20... Training loss: 0.4554\n",
      "Epoch: 3/20... Training loss: 0.4579\n",
      "Epoch: 3/20... Training loss: 0.4593\n",
      "Epoch: 3/20... Training loss: 0.4443\n",
      "Epoch: 3/20... Training loss: 0.4501\n",
      "Epoch: 3/20... Training loss: 0.4556\n",
      "Epoch: 3/20... Training loss: 0.4425\n",
      "Epoch: 3/20... Training loss: 0.4635\n",
      "Epoch: 3/20... Training loss: 0.4530\n",
      "Epoch: 3/20... Training loss: 0.4580\n",
      "Epoch: 3/20... Training loss: 0.4526\n",
      "Epoch: 3/20... Training loss: 0.4544\n",
      "Epoch: 3/20... Training loss: 0.4560\n",
      "Epoch: 3/20... Training loss: 0.4542\n",
      "Epoch: 3/20... Training loss: 0.4458\n",
      "Epoch: 3/20... Training loss: 0.4521\n",
      "Epoch: 3/20... Training loss: 0.4577\n",
      "Epoch: 3/20... Training loss: 0.4551\n",
      "Epoch: 3/20... Training loss: 0.4550\n",
      "Epoch: 3/20... Training loss: 0.4562\n",
      "Epoch: 3/20... Training loss: 0.4549\n",
      "Epoch: 3/20... Training loss: 0.4537\n",
      "Epoch: 3/20... Training loss: 0.4429\n",
      "Epoch: 3/20... Training loss: 0.4496\n",
      "Epoch: 3/20... Training loss: 0.4451\n",
      "Epoch: 3/20... Training loss: 0.4516\n",
      "Epoch: 3/20... Training loss: 0.4622\n",
      "Epoch: 3/20... Training loss: 0.4532\n",
      "Epoch: 3/20... Training loss: 0.4570\n",
      "Epoch: 3/20... Training loss: 0.4551\n",
      "Epoch: 3/20... Training loss: 0.4555\n",
      "Epoch: 3/20... Training loss: 0.4488\n",
      "Epoch: 3/20... Training loss: 0.4568\n",
      "Epoch: 4/20... Training loss: 0.4500\n",
      "Epoch: 4/20... Training loss: 0.4546\n",
      "Epoch: 4/20... Training loss: 0.4444\n",
      "Epoch: 4/20... Training loss: 0.4489\n",
      "Epoch: 4/20... Training loss: 0.4446\n",
      "Epoch: 4/20... Training loss: 0.4559\n",
      "Epoch: 4/20... Training loss: 0.4449\n",
      "Epoch: 4/20... Training loss: 0.4498\n",
      "Epoch: 4/20... Training loss: 0.4605\n",
      "Epoch: 4/20... Training loss: 0.4607\n",
      "Epoch: 4/20... Training loss: 0.4513\n",
      "Epoch: 4/20... Training loss: 0.4466\n",
      "Epoch: 4/20... Training loss: 0.4469\n",
      "Epoch: 4/20... Training loss: 0.4568\n",
      "Epoch: 4/20... Training loss: 0.4593\n",
      "Epoch: 4/20... Training loss: 0.4576\n",
      "Epoch: 4/20... Training loss: 0.4470\n",
      "Epoch: 4/20... Training loss: 0.4456\n",
      "Epoch: 4/20... Training loss: 0.4522\n",
      "Epoch: 4/20... Training loss: 0.4540\n",
      "Epoch: 4/20... Training loss: 0.4520\n",
      "Epoch: 4/20... Training loss: 0.4526\n",
      "Epoch: 4/20... Training loss: 0.4523\n",
      "Epoch: 4/20... Training loss: 0.4501\n",
      "Epoch: 4/20... Training loss: 0.4545\n",
      "Epoch: 4/20... Training loss: 0.4507\n",
      "Epoch: 4/20... Training loss: 0.4481\n",
      "Epoch: 4/20... Training loss: 0.4626\n",
      "Epoch: 4/20... Training loss: 0.4539\n",
      "Epoch: 4/20... Training loss: 0.4458\n",
      "Epoch: 4/20... Training loss: 0.4496\n",
      "Epoch: 4/20... Training loss: 0.4446\n",
      "Epoch: 4/20... Training loss: 0.4536\n",
      "Epoch: 4/20... Training loss: 0.4445\n",
      "Epoch: 4/20... Training loss: 0.4450\n",
      "Epoch: 4/20... Training loss: 0.4532\n",
      "Epoch: 4/20... Training loss: 0.4522\n",
      "Epoch: 4/20... Training loss: 0.4555\n",
      "Epoch: 4/20... Training loss: 0.4477\n",
      "Epoch: 4/20... Training loss: 0.4458\n",
      "Epoch: 4/20... Training loss: 0.4412\n",
      "Epoch: 4/20... Training loss: 0.4600\n",
      "Epoch: 4/20... Training loss: 0.4482\n",
      "Epoch: 4/20... Training loss: 0.4463\n",
      "Epoch: 4/20... Training loss: 0.4538\n",
      "Epoch: 4/20... Training loss: 0.4540\n",
      "Epoch: 4/20... Training loss: 0.4482\n",
      "Epoch: 4/20... Training loss: 0.4577\n",
      "Epoch: 4/20... Training loss: 0.4509\n",
      "Epoch: 4/20... Training loss: 0.4533\n",
      "Epoch: 4/20... Training loss: 0.4446\n",
      "Epoch: 4/20... Training loss: 0.4406\n",
      "Epoch: 4/20... Training loss: 0.4563\n",
      "Epoch: 4/20... Training loss: 0.4519\n",
      "Epoch: 4/20... Training loss: 0.4464\n",
      "Epoch: 4/20... Training loss: 0.4626\n",
      "Epoch: 4/20... Training loss: 0.4562\n",
      "Epoch: 4/20... Training loss: 0.4565\n",
      "Epoch: 4/20... Training loss: 0.4515\n",
      "Epoch: 4/20... Training loss: 0.4547\n",
      "Epoch: 4/20... Training loss: 0.4463\n",
      "Epoch: 4/20... Training loss: 0.4516\n",
      "Epoch: 4/20... Training loss: 0.4522\n",
      "Epoch: 4/20... Training loss: 0.4541\n",
      "Epoch: 4/20... Training loss: 0.4502\n",
      "Epoch: 4/20... Training loss: 0.4505\n",
      "Epoch: 4/20... Training loss: 0.4560\n",
      "Epoch: 4/20... Training loss: 0.4510\n",
      "Epoch: 4/20... Training loss: 0.4519\n",
      "Epoch: 4/20... Training loss: 0.4441\n",
      "Epoch: 4/20... Training loss: 0.4538\n",
      "Epoch: 4/20... Training loss: 0.4567\n",
      "Epoch: 4/20... Training loss: 0.4502\n",
      "Epoch: 4/20... Training loss: 0.4523\n",
      "Epoch: 4/20... Training loss: 0.4496\n",
      "Epoch: 4/20... Training loss: 0.4529\n",
      "Epoch: 4/20... Training loss: 0.4387\n",
      "Epoch: 4/20... Training loss: 0.4592\n",
      "Epoch: 4/20... Training loss: 0.4477\n",
      "Epoch: 4/20... Training loss: 0.4510\n",
      "Epoch: 4/20... Training loss: 0.4418\n",
      "Epoch: 4/20... Training loss: 0.4604\n",
      "Epoch: 4/20... Training loss: 0.4466\n",
      "Epoch: 4/20... Training loss: 0.4566\n",
      "Epoch: 4/20... Training loss: 0.4563\n",
      "Epoch: 4/20... Training loss: 0.4391\n",
      "Epoch: 4/20... Training loss: 0.4495\n",
      "Epoch: 4/20... Training loss: 0.4400\n",
      "Epoch: 4/20... Training loss: 0.4445\n",
      "Epoch: 4/20... Training loss: 0.4403\n",
      "Epoch: 4/20... Training loss: 0.4535\n",
      "Epoch: 4/20... Training loss: 0.4568\n",
      "Epoch: 4/20... Training loss: 0.4588\n",
      "Epoch: 4/20... Training loss: 0.4539\n",
      "Epoch: 4/20... Training loss: 0.4610\n",
      "Epoch: 4/20... Training loss: 0.4456\n",
      "Epoch: 4/20... Training loss: 0.4475\n",
      "Epoch: 4/20... Training loss: 0.4427\n",
      "Epoch: 4/20... Training loss: 0.4564\n",
      "Epoch: 4/20... Training loss: 0.4477\n",
      "Epoch: 4/20... Training loss: 0.4545\n",
      "Epoch: 4/20... Training loss: 0.4563\n",
      "Epoch: 4/20... Training loss: 0.4517\n",
      "Epoch: 4/20... Training loss: 0.4466\n",
      "Epoch: 4/20... Training loss: 0.4563\n",
      "Epoch: 4/20... Training loss: 0.4639\n",
      "Epoch: 4/20... Training loss: 0.4473\n",
      "Epoch: 4/20... Training loss: 0.4536\n",
      "Epoch: 4/20... Training loss: 0.4480\n",
      "Epoch: 4/20... Training loss: 0.4411\n",
      "Epoch: 4/20... Training loss: 0.4454\n",
      "Epoch: 4/20... Training loss: 0.4623\n",
      "Epoch: 4/20... Training loss: 0.4461\n",
      "Epoch: 4/20... Training loss: 0.4500\n",
      "Epoch: 4/20... Training loss: 0.4404\n",
      "Epoch: 4/20... Training loss: 0.4605\n",
      "Epoch: 4/20... Training loss: 0.4436\n",
      "Epoch: 4/20... Training loss: 0.4523\n",
      "Epoch: 4/20... Training loss: 0.4469\n",
      "Epoch: 4/20... Training loss: 0.4514\n",
      "Epoch: 4/20... Training loss: 0.4438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20... Training loss: 0.4528\n",
      "Epoch: 4/20... Training loss: 0.4607\n",
      "Epoch: 4/20... Training loss: 0.4555\n",
      "Epoch: 4/20... Training loss: 0.4545\n",
      "Epoch: 4/20... Training loss: 0.4581\n",
      "Epoch: 4/20... Training loss: 0.4466\n",
      "Epoch: 4/20... Training loss: 0.4485\n",
      "Epoch: 4/20... Training loss: 0.4546\n",
      "Epoch: 4/20... Training loss: 0.4475\n",
      "Epoch: 4/20... Training loss: 0.4533\n",
      "Epoch: 4/20... Training loss: 0.4540\n",
      "Epoch: 4/20... Training loss: 0.4578\n",
      "Epoch: 4/20... Training loss: 0.4661\n",
      "Epoch: 4/20... Training loss: 0.4460\n",
      "Epoch: 4/20... Training loss: 0.4499\n",
      "Epoch: 4/20... Training loss: 0.4595\n",
      "Epoch: 4/20... Training loss: 0.4535\n",
      "Epoch: 4/20... Training loss: 0.4540\n",
      "Epoch: 4/20... Training loss: 0.4611\n",
      "Epoch: 4/20... Training loss: 0.4552\n",
      "Epoch: 4/20... Training loss: 0.4483\n",
      "Epoch: 4/20... Training loss: 0.4414\n",
      "Epoch: 4/20... Training loss: 0.4451\n",
      "Epoch: 4/20... Training loss: 0.4515\n",
      "Epoch: 4/20... Training loss: 0.4517\n",
      "Epoch: 4/20... Training loss: 0.4502\n",
      "Epoch: 4/20... Training loss: 0.4500\n",
      "Epoch: 4/20... Training loss: 0.4458\n",
      "Epoch: 4/20... Training loss: 0.4469\n",
      "Epoch: 4/20... Training loss: 0.4441\n",
      "Epoch: 4/20... Training loss: 0.4552\n",
      "Epoch: 4/20... Training loss: 0.4512\n",
      "Epoch: 4/20... Training loss: 0.4529\n",
      "Epoch: 4/20... Training loss: 0.4494\n",
      "Epoch: 4/20... Training loss: 0.4405\n",
      "Epoch: 4/20... Training loss: 0.4594\n",
      "Epoch: 4/20... Training loss: 0.4488\n",
      "Epoch: 4/20... Training loss: 0.4429\n",
      "Epoch: 4/20... Training loss: 0.4521\n",
      "Epoch: 4/20... Training loss: 0.4508\n",
      "Epoch: 4/20... Training loss: 0.4482\n",
      "Epoch: 4/20... Training loss: 0.4558\n",
      "Epoch: 4/20... Training loss: 0.4517\n",
      "Epoch: 4/20... Training loss: 0.4543\n",
      "Epoch: 4/20... Training loss: 0.4454\n",
      "Epoch: 4/20... Training loss: 0.4487\n",
      "Epoch: 4/20... Training loss: 0.4616\n",
      "Epoch: 4/20... Training loss: 0.4456\n",
      "Epoch: 4/20... Training loss: 0.4467\n",
      "Epoch: 4/20... Training loss: 0.4428\n",
      "Epoch: 4/20... Training loss: 0.4414\n",
      "Epoch: 4/20... Training loss: 0.4514\n",
      "Epoch: 4/20... Training loss: 0.4578\n",
      "Epoch: 4/20... Training loss: 0.4507\n",
      "Epoch: 4/20... Training loss: 0.4441\n",
      "Epoch: 4/20... Training loss: 0.4433\n",
      "Epoch: 4/20... Training loss: 0.4476\n",
      "Epoch: 4/20... Training loss: 0.4491\n",
      "Epoch: 4/20... Training loss: 0.4533\n",
      "Epoch: 4/20... Training loss: 0.4490\n",
      "Epoch: 4/20... Training loss: 0.4532\n",
      "Epoch: 4/20... Training loss: 0.4652\n",
      "Epoch: 5/20... Training loss: 0.4470\n",
      "Epoch: 5/20... Training loss: 0.4445\n",
      "Epoch: 5/20... Training loss: 0.4586\n",
      "Epoch: 5/20... Training loss: 0.4537\n",
      "Epoch: 5/20... Training loss: 0.4486\n",
      "Epoch: 5/20... Training loss: 0.4488\n",
      "Epoch: 5/20... Training loss: 0.4486\n",
      "Epoch: 5/20... Training loss: 0.4444\n",
      "Epoch: 5/20... Training loss: 0.4390\n",
      "Epoch: 5/20... Training loss: 0.4471\n",
      "Epoch: 5/20... Training loss: 0.4444\n",
      "Epoch: 5/20... Training loss: 0.4486\n",
      "Epoch: 5/20... Training loss: 0.4429\n",
      "Epoch: 5/20... Training loss: 0.4544\n",
      "Epoch: 5/20... Training loss: 0.4488\n",
      "Epoch: 5/20... Training loss: 0.4508\n",
      "Epoch: 5/20... Training loss: 0.4461\n",
      "Epoch: 5/20... Training loss: 0.4481\n",
      "Epoch: 5/20... Training loss: 0.4539\n",
      "Epoch: 5/20... Training loss: 0.4477\n",
      "Epoch: 5/20... Training loss: 0.4459\n",
      "Epoch: 5/20... Training loss: 0.4567\n",
      "Epoch: 5/20... Training loss: 0.4456\n",
      "Epoch: 5/20... Training loss: 0.4517\n",
      "Epoch: 5/20... Training loss: 0.4515\n",
      "Epoch: 5/20... Training loss: 0.4483\n",
      "Epoch: 5/20... Training loss: 0.4472\n",
      "Epoch: 5/20... Training loss: 0.4463\n",
      "Epoch: 5/20... Training loss: 0.4461\n",
      "Epoch: 5/20... Training loss: 0.4529\n",
      "Epoch: 5/20... Training loss: 0.4538\n",
      "Epoch: 5/20... Training loss: 0.4466\n",
      "Epoch: 5/20... Training loss: 0.4443\n",
      "Epoch: 5/20... Training loss: 0.4540\n",
      "Epoch: 5/20... Training loss: 0.4526\n",
      "Epoch: 5/20... Training loss: 0.4539\n",
      "Epoch: 5/20... Training loss: 0.4492\n",
      "Epoch: 5/20... Training loss: 0.4484\n",
      "Epoch: 5/20... Training loss: 0.4553\n",
      "Epoch: 5/20... Training loss: 0.4494\n",
      "Epoch: 5/20... Training loss: 0.4538\n",
      "Epoch: 5/20... Training loss: 0.4438\n",
      "Epoch: 5/20... Training loss: 0.4554\n",
      "Epoch: 5/20... Training loss: 0.4459\n",
      "Epoch: 5/20... Training loss: 0.4615\n",
      "Epoch: 5/20... Training loss: 0.4455\n",
      "Epoch: 5/20... Training loss: 0.4412\n",
      "Epoch: 5/20... Training loss: 0.4454\n",
      "Epoch: 5/20... Training loss: 0.4452\n",
      "Epoch: 5/20... Training loss: 0.4546\n",
      "Epoch: 5/20... Training loss: 0.4564\n",
      "Epoch: 5/20... Training loss: 0.4396\n",
      "Epoch: 5/20... Training loss: 0.4610\n",
      "Epoch: 5/20... Training loss: 0.4498\n",
      "Epoch: 5/20... Training loss: 0.4551\n",
      "Epoch: 5/20... Training loss: 0.4475\n",
      "Epoch: 5/20... Training loss: 0.4430\n",
      "Epoch: 5/20... Training loss: 0.4593\n",
      "Epoch: 5/20... Training loss: 0.4489\n",
      "Epoch: 5/20... Training loss: 0.4472\n",
      "Epoch: 5/20... Training loss: 0.4469\n",
      "Epoch: 5/20... Training loss: 0.4530\n",
      "Epoch: 5/20... Training loss: 0.4379\n",
      "Epoch: 5/20... Training loss: 0.4583\n",
      "Epoch: 5/20... Training loss: 0.4490\n",
      "Epoch: 5/20... Training loss: 0.4482\n",
      "Epoch: 5/20... Training loss: 0.4567\n",
      "Epoch: 5/20... Training loss: 0.4497\n",
      "Epoch: 5/20... Training loss: 0.4471\n",
      "Epoch: 5/20... Training loss: 0.4481\n",
      "Epoch: 5/20... Training loss: 0.4500\n",
      "Epoch: 5/20... Training loss: 0.4422\n",
      "Epoch: 5/20... Training loss: 0.4507\n",
      "Epoch: 5/20... Training loss: 0.4456\n",
      "Epoch: 5/20... Training loss: 0.4471\n",
      "Epoch: 5/20... Training loss: 0.4438\n",
      "Epoch: 5/20... Training loss: 0.4471\n",
      "Epoch: 5/20... Training loss: 0.4519\n",
      "Epoch: 5/20... Training loss: 0.4537\n",
      "Epoch: 5/20... Training loss: 0.4421\n",
      "Epoch: 5/20... Training loss: 0.4470\n",
      "Epoch: 5/20... Training loss: 0.4457\n",
      "Epoch: 5/20... Training loss: 0.4543\n",
      "Epoch: 5/20... Training loss: 0.4500\n",
      "Epoch: 5/20... Training loss: 0.4473\n",
      "Epoch: 5/20... Training loss: 0.4544\n",
      "Epoch: 5/20... Training loss: 0.4486\n",
      "Epoch: 5/20... Training loss: 0.4506\n",
      "Epoch: 5/20... Training loss: 0.4418\n",
      "Epoch: 5/20... Training loss: 0.4479\n",
      "Epoch: 5/20... Training loss: 0.4501\n",
      "Epoch: 5/20... Training loss: 0.4528\n",
      "Epoch: 5/20... Training loss: 0.4440\n",
      "Epoch: 5/20... Training loss: 0.4383\n",
      "Epoch: 5/20... Training loss: 0.4505\n",
      "Epoch: 5/20... Training loss: 0.4527\n",
      "Epoch: 5/20... Training loss: 0.4498\n",
      "Epoch: 5/20... Training loss: 0.4495\n",
      "Epoch: 5/20... Training loss: 0.4402\n",
      "Epoch: 5/20... Training loss: 0.4528\n",
      "Epoch: 5/20... Training loss: 0.4449\n",
      "Epoch: 5/20... Training loss: 0.4463\n",
      "Epoch: 5/20... Training loss: 0.4525\n",
      "Epoch: 5/20... Training loss: 0.4558\n",
      "Epoch: 5/20... Training loss: 0.4532\n",
      "Epoch: 5/20... Training loss: 0.4435\n",
      "Epoch: 5/20... Training loss: 0.4521\n",
      "Epoch: 5/20... Training loss: 0.4431\n",
      "Epoch: 5/20... Training loss: 0.4469\n",
      "Epoch: 5/20... Training loss: 0.4413\n",
      "Epoch: 5/20... Training loss: 0.4491\n",
      "Epoch: 5/20... Training loss: 0.4577\n",
      "Epoch: 5/20... Training loss: 0.4413\n",
      "Epoch: 5/20... Training loss: 0.4393\n",
      "Epoch: 5/20... Training loss: 0.4493\n",
      "Epoch: 5/20... Training loss: 0.4497\n",
      "Epoch: 5/20... Training loss: 0.4433\n",
      "Epoch: 5/20... Training loss: 0.4397\n",
      "Epoch: 5/20... Training loss: 0.4535\n",
      "Epoch: 5/20... Training loss: 0.4379\n",
      "Epoch: 5/20... Training loss: 0.4464\n",
      "Epoch: 5/20... Training loss: 0.4488\n",
      "Epoch: 5/20... Training loss: 0.4550\n",
      "Epoch: 5/20... Training loss: 0.4485\n",
      "Epoch: 5/20... Training loss: 0.4460\n",
      "Epoch: 5/20... Training loss: 0.4501\n",
      "Epoch: 5/20... Training loss: 0.4411\n",
      "Epoch: 5/20... Training loss: 0.4483\n",
      "Epoch: 5/20... Training loss: 0.4522\n",
      "Epoch: 5/20... Training loss: 0.4489\n",
      "Epoch: 5/20... Training loss: 0.4504\n",
      "Epoch: 5/20... Training loss: 0.4472\n",
      "Epoch: 5/20... Training loss: 0.4425\n",
      "Epoch: 5/20... Training loss: 0.4614\n",
      "Epoch: 5/20... Training loss: 0.4446\n",
      "Epoch: 5/20... Training loss: 0.4579\n",
      "Epoch: 5/20... Training loss: 0.4543\n",
      "Epoch: 5/20... Training loss: 0.4449\n",
      "Epoch: 5/20... Training loss: 0.4501\n",
      "Epoch: 5/20... Training loss: 0.4461\n",
      "Epoch: 5/20... Training loss: 0.4277\n",
      "Epoch: 5/20... Training loss: 0.4397\n",
      "Epoch: 5/20... Training loss: 0.4462\n",
      "Epoch: 5/20... Training loss: 0.4458\n",
      "Epoch: 5/20... Training loss: 0.4498\n",
      "Epoch: 5/20... Training loss: 0.4397\n",
      "Epoch: 5/20... Training loss: 0.4461\n",
      "Epoch: 5/20... Training loss: 0.4398\n",
      "Epoch: 5/20... Training loss: 0.4457\n",
      "Epoch: 5/20... Training loss: 0.4512\n",
      "Epoch: 5/20... Training loss: 0.4524\n",
      "Epoch: 5/20... Training loss: 0.4541\n",
      "Epoch: 5/20... Training loss: 0.4419\n",
      "Epoch: 5/20... Training loss: 0.4483\n",
      "Epoch: 5/20... Training loss: 0.4397\n",
      "Epoch: 5/20... Training loss: 0.4399\n",
      "Epoch: 5/20... Training loss: 0.4330\n",
      "Epoch: 5/20... Training loss: 0.4585\n",
      "Epoch: 5/20... Training loss: 0.4486\n",
      "Epoch: 5/20... Training loss: 0.4419\n",
      "Epoch: 5/20... Training loss: 0.4465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20... Training loss: 0.4403\n",
      "Epoch: 5/20... Training loss: 0.4399\n",
      "Epoch: 5/20... Training loss: 0.4509\n",
      "Epoch: 5/20... Training loss: 0.4443\n",
      "Epoch: 5/20... Training loss: 0.4491\n",
      "Epoch: 5/20... Training loss: 0.4438\n",
      "Epoch: 5/20... Training loss: 0.4451\n",
      "Epoch: 5/20... Training loss: 0.4473\n",
      "Epoch: 5/20... Training loss: 0.4522\n",
      "Epoch: 5/20... Training loss: 0.4513\n",
      "Epoch: 5/20... Training loss: 0.4409\n",
      "Epoch: 5/20... Training loss: 0.4535\n",
      "Epoch: 5/20... Training loss: 0.4474\n",
      "Epoch: 5/20... Training loss: 0.4432\n",
      "Epoch: 5/20... Training loss: 0.4530\n",
      "Epoch: 5/20... Training loss: 0.4544\n",
      "Epoch: 5/20... Training loss: 0.4534\n",
      "Epoch: 5/20... Training loss: 0.4477\n",
      "Epoch: 5/20... Training loss: 0.4539\n",
      "Epoch: 5/20... Training loss: 0.4483\n",
      "Epoch: 5/20... Training loss: 0.4560\n",
      "Epoch: 5/20... Training loss: 0.4401\n",
      "Epoch: 6/20... Training loss: 0.4442\n",
      "Epoch: 6/20... Training loss: 0.4449\n",
      "Epoch: 6/20... Training loss: 0.4349\n",
      "Epoch: 6/20... Training loss: 0.4476\n",
      "Epoch: 6/20... Training loss: 0.4431\n",
      "Epoch: 6/20... Training loss: 0.4424\n",
      "Epoch: 6/20... Training loss: 0.4451\n",
      "Epoch: 6/20... Training loss: 0.4451\n",
      "Epoch: 6/20... Training loss: 0.4358\n",
      "Epoch: 6/20... Training loss: 0.4486\n",
      "Epoch: 6/20... Training loss: 0.4342\n",
      "Epoch: 6/20... Training loss: 0.4498\n",
      "Epoch: 6/20... Training loss: 0.4444\n",
      "Epoch: 6/20... Training loss: 0.4381\n",
      "Epoch: 6/20... Training loss: 0.4586\n",
      "Epoch: 6/20... Training loss: 0.4307\n",
      "Epoch: 6/20... Training loss: 0.4587\n",
      "Epoch: 6/20... Training loss: 0.4567\n",
      "Epoch: 6/20... Training loss: 0.4397\n",
      "Epoch: 6/20... Training loss: 0.4523\n",
      "Epoch: 6/20... Training loss: 0.4453\n",
      "Epoch: 6/20... Training loss: 0.4532\n",
      "Epoch: 6/20... Training loss: 0.4415\n",
      "Epoch: 6/20... Training loss: 0.4533\n",
      "Epoch: 6/20... Training loss: 0.4508\n",
      "Epoch: 6/20... Training loss: 0.4463\n",
      "Epoch: 6/20... Training loss: 0.4416\n",
      "Epoch: 6/20... Training loss: 0.4461\n",
      "Epoch: 6/20... Training loss: 0.4373\n",
      "Epoch: 6/20... Training loss: 0.4423\n",
      "Epoch: 6/20... Training loss: 0.4379\n",
      "Epoch: 6/20... Training loss: 0.4463\n",
      "Epoch: 6/20... Training loss: 0.4521\n",
      "Epoch: 6/20... Training loss: 0.4471\n",
      "Epoch: 6/20... Training loss: 0.4378\n",
      "Epoch: 6/20... Training loss: 0.4522\n",
      "Epoch: 6/20... Training loss: 0.4532\n",
      "Epoch: 6/20... Training loss: 0.4555\n",
      "Epoch: 6/20... Training loss: 0.4494\n",
      "Epoch: 6/20... Training loss: 0.4471\n",
      "Epoch: 6/20... Training loss: 0.4471\n",
      "Epoch: 6/20... Training loss: 0.4475\n",
      "Epoch: 6/20... Training loss: 0.4415\n",
      "Epoch: 6/20... Training loss: 0.4513\n",
      "Epoch: 6/20... Training loss: 0.4531\n",
      "Epoch: 6/20... Training loss: 0.4382\n",
      "Epoch: 6/20... Training loss: 0.4479\n",
      "Epoch: 6/20... Training loss: 0.4475\n",
      "Epoch: 6/20... Training loss: 0.4525\n",
      "Epoch: 6/20... Training loss: 0.4595\n",
      "Epoch: 6/20... Training loss: 0.4441\n",
      "Epoch: 6/20... Training loss: 0.4455\n",
      "Epoch: 6/20... Training loss: 0.4483\n",
      "Epoch: 6/20... Training loss: 0.4452\n",
      "Epoch: 6/20... Training loss: 0.4527\n",
      "Epoch: 6/20... Training loss: 0.4482\n",
      "Epoch: 6/20... Training loss: 0.4572\n",
      "Epoch: 6/20... Training loss: 0.4401\n",
      "Epoch: 6/20... Training loss: 0.4480\n",
      "Epoch: 6/20... Training loss: 0.4461\n",
      "Epoch: 6/20... Training loss: 0.4391\n",
      "Epoch: 6/20... Training loss: 0.4458\n",
      "Epoch: 6/20... Training loss: 0.4406\n",
      "Epoch: 6/20... Training loss: 0.4530\n",
      "Epoch: 6/20... Training loss: 0.4468\n",
      "Epoch: 6/20... Training loss: 0.4452\n",
      "Epoch: 6/20... Training loss: 0.4588\n",
      "Epoch: 6/20... Training loss: 0.4415\n",
      "Epoch: 6/20... Training loss: 0.4482\n",
      "Epoch: 6/20... Training loss: 0.4490\n",
      "Epoch: 6/20... Training loss: 0.4470\n",
      "Epoch: 6/20... Training loss: 0.4458\n",
      "Epoch: 6/20... Training loss: 0.4523\n",
      "Epoch: 6/20... Training loss: 0.4399\n",
      "Epoch: 6/20... Training loss: 0.4437\n",
      "Epoch: 6/20... Training loss: 0.4371\n",
      "Epoch: 6/20... Training loss: 0.4481\n",
      "Epoch: 6/20... Training loss: 0.4478\n",
      "Epoch: 6/20... Training loss: 0.4538\n",
      "Epoch: 6/20... Training loss: 0.4427\n",
      "Epoch: 6/20... Training loss: 0.4506\n",
      "Epoch: 6/20... Training loss: 0.4459\n",
      "Epoch: 6/20... Training loss: 0.4459\n",
      "Epoch: 6/20... Training loss: 0.4480\n",
      "Epoch: 6/20... Training loss: 0.4496\n",
      "Epoch: 6/20... Training loss: 0.4424\n",
      "Epoch: 6/20... Training loss: 0.4472\n",
      "Epoch: 6/20... Training loss: 0.4361\n",
      "Epoch: 6/20... Training loss: 0.4443\n",
      "Epoch: 6/20... Training loss: 0.4383\n",
      "Epoch: 6/20... Training loss: 0.4431\n",
      "Epoch: 6/20... Training loss: 0.4511\n",
      "Epoch: 6/20... Training loss: 0.4506\n",
      "Epoch: 6/20... Training loss: 0.4366\n",
      "Epoch: 6/20... Training loss: 0.4474\n",
      "Epoch: 6/20... Training loss: 0.4402\n",
      "Epoch: 6/20... Training loss: 0.4415\n",
      "Epoch: 6/20... Training loss: 0.4438\n",
      "Epoch: 6/20... Training loss: 0.4389\n",
      "Epoch: 6/20... Training loss: 0.4399\n",
      "Epoch: 6/20... Training loss: 0.4489\n",
      "Epoch: 6/20... Training loss: 0.4485\n",
      "Epoch: 6/20... Training loss: 0.4538\n",
      "Epoch: 6/20... Training loss: 0.4445\n",
      "Epoch: 6/20... Training loss: 0.4415\n",
      "Epoch: 6/20... Training loss: 0.4410\n",
      "Epoch: 6/20... Training loss: 0.4459\n",
      "Epoch: 6/20... Training loss: 0.4584\n",
      "Epoch: 6/20... Training loss: 0.4470\n",
      "Epoch: 6/20... Training loss: 0.4467\n",
      "Epoch: 6/20... Training loss: 0.4435\n",
      "Epoch: 6/20... Training loss: 0.4338\n",
      "Epoch: 6/20... Training loss: 0.4511\n",
      "Epoch: 6/20... Training loss: 0.4460\n",
      "Epoch: 6/20... Training loss: 0.4467\n",
      "Epoch: 6/20... Training loss: 0.4382\n",
      "Epoch: 6/20... Training loss: 0.4401\n",
      "Epoch: 6/20... Training loss: 0.4364\n",
      "Epoch: 6/20... Training loss: 0.4408\n",
      "Epoch: 6/20... Training loss: 0.4468\n",
      "Epoch: 6/20... Training loss: 0.4491\n",
      "Epoch: 6/20... Training loss: 0.4373\n",
      "Epoch: 6/20... Training loss: 0.4371\n",
      "Epoch: 6/20... Training loss: 0.4468\n",
      "Epoch: 6/20... Training loss: 0.4423\n",
      "Epoch: 6/20... Training loss: 0.4523\n",
      "Epoch: 6/20... Training loss: 0.4429\n",
      "Epoch: 6/20... Training loss: 0.4494\n",
      "Epoch: 6/20... Training loss: 0.4462\n",
      "Epoch: 6/20... Training loss: 0.4402\n",
      "Epoch: 6/20... Training loss: 0.4535\n",
      "Epoch: 6/20... Training loss: 0.4425\n",
      "Epoch: 6/20... Training loss: 0.4501\n",
      "Epoch: 6/20... Training loss: 0.4528\n",
      "Epoch: 6/20... Training loss: 0.4465\n",
      "Epoch: 6/20... Training loss: 0.4504\n",
      "Epoch: 6/20... Training loss: 0.4533\n",
      "Epoch: 6/20... Training loss: 0.4414\n",
      "Epoch: 6/20... Training loss: 0.4406\n",
      "Epoch: 6/20... Training loss: 0.4406\n",
      "Epoch: 6/20... Training loss: 0.4532\n",
      "Epoch: 6/20... Training loss: 0.4536\n",
      "Epoch: 6/20... Training loss: 0.4480\n",
      "Epoch: 6/20... Training loss: 0.4499\n",
      "Epoch: 6/20... Training loss: 0.4389\n",
      "Epoch: 6/20... Training loss: 0.4416\n",
      "Epoch: 6/20... Training loss: 0.4544\n",
      "Epoch: 6/20... Training loss: 0.4517\n",
      "Epoch: 6/20... Training loss: 0.4413\n",
      "Epoch: 6/20... Training loss: 0.4414\n",
      "Epoch: 6/20... Training loss: 0.4465\n",
      "Epoch: 6/20... Training loss: 0.4465\n",
      "Epoch: 6/20... Training loss: 0.4416\n",
      "Epoch: 6/20... Training loss: 0.4525\n",
      "Epoch: 6/20... Training loss: 0.4443\n",
      "Epoch: 6/20... Training loss: 0.4405\n",
      "Epoch: 6/20... Training loss: 0.4430\n",
      "Epoch: 6/20... Training loss: 0.4480\n",
      "Epoch: 6/20... Training loss: 0.4498\n",
      "Epoch: 6/20... Training loss: 0.4369\n",
      "Epoch: 6/20... Training loss: 0.4457\n",
      "Epoch: 6/20... Training loss: 0.4529\n",
      "Epoch: 6/20... Training loss: 0.4460\n",
      "Epoch: 6/20... Training loss: 0.4521\n",
      "Epoch: 6/20... Training loss: 0.4385\n",
      "Epoch: 6/20... Training loss: 0.4494\n",
      "Epoch: 6/20... Training loss: 0.4501\n",
      "Epoch: 6/20... Training loss: 0.4521\n",
      "Epoch: 6/20... Training loss: 0.4620\n",
      "Epoch: 6/20... Training loss: 0.4470\n",
      "Epoch: 6/20... Training loss: 0.4539\n",
      "Epoch: 6/20... Training loss: 0.4579\n",
      "Epoch: 6/20... Training loss: 0.4486\n",
      "Epoch: 6/20... Training loss: 0.4400\n",
      "Epoch: 6/20... Training loss: 0.4404\n",
      "Epoch: 6/20... Training loss: 0.4486\n",
      "Epoch: 6/20... Training loss: 0.4372\n",
      "Epoch: 6/20... Training loss: 0.4523\n",
      "Epoch: 6/20... Training loss: 0.4483\n",
      "Epoch: 6/20... Training loss: 0.4510\n",
      "Epoch: 6/20... Training loss: 0.4464\n",
      "Epoch: 6/20... Training loss: 0.4519\n",
      "Epoch: 6/20... Training loss: 0.4523\n",
      "Epoch: 7/20... Training loss: 0.4551\n",
      "Epoch: 7/20... Training loss: 0.4480\n",
      "Epoch: 7/20... Training loss: 0.4530\n",
      "Epoch: 7/20... Training loss: 0.4518\n",
      "Epoch: 7/20... Training loss: 0.4468\n",
      "Epoch: 7/20... Training loss: 0.4551\n",
      "Epoch: 7/20... Training loss: 0.4464\n",
      "Epoch: 7/20... Training loss: 0.4378\n",
      "Epoch: 7/20... Training loss: 0.4375\n",
      "Epoch: 7/20... Training loss: 0.4547\n",
      "Epoch: 7/20... Training loss: 0.4473\n",
      "Epoch: 7/20... Training loss: 0.4494\n",
      "Epoch: 7/20... Training loss: 0.4404\n",
      "Epoch: 7/20... Training loss: 0.4520\n",
      "Epoch: 7/20... Training loss: 0.4508\n",
      "Epoch: 7/20... Training loss: 0.4561\n",
      "Epoch: 7/20... Training loss: 0.4582\n",
      "Epoch: 7/20... Training loss: 0.4458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20... Training loss: 0.4373\n",
      "Epoch: 7/20... Training loss: 0.4495\n",
      "Epoch: 7/20... Training loss: 0.4539\n",
      "Epoch: 7/20... Training loss: 0.4424\n",
      "Epoch: 7/20... Training loss: 0.4444\n",
      "Epoch: 7/20... Training loss: 0.4373\n",
      "Epoch: 7/20... Training loss: 0.4460\n",
      "Epoch: 7/20... Training loss: 0.4566\n",
      "Epoch: 7/20... Training loss: 0.4398\n",
      "Epoch: 7/20... Training loss: 0.4516\n",
      "Epoch: 7/20... Training loss: 0.4429\n",
      "Epoch: 7/20... Training loss: 0.4520\n",
      "Epoch: 7/20... Training loss: 0.4547\n",
      "Epoch: 7/20... Training loss: 0.4534\n",
      "Epoch: 7/20... Training loss: 0.4468\n",
      "Epoch: 7/20... Training loss: 0.4417\n",
      "Epoch: 7/20... Training loss: 0.4498\n",
      "Epoch: 7/20... Training loss: 0.4379\n",
      "Epoch: 7/20... Training loss: 0.4527\n",
      "Epoch: 7/20... Training loss: 0.4482\n",
      "Epoch: 7/20... Training loss: 0.4328\n",
      "Epoch: 7/20... Training loss: 0.4358\n",
      "Epoch: 7/20... Training loss: 0.4476\n",
      "Epoch: 7/20... Training loss: 0.4478\n",
      "Epoch: 7/20... Training loss: 0.4451\n",
      "Epoch: 7/20... Training loss: 0.4484\n",
      "Epoch: 7/20... Training loss: 0.4538\n",
      "Epoch: 7/20... Training loss: 0.4423\n",
      "Epoch: 7/20... Training loss: 0.4466\n",
      "Epoch: 7/20... Training loss: 0.4465\n",
      "Epoch: 7/20... Training loss: 0.4489\n",
      "Epoch: 7/20... Training loss: 0.4420\n",
      "Epoch: 7/20... Training loss: 0.4428\n",
      "Epoch: 7/20... Training loss: 0.4532\n",
      "Epoch: 7/20... Training loss: 0.4396\n",
      "Epoch: 7/20... Training loss: 0.4387\n",
      "Epoch: 7/20... Training loss: 0.4553\n",
      "Epoch: 7/20... Training loss: 0.4361\n",
      "Epoch: 7/20... Training loss: 0.4544\n",
      "Epoch: 7/20... Training loss: 0.4518\n",
      "Epoch: 7/20... Training loss: 0.4528\n",
      "Epoch: 7/20... Training loss: 0.4438\n",
      "Epoch: 7/20... Training loss: 0.4509\n",
      "Epoch: 7/20... Training loss: 0.4380\n",
      "Epoch: 7/20... Training loss: 0.4407\n",
      "Epoch: 7/20... Training loss: 0.4398\n",
      "Epoch: 7/20... Training loss: 0.4467\n",
      "Epoch: 7/20... Training loss: 0.4573\n",
      "Epoch: 7/20... Training loss: 0.4475\n",
      "Epoch: 7/20... Training loss: 0.4390\n",
      "Epoch: 7/20... Training loss: 0.4413\n",
      "Epoch: 7/20... Training loss: 0.4371\n",
      "Epoch: 7/20... Training loss: 0.4516\n",
      "Epoch: 7/20... Training loss: 0.4442\n",
      "Epoch: 7/20... Training loss: 0.4368\n",
      "Epoch: 7/20... Training loss: 0.4522\n",
      "Epoch: 7/20... Training loss: 0.4380\n",
      "Epoch: 7/20... Training loss: 0.4465\n",
      "Epoch: 7/20... Training loss: 0.4589\n",
      "Epoch: 7/20... Training loss: 0.4456\n",
      "Epoch: 7/20... Training loss: 0.4431\n",
      "Epoch: 7/20... Training loss: 0.4511\n",
      "Epoch: 7/20... Training loss: 0.4439\n",
      "Epoch: 7/20... Training loss: 0.4525\n",
      "Epoch: 7/20... Training loss: 0.4462\n",
      "Epoch: 7/20... Training loss: 0.4455\n",
      "Epoch: 7/20... Training loss: 0.4476\n",
      "Epoch: 7/20... Training loss: 0.4472\n",
      "Epoch: 7/20... Training loss: 0.4434\n",
      "Epoch: 7/20... Training loss: 0.4394\n",
      "Epoch: 7/20... Training loss: 0.4486\n",
      "Epoch: 7/20... Training loss: 0.4469\n",
      "Epoch: 7/20... Training loss: 0.4481\n",
      "Epoch: 7/20... Training loss: 0.4432\n",
      "Epoch: 7/20... Training loss: 0.4471\n",
      "Epoch: 7/20... Training loss: 0.4360\n",
      "Epoch: 7/20... Training loss: 0.4416\n",
      "Epoch: 7/20... Training loss: 0.4433\n",
      "Epoch: 7/20... Training loss: 0.4460\n",
      "Epoch: 7/20... Training loss: 0.4469\n",
      "Epoch: 7/20... Training loss: 0.4367\n",
      "Epoch: 7/20... Training loss: 0.4426\n",
      "Epoch: 7/20... Training loss: 0.4416\n",
      "Epoch: 7/20... Training loss: 0.4482\n",
      "Epoch: 7/20... Training loss: 0.4393\n",
      "Epoch: 7/20... Training loss: 0.4420\n",
      "Epoch: 7/20... Training loss: 0.4425\n",
      "Epoch: 7/20... Training loss: 0.4438\n",
      "Epoch: 7/20... Training loss: 0.4502\n",
      "Epoch: 7/20... Training loss: 0.4430\n",
      "Epoch: 7/20... Training loss: 0.4478\n",
      "Epoch: 7/20... Training loss: 0.4500\n",
      "Epoch: 7/20... Training loss: 0.4415\n",
      "Epoch: 7/20... Training loss: 0.4459\n",
      "Epoch: 7/20... Training loss: 0.4270\n",
      "Epoch: 7/20... Training loss: 0.4415\n",
      "Epoch: 7/20... Training loss: 0.4495\n",
      "Epoch: 7/20... Training loss: 0.4556\n",
      "Epoch: 7/20... Training loss: 0.4384\n",
      "Epoch: 7/20... Training loss: 0.4401\n",
      "Epoch: 7/20... Training loss: 0.4383\n",
      "Epoch: 7/20... Training loss: 0.4409\n",
      "Epoch: 7/20... Training loss: 0.4492\n",
      "Epoch: 7/20... Training loss: 0.4492\n",
      "Epoch: 7/20... Training loss: 0.4400\n",
      "Epoch: 7/20... Training loss: 0.4423\n",
      "Epoch: 7/20... Training loss: 0.4414\n",
      "Epoch: 7/20... Training loss: 0.4490\n",
      "Epoch: 7/20... Training loss: 0.4406\n",
      "Epoch: 7/20... Training loss: 0.4477\n",
      "Epoch: 7/20... Training loss: 0.4364\n",
      "Epoch: 7/20... Training loss: 0.4412\n",
      "Epoch: 7/20... Training loss: 0.4434\n",
      "Epoch: 7/20... Training loss: 0.4460\n",
      "Epoch: 7/20... Training loss: 0.4406\n",
      "Epoch: 7/20... Training loss: 0.4409\n",
      "Epoch: 7/20... Training loss: 0.4484\n",
      "Epoch: 7/20... Training loss: 0.4514\n",
      "Epoch: 7/20... Training loss: 0.4390\n",
      "Epoch: 7/20... Training loss: 0.4405\n",
      "Epoch: 7/20... Training loss: 0.4459\n",
      "Epoch: 7/20... Training loss: 0.4484\n",
      "Epoch: 7/20... Training loss: 0.4365\n",
      "Epoch: 7/20... Training loss: 0.4444\n",
      "Epoch: 7/20... Training loss: 0.4351\n",
      "Epoch: 7/20... Training loss: 0.4431\n",
      "Epoch: 7/20... Training loss: 0.4515\n",
      "Epoch: 7/20... Training loss: 0.4454\n",
      "Epoch: 7/20... Training loss: 0.4413\n",
      "Epoch: 7/20... Training loss: 0.4515\n",
      "Epoch: 7/20... Training loss: 0.4352\n",
      "Epoch: 7/20... Training loss: 0.4424\n",
      "Epoch: 7/20... Training loss: 0.4430\n",
      "Epoch: 7/20... Training loss: 0.4463\n",
      "Epoch: 7/20... Training loss: 0.4454\n",
      "Epoch: 7/20... Training loss: 0.4372\n",
      "Epoch: 7/20... Training loss: 0.4539\n",
      "Epoch: 7/20... Training loss: 0.4496\n",
      "Epoch: 7/20... Training loss: 0.4348\n",
      "Epoch: 7/20... Training loss: 0.4519\n",
      "Epoch: 7/20... Training loss: 0.4472\n",
      "Epoch: 7/20... Training loss: 0.4522\n",
      "Epoch: 7/20... Training loss: 0.4343\n",
      "Epoch: 7/20... Training loss: 0.4449\n",
      "Epoch: 7/20... Training loss: 0.4387\n",
      "Epoch: 7/20... Training loss: 0.4504\n",
      "Epoch: 7/20... Training loss: 0.4510\n",
      "Epoch: 7/20... Training loss: 0.4361\n",
      "Epoch: 7/20... Training loss: 0.4462\n",
      "Epoch: 7/20... Training loss: 0.4366\n",
      "Epoch: 7/20... Training loss: 0.4423\n",
      "Epoch: 7/20... Training loss: 0.4420\n",
      "Epoch: 7/20... Training loss: 0.4493\n",
      "Epoch: 7/20... Training loss: 0.4448\n",
      "Epoch: 7/20... Training loss: 0.4517\n",
      "Epoch: 7/20... Training loss: 0.4499\n",
      "Epoch: 7/20... Training loss: 0.4467\n",
      "Epoch: 7/20... Training loss: 0.4431\n",
      "Epoch: 7/20... Training loss: 0.4509\n",
      "Epoch: 7/20... Training loss: 0.4489\n",
      "Epoch: 7/20... Training loss: 0.4454\n",
      "Epoch: 7/20... Training loss: 0.4343\n",
      "Epoch: 7/20... Training loss: 0.4440\n",
      "Epoch: 7/20... Training loss: 0.4354\n",
      "Epoch: 7/20... Training loss: 0.4358\n",
      "Epoch: 8/20... Training loss: 0.4468\n",
      "Epoch: 8/20... Training loss: 0.4488\n",
      "Epoch: 8/20... Training loss: 0.4434\n",
      "Epoch: 8/20... Training loss: 0.4431\n",
      "Epoch: 8/20... Training loss: 0.4472\n",
      "Epoch: 8/20... Training loss: 0.4476\n",
      "Epoch: 8/20... Training loss: 0.4585\n",
      "Epoch: 8/20... Training loss: 0.4475\n",
      "Epoch: 8/20... Training loss: 0.4473\n",
      "Epoch: 8/20... Training loss: 0.4378\n",
      "Epoch: 8/20... Training loss: 0.4531\n",
      "Epoch: 8/20... Training loss: 0.4489\n",
      "Epoch: 8/20... Training loss: 0.4463\n",
      "Epoch: 8/20... Training loss: 0.4483\n",
      "Epoch: 8/20... Training loss: 0.4441\n",
      "Epoch: 8/20... Training loss: 0.4441\n",
      "Epoch: 8/20... Training loss: 0.4460\n",
      "Epoch: 8/20... Training loss: 0.4443\n",
      "Epoch: 8/20... Training loss: 0.4453\n",
      "Epoch: 8/20... Training loss: 0.4452\n",
      "Epoch: 8/20... Training loss: 0.4412\n",
      "Epoch: 8/20... Training loss: 0.4422\n",
      "Epoch: 8/20... Training loss: 0.4363\n",
      "Epoch: 8/20... Training loss: 0.4371\n",
      "Epoch: 8/20... Training loss: 0.4464\n",
      "Epoch: 8/20... Training loss: 0.4418\n",
      "Epoch: 8/20... Training loss: 0.4450\n",
      "Epoch: 8/20... Training loss: 0.4460\n",
      "Epoch: 8/20... Training loss: 0.4441\n",
      "Epoch: 8/20... Training loss: 0.4456\n",
      "Epoch: 8/20... Training loss: 0.4412\n",
      "Epoch: 8/20... Training loss: 0.4348\n",
      "Epoch: 8/20... Training loss: 0.4505\n",
      "Epoch: 8/20... Training loss: 0.4498\n",
      "Epoch: 8/20... Training loss: 0.4475\n",
      "Epoch: 8/20... Training loss: 0.4438\n",
      "Epoch: 8/20... Training loss: 0.4443\n",
      "Epoch: 8/20... Training loss: 0.4454\n",
      "Epoch: 8/20... Training loss: 0.4285\n",
      "Epoch: 8/20... Training loss: 0.4471\n",
      "Epoch: 8/20... Training loss: 0.4465\n",
      "Epoch: 8/20... Training loss: 0.4467\n",
      "Epoch: 8/20... Training loss: 0.4470\n",
      "Epoch: 8/20... Training loss: 0.4482\n",
      "Epoch: 8/20... Training loss: 0.4456\n",
      "Epoch: 8/20... Training loss: 0.4421\n",
      "Epoch: 8/20... Training loss: 0.4472\n",
      "Epoch: 8/20... Training loss: 0.4393\n",
      "Epoch: 8/20... Training loss: 0.4527\n",
      "Epoch: 8/20... Training loss: 0.4437\n",
      "Epoch: 8/20... Training loss: 0.4340\n",
      "Epoch: 8/20... Training loss: 0.4467\n",
      "Epoch: 8/20... Training loss: 0.4384\n",
      "Epoch: 8/20... Training loss: 0.4325\n",
      "Epoch: 8/20... Training loss: 0.4422\n",
      "Epoch: 8/20... Training loss: 0.4531\n",
      "Epoch: 8/20... Training loss: 0.4450\n",
      "Epoch: 8/20... Training loss: 0.4460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20... Training loss: 0.4468\n",
      "Epoch: 8/20... Training loss: 0.4436\n",
      "Epoch: 8/20... Training loss: 0.4401\n",
      "Epoch: 8/20... Training loss: 0.4383\n",
      "Epoch: 8/20... Training loss: 0.4389\n",
      "Epoch: 8/20... Training loss: 0.4375\n",
      "Epoch: 8/20... Training loss: 0.4461\n",
      "Epoch: 8/20... Training loss: 0.4453\n",
      "Epoch: 8/20... Training loss: 0.4531\n",
      "Epoch: 8/20... Training loss: 0.4540\n",
      "Epoch: 8/20... Training loss: 0.4451\n",
      "Epoch: 8/20... Training loss: 0.4424\n",
      "Epoch: 8/20... Training loss: 0.4426\n",
      "Epoch: 8/20... Training loss: 0.4399\n",
      "Epoch: 8/20... Training loss: 0.4351\n",
      "Epoch: 8/20... Training loss: 0.4612\n",
      "Epoch: 8/20... Training loss: 0.4520\n",
      "Epoch: 8/20... Training loss: 0.4463\n",
      "Epoch: 8/20... Training loss: 0.4430\n",
      "Epoch: 8/20... Training loss: 0.4393\n",
      "Epoch: 8/20... Training loss: 0.4436\n",
      "Epoch: 8/20... Training loss: 0.4456\n",
      "Epoch: 8/20... Training loss: 0.4415\n",
      "Epoch: 8/20... Training loss: 0.4476\n",
      "Epoch: 8/20... Training loss: 0.4423\n",
      "Epoch: 8/20... Training loss: 0.4467\n",
      "Epoch: 8/20... Training loss: 0.4442\n",
      "Epoch: 8/20... Training loss: 0.4474\n",
      "Epoch: 8/20... Training loss: 0.4399\n",
      "Epoch: 8/20... Training loss: 0.4590\n",
      "Epoch: 8/20... Training loss: 0.4418\n",
      "Epoch: 8/20... Training loss: 0.4383\n",
      "Epoch: 8/20... Training loss: 0.4442\n",
      "Epoch: 8/20... Training loss: 0.4511\n",
      "Epoch: 8/20... Training loss: 0.4308\n",
      "Epoch: 8/20... Training loss: 0.4424\n",
      "Epoch: 8/20... Training loss: 0.4400\n",
      "Epoch: 8/20... Training loss: 0.4480\n",
      "Epoch: 8/20... Training loss: 0.4442\n",
      "Epoch: 8/20... Training loss: 0.4421\n",
      "Epoch: 8/20... Training loss: 0.4391\n",
      "Epoch: 8/20... Training loss: 0.4447\n",
      "Epoch: 8/20... Training loss: 0.4361\n",
      "Epoch: 8/20... Training loss: 0.4439\n",
      "Epoch: 8/20... Training loss: 0.4469\n",
      "Epoch: 8/20... Training loss: 0.4410\n",
      "Epoch: 8/20... Training loss: 0.4461\n",
      "Epoch: 8/20... Training loss: 0.4493\n",
      "Epoch: 8/20... Training loss: 0.4439\n",
      "Epoch: 8/20... Training loss: 0.4302\n",
      "Epoch: 8/20... Training loss: 0.4413\n",
      "Epoch: 8/20... Training loss: 0.4545\n",
      "Epoch: 8/20... Training loss: 0.4361\n",
      "Epoch: 8/20... Training loss: 0.4436\n",
      "Epoch: 8/20... Training loss: 0.4441\n",
      "Epoch: 8/20... Training loss: 0.4356\n",
      "Epoch: 8/20... Training loss: 0.4397\n",
      "Epoch: 8/20... Training loss: 0.4419\n",
      "Epoch: 8/20... Training loss: 0.4398\n",
      "Epoch: 8/20... Training loss: 0.4424\n",
      "Epoch: 8/20... Training loss: 0.4530\n",
      "Epoch: 8/20... Training loss: 0.4499\n",
      "Epoch: 8/20... Training loss: 0.4466\n",
      "Epoch: 8/20... Training loss: 0.4439\n",
      "Epoch: 8/20... Training loss: 0.4461\n",
      "Epoch: 8/20... Training loss: 0.4392\n",
      "Epoch: 8/20... Training loss: 0.4393\n",
      "Epoch: 8/20... Training loss: 0.4340\n",
      "Epoch: 8/20... Training loss: 0.4433\n",
      "Epoch: 8/20... Training loss: 0.4477\n",
      "Epoch: 8/20... Training loss: 0.4489\n",
      "Epoch: 8/20... Training loss: 0.4469\n",
      "Epoch: 8/20... Training loss: 0.4249\n",
      "Epoch: 8/20... Training loss: 0.4440\n",
      "Epoch: 8/20... Training loss: 0.4381\n",
      "Epoch: 8/20... Training loss: 0.4494\n",
      "Epoch: 8/20... Training loss: 0.4388\n",
      "Epoch: 8/20... Training loss: 0.4504\n",
      "Epoch: 8/20... Training loss: 0.4452\n",
      "Epoch: 8/20... Training loss: 0.4320\n",
      "Epoch: 8/20... Training loss: 0.4486\n",
      "Epoch: 8/20... Training loss: 0.4422\n",
      "Epoch: 8/20... Training loss: 0.4448\n",
      "Epoch: 8/20... Training loss: 0.4559\n",
      "Epoch: 8/20... Training loss: 0.4472\n",
      "Epoch: 8/20... Training loss: 0.4352\n",
      "Epoch: 8/20... Training loss: 0.4437\n",
      "Epoch: 8/20... Training loss: 0.4428\n",
      "Epoch: 8/20... Training loss: 0.4374\n",
      "Epoch: 8/20... Training loss: 0.4457\n",
      "Epoch: 8/20... Training loss: 0.4477\n",
      "Epoch: 8/20... Training loss: 0.4404\n",
      "Epoch: 8/20... Training loss: 0.4556\n",
      "Epoch: 8/20... Training loss: 0.4423\n",
      "Epoch: 8/20... Training loss: 0.4487\n",
      "Epoch: 8/20... Training loss: 0.4486\n",
      "Epoch: 8/20... Training loss: 0.4520\n",
      "Epoch: 8/20... Training loss: 0.4453\n",
      "Epoch: 8/20... Training loss: 0.4472\n",
      "Epoch: 8/20... Training loss: 0.4474\n",
      "Epoch: 8/20... Training loss: 0.4532\n",
      "Epoch: 8/20... Training loss: 0.4518\n",
      "Epoch: 8/20... Training loss: 0.4463\n",
      "Epoch: 8/20... Training loss: 0.4458\n",
      "Epoch: 8/20... Training loss: 0.4448\n",
      "Epoch: 8/20... Training loss: 0.4436\n",
      "Epoch: 8/20... Training loss: 0.4310\n",
      "Epoch: 8/20... Training loss: 0.4482\n",
      "Epoch: 8/20... Training loss: 0.4415\n",
      "Epoch: 8/20... Training loss: 0.4542\n",
      "Epoch: 8/20... Training loss: 0.4499\n",
      "Epoch: 8/20... Training loss: 0.4401\n",
      "Epoch: 8/20... Training loss: 0.4465\n",
      "Epoch: 8/20... Training loss: 0.4461\n",
      "Epoch: 8/20... Training loss: 0.4425\n",
      "Epoch: 8/20... Training loss: 0.4488\n",
      "Epoch: 8/20... Training loss: 0.4409\n",
      "Epoch: 8/20... Training loss: 0.4399\n",
      "Epoch: 8/20... Training loss: 0.4399\n",
      "Epoch: 8/20... Training loss: 0.4438\n",
      "Epoch: 8/20... Training loss: 0.4513\n",
      "Epoch: 8/20... Training loss: 0.4414\n",
      "Epoch: 8/20... Training loss: 0.4525\n",
      "Epoch: 8/20... Training loss: 0.4462\n",
      "Epoch: 8/20... Training loss: 0.4405\n",
      "Epoch: 9/20... Training loss: 0.4373\n",
      "Epoch: 9/20... Training loss: 0.4533\n",
      "Epoch: 9/20... Training loss: 0.4432\n",
      "Epoch: 9/20... Training loss: 0.4482\n",
      "Epoch: 9/20... Training loss: 0.4483\n",
      "Epoch: 9/20... Training loss: 0.4510\n",
      "Epoch: 9/20... Training loss: 0.4498\n",
      "Epoch: 9/20... Training loss: 0.4461\n",
      "Epoch: 9/20... Training loss: 0.4362\n",
      "Epoch: 9/20... Training loss: 0.4378\n",
      "Epoch: 9/20... Training loss: 0.4411\n",
      "Epoch: 9/20... Training loss: 0.4432\n",
      "Epoch: 9/20... Training loss: 0.4459\n",
      "Epoch: 9/20... Training loss: 0.4535\n",
      "Epoch: 9/20... Training loss: 0.4488\n",
      "Epoch: 9/20... Training loss: 0.4435\n",
      "Epoch: 9/20... Training loss: 0.4381\n",
      "Epoch: 9/20... Training loss: 0.4387\n",
      "Epoch: 9/20... Training loss: 0.4451\n",
      "Epoch: 9/20... Training loss: 0.4356\n",
      "Epoch: 9/20... Training loss: 0.4369\n",
      "Epoch: 9/20... Training loss: 0.4459\n",
      "Epoch: 9/20... Training loss: 0.4414\n",
      "Epoch: 9/20... Training loss: 0.4477\n",
      "Epoch: 9/20... Training loss: 0.4403\n",
      "Epoch: 9/20... Training loss: 0.4391\n",
      "Epoch: 9/20... Training loss: 0.4434\n",
      "Epoch: 9/20... Training loss: 0.4495\n",
      "Epoch: 9/20... Training loss: 0.4459\n",
      "Epoch: 9/20... Training loss: 0.4508\n",
      "Epoch: 9/20... Training loss: 0.4403\n",
      "Epoch: 9/20... Training loss: 0.4471\n",
      "Epoch: 9/20... Training loss: 0.4357\n",
      "Epoch: 9/20... Training loss: 0.4406\n",
      "Epoch: 9/20... Training loss: 0.4437\n",
      "Epoch: 9/20... Training loss: 0.4514\n",
      "Epoch: 9/20... Training loss: 0.4482\n",
      "Epoch: 9/20... Training loss: 0.4453\n",
      "Epoch: 9/20... Training loss: 0.4418\n",
      "Epoch: 9/20... Training loss: 0.4422\n",
      "Epoch: 9/20... Training loss: 0.4424\n",
      "Epoch: 9/20... Training loss: 0.4469\n",
      "Epoch: 9/20... Training loss: 0.4371\n",
      "Epoch: 9/20... Training loss: 0.4401\n",
      "Epoch: 9/20... Training loss: 0.4411\n",
      "Epoch: 9/20... Training loss: 0.4382\n",
      "Epoch: 9/20... Training loss: 0.4408\n",
      "Epoch: 9/20... Training loss: 0.4419\n",
      "Epoch: 9/20... Training loss: 0.4525\n",
      "Epoch: 9/20... Training loss: 0.4420\n",
      "Epoch: 9/20... Training loss: 0.4263\n",
      "Epoch: 9/20... Training loss: 0.4523\n",
      "Epoch: 9/20... Training loss: 0.4473\n",
      "Epoch: 9/20... Training loss: 0.4392\n",
      "Epoch: 9/20... Training loss: 0.4467\n",
      "Epoch: 9/20... Training loss: 0.4391\n",
      "Epoch: 9/20... Training loss: 0.4482\n",
      "Epoch: 9/20... Training loss: 0.4491\n",
      "Epoch: 9/20... Training loss: 0.4499\n",
      "Epoch: 9/20... Training loss: 0.4385\n",
      "Epoch: 9/20... Training loss: 0.4379\n",
      "Epoch: 9/20... Training loss: 0.4440\n",
      "Epoch: 9/20... Training loss: 0.4347\n",
      "Epoch: 9/20... Training loss: 0.4464\n",
      "Epoch: 9/20... Training loss: 0.4406\n",
      "Epoch: 9/20... Training loss: 0.4564\n",
      "Epoch: 9/20... Training loss: 0.4465\n",
      "Epoch: 9/20... Training loss: 0.4478\n",
      "Epoch: 9/20... Training loss: 0.4377\n",
      "Epoch: 9/20... Training loss: 0.4347\n",
      "Epoch: 9/20... Training loss: 0.4470\n",
      "Epoch: 9/20... Training loss: 0.4425\n",
      "Epoch: 9/20... Training loss: 0.4432\n",
      "Epoch: 9/20... Training loss: 0.4409\n",
      "Epoch: 9/20... Training loss: 0.4385\n",
      "Epoch: 9/20... Training loss: 0.4462\n",
      "Epoch: 9/20... Training loss: 0.4470\n",
      "Epoch: 9/20... Training loss: 0.4477\n",
      "Epoch: 9/20... Training loss: 0.4491\n",
      "Epoch: 9/20... Training loss: 0.4391\n",
      "Epoch: 9/20... Training loss: 0.4433\n",
      "Epoch: 9/20... Training loss: 0.4484\n",
      "Epoch: 9/20... Training loss: 0.4501\n",
      "Epoch: 9/20... Training loss: 0.4371\n",
      "Epoch: 9/20... Training loss: 0.4451\n",
      "Epoch: 9/20... Training loss: 0.4420\n",
      "Epoch: 9/20... Training loss: 0.4456\n",
      "Epoch: 9/20... Training loss: 0.4518\n",
      "Epoch: 9/20... Training loss: 0.4381\n",
      "Epoch: 9/20... Training loss: 0.4396\n",
      "Epoch: 9/20... Training loss: 0.4344\n",
      "Epoch: 9/20... Training loss: 0.4380\n",
      "Epoch: 9/20... Training loss: 0.4426\n",
      "Epoch: 9/20... Training loss: 0.4430\n",
      "Epoch: 9/20... Training loss: 0.4412\n",
      "Epoch: 9/20... Training loss: 0.4474\n",
      "Epoch: 9/20... Training loss: 0.4548\n",
      "Epoch: 9/20... Training loss: 0.4402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20... Training loss: 0.4530\n",
      "Epoch: 9/20... Training loss: 0.4488\n",
      "Epoch: 9/20... Training loss: 0.4473\n",
      "Epoch: 9/20... Training loss: 0.4424\n",
      "Epoch: 9/20... Training loss: 0.4408\n",
      "Epoch: 9/20... Training loss: 0.4379\n",
      "Epoch: 9/20... Training loss: 0.4419\n",
      "Epoch: 9/20... Training loss: 0.4463\n",
      "Epoch: 9/20... Training loss: 0.4479\n",
      "Epoch: 9/20... Training loss: 0.4348\n",
      "Epoch: 9/20... Training loss: 0.4514\n",
      "Epoch: 9/20... Training loss: 0.4387\n",
      "Epoch: 9/20... Training loss: 0.4479\n",
      "Epoch: 9/20... Training loss: 0.4422\n",
      "Epoch: 9/20... Training loss: 0.4494\n",
      "Epoch: 9/20... Training loss: 0.4398\n",
      "Epoch: 9/20... Training loss: 0.4494\n",
      "Epoch: 9/20... Training loss: 0.4534\n",
      "Epoch: 9/20... Training loss: 0.4351\n",
      "Epoch: 9/20... Training loss: 0.4447\n",
      "Epoch: 9/20... Training loss: 0.4372\n",
      "Epoch: 9/20... Training loss: 0.4511\n",
      "Epoch: 9/20... Training loss: 0.4429\n",
      "Epoch: 9/20... Training loss: 0.4346\n",
      "Epoch: 9/20... Training loss: 0.4419\n",
      "Epoch: 9/20... Training loss: 0.4507\n",
      "Epoch: 9/20... Training loss: 0.4421\n",
      "Epoch: 9/20... Training loss: 0.4446\n",
      "Epoch: 9/20... Training loss: 0.4443\n",
      "Epoch: 9/20... Training loss: 0.4425\n",
      "Epoch: 9/20... Training loss: 0.4414\n",
      "Epoch: 9/20... Training loss: 0.4461\n",
      "Epoch: 9/20... Training loss: 0.4439\n",
      "Epoch: 9/20... Training loss: 0.4392\n",
      "Epoch: 9/20... Training loss: 0.4484\n",
      "Epoch: 9/20... Training loss: 0.4448\n",
      "Epoch: 9/20... Training loss: 0.4415\n",
      "Epoch: 9/20... Training loss: 0.4428\n",
      "Epoch: 9/20... Training loss: 0.4474\n",
      "Epoch: 9/20... Training loss: 0.4455\n",
      "Epoch: 9/20... Training loss: 0.4446\n",
      "Epoch: 9/20... Training loss: 0.4386\n",
      "Epoch: 9/20... Training loss: 0.4508\n",
      "Epoch: 9/20... Training loss: 0.4443\n",
      "Epoch: 9/20... Training loss: 0.4402\n",
      "Epoch: 9/20... Training loss: 0.4475\n",
      "Epoch: 9/20... Training loss: 0.4468\n",
      "Epoch: 9/20... Training loss: 0.4478\n",
      "Epoch: 9/20... Training loss: 0.4368\n",
      "Epoch: 9/20... Training loss: 0.4324\n",
      "Epoch: 9/20... Training loss: 0.4435\n",
      "Epoch: 9/20... Training loss: 0.4443\n",
      "Epoch: 9/20... Training loss: 0.4379\n",
      "Epoch: 9/20... Training loss: 0.4494\n",
      "Epoch: 9/20... Training loss: 0.4408\n",
      "Epoch: 9/20... Training loss: 0.4508\n",
      "Epoch: 9/20... Training loss: 0.4430\n",
      "Epoch: 9/20... Training loss: 0.4438\n",
      "Epoch: 9/20... Training loss: 0.4521\n",
      "Epoch: 9/20... Training loss: 0.4516\n",
      "Epoch: 9/20... Training loss: 0.4435\n",
      "Epoch: 9/20... Training loss: 0.4482\n",
      "Epoch: 9/20... Training loss: 0.4414\n",
      "Epoch: 9/20... Training loss: 0.4452\n",
      "Epoch: 9/20... Training loss: 0.4434\n",
      "Epoch: 9/20... Training loss: 0.4319\n",
      "Epoch: 9/20... Training loss: 0.4405\n",
      "Epoch: 9/20... Training loss: 0.4359\n",
      "Epoch: 9/20... Training loss: 0.4366\n",
      "Epoch: 9/20... Training loss: 0.4445\n",
      "Epoch: 9/20... Training loss: 0.4470\n",
      "Epoch: 9/20... Training loss: 0.4441\n",
      "Epoch: 9/20... Training loss: 0.4392\n",
      "Epoch: 9/20... Training loss: 0.4472\n",
      "Epoch: 9/20... Training loss: 0.4349\n",
      "Epoch: 9/20... Training loss: 0.4511\n",
      "Epoch: 9/20... Training loss: 0.4440\n",
      "Epoch: 9/20... Training loss: 0.4453\n",
      "Epoch: 9/20... Training loss: 0.4463\n",
      "Epoch: 9/20... Training loss: 0.4428\n",
      "Epoch: 9/20... Training loss: 0.4545\n",
      "Epoch: 9/20... Training loss: 0.4457\n",
      "Epoch: 9/20... Training loss: 0.4448\n",
      "Epoch: 9/20... Training loss: 0.4466\n",
      "Epoch: 9/20... Training loss: 0.4416\n",
      "Epoch: 10/20... Training loss: 0.4445\n",
      "Epoch: 10/20... Training loss: 0.4463\n",
      "Epoch: 10/20... Training loss: 0.4517\n",
      "Epoch: 10/20... Training loss: 0.4375\n",
      "Epoch: 10/20... Training loss: 0.4433\n",
      "Epoch: 10/20... Training loss: 0.4480\n",
      "Epoch: 10/20... Training loss: 0.4495\n",
      "Epoch: 10/20... Training loss: 0.4356\n",
      "Epoch: 10/20... Training loss: 0.4507\n",
      "Epoch: 10/20... Training loss: 0.4457\n",
      "Epoch: 10/20... Training loss: 0.4303\n",
      "Epoch: 10/20... Training loss: 0.4449\n",
      "Epoch: 10/20... Training loss: 0.4428\n",
      "Epoch: 10/20... Training loss: 0.4463\n",
      "Epoch: 10/20... Training loss: 0.4455\n",
      "Epoch: 10/20... Training loss: 0.4464\n",
      "Epoch: 10/20... Training loss: 0.4451\n",
      "Epoch: 10/20... Training loss: 0.4352\n",
      "Epoch: 10/20... Training loss: 0.4410\n",
      "Epoch: 10/20... Training loss: 0.4431\n",
      "Epoch: 10/20... Training loss: 0.4385\n",
      "Epoch: 10/20... Training loss: 0.4411\n",
      "Epoch: 10/20... Training loss: 0.4409\n",
      "Epoch: 10/20... Training loss: 0.4460\n",
      "Epoch: 10/20... Training loss: 0.4351\n",
      "Epoch: 10/20... Training loss: 0.4501\n",
      "Epoch: 10/20... Training loss: 0.4516\n",
      "Epoch: 10/20... Training loss: 0.4423\n",
      "Epoch: 10/20... Training loss: 0.4456\n",
      "Epoch: 10/20... Training loss: 0.4438\n",
      "Epoch: 10/20... Training loss: 0.4319\n",
      "Epoch: 10/20... Training loss: 0.4479\n",
      "Epoch: 10/20... Training loss: 0.4360\n",
      "Epoch: 10/20... Training loss: 0.4433\n",
      "Epoch: 10/20... Training loss: 0.4406\n",
      "Epoch: 10/20... Training loss: 0.4339\n",
      "Epoch: 10/20... Training loss: 0.4401\n",
      "Epoch: 10/20... Training loss: 0.4439\n",
      "Epoch: 10/20... Training loss: 0.4466\n",
      "Epoch: 10/20... Training loss: 0.4409\n",
      "Epoch: 10/20... Training loss: 0.4381\n",
      "Epoch: 10/20... Training loss: 0.4381\n",
      "Epoch: 10/20... Training loss: 0.4411\n",
      "Epoch: 10/20... Training loss: 0.4526\n",
      "Epoch: 10/20... Training loss: 0.4516\n",
      "Epoch: 10/20... Training loss: 0.4483\n",
      "Epoch: 10/20... Training loss: 0.4531\n",
      "Epoch: 10/20... Training loss: 0.4350\n",
      "Epoch: 10/20... Training loss: 0.4432\n",
      "Epoch: 10/20... Training loss: 0.4405\n",
      "Epoch: 10/20... Training loss: 0.4349\n",
      "Epoch: 10/20... Training loss: 0.4437\n",
      "Epoch: 10/20... Training loss: 0.4418\n",
      "Epoch: 10/20... Training loss: 0.4512\n",
      "Epoch: 10/20... Training loss: 0.4406\n",
      "Epoch: 10/20... Training loss: 0.4410\n",
      "Epoch: 10/20... Training loss: 0.4359\n",
      "Epoch: 10/20... Training loss: 0.4462\n",
      "Epoch: 10/20... Training loss: 0.4416\n",
      "Epoch: 10/20... Training loss: 0.4398\n",
      "Epoch: 10/20... Training loss: 0.4478\n",
      "Epoch: 10/20... Training loss: 0.4488\n",
      "Epoch: 10/20... Training loss: 0.4451\n",
      "Epoch: 10/20... Training loss: 0.4498\n",
      "Epoch: 10/20... Training loss: 0.4489\n",
      "Epoch: 10/20... Training loss: 0.4454\n",
      "Epoch: 10/20... Training loss: 0.4543\n",
      "Epoch: 10/20... Training loss: 0.4422\n",
      "Epoch: 10/20... Training loss: 0.4420\n",
      "Epoch: 10/20... Training loss: 0.4391\n",
      "Epoch: 10/20... Training loss: 0.4487\n",
      "Epoch: 10/20... Training loss: 0.4361\n",
      "Epoch: 10/20... Training loss: 0.4531\n",
      "Epoch: 10/20... Training loss: 0.4506\n",
      "Epoch: 10/20... Training loss: 0.4433\n",
      "Epoch: 10/20... Training loss: 0.4535\n",
      "Epoch: 10/20... Training loss: 0.4431\n",
      "Epoch: 10/20... Training loss: 0.4489\n",
      "Epoch: 10/20... Training loss: 0.4470\n",
      "Epoch: 10/20... Training loss: 0.4339\n",
      "Epoch: 10/20... Training loss: 0.4495\n",
      "Epoch: 10/20... Training loss: 0.4389\n",
      "Epoch: 10/20... Training loss: 0.4430\n",
      "Epoch: 10/20... Training loss: 0.4417\n",
      "Epoch: 10/20... Training loss: 0.4431\n",
      "Epoch: 10/20... Training loss: 0.4432\n",
      "Epoch: 10/20... Training loss: 0.4503\n",
      "Epoch: 10/20... Training loss: 0.4412\n",
      "Epoch: 10/20... Training loss: 0.4253\n",
      "Epoch: 10/20... Training loss: 0.4412\n",
      "Epoch: 10/20... Training loss: 0.4477\n",
      "Epoch: 10/20... Training loss: 0.4463\n",
      "Epoch: 10/20... Training loss: 0.4292\n",
      "Epoch: 10/20... Training loss: 0.4431\n",
      "Epoch: 10/20... Training loss: 0.4379\n",
      "Epoch: 10/20... Training loss: 0.4449\n",
      "Epoch: 10/20... Training loss: 0.4475\n",
      "Epoch: 10/20... Training loss: 0.4507\n",
      "Epoch: 10/20... Training loss: 0.4505\n",
      "Epoch: 10/20... Training loss: 0.4494\n",
      "Epoch: 10/20... Training loss: 0.4483\n",
      "Epoch: 10/20... Training loss: 0.4370\n",
      "Epoch: 10/20... Training loss: 0.4475\n",
      "Epoch: 10/20... Training loss: 0.4312\n",
      "Epoch: 10/20... Training loss: 0.4398\n",
      "Epoch: 10/20... Training loss: 0.4365\n",
      "Epoch: 10/20... Training loss: 0.4394\n",
      "Epoch: 10/20... Training loss: 0.4462\n",
      "Epoch: 10/20... Training loss: 0.4392\n",
      "Epoch: 10/20... Training loss: 0.4495\n",
      "Epoch: 10/20... Training loss: 0.4275\n",
      "Epoch: 10/20... Training loss: 0.4480\n",
      "Epoch: 10/20... Training loss: 0.4413\n",
      "Epoch: 10/20... Training loss: 0.4436\n",
      "Epoch: 10/20... Training loss: 0.4339\n",
      "Epoch: 10/20... Training loss: 0.4446\n",
      "Epoch: 10/20... Training loss: 0.4561\n",
      "Epoch: 10/20... Training loss: 0.4410\n",
      "Epoch: 10/20... Training loss: 0.4477\n",
      "Epoch: 10/20... Training loss: 0.4384\n",
      "Epoch: 10/20... Training loss: 0.4419\n",
      "Epoch: 10/20... Training loss: 0.4512\n",
      "Epoch: 10/20... Training loss: 0.4435\n",
      "Epoch: 10/20... Training loss: 0.4399\n",
      "Epoch: 10/20... Training loss: 0.4422\n",
      "Epoch: 10/20... Training loss: 0.4498\n",
      "Epoch: 10/20... Training loss: 0.4390\n",
      "Epoch: 10/20... Training loss: 0.4367\n",
      "Epoch: 10/20... Training loss: 0.4423\n",
      "Epoch: 10/20... Training loss: 0.4390\n",
      "Epoch: 10/20... Training loss: 0.4548\n",
      "Epoch: 10/20... Training loss: 0.4371\n",
      "Epoch: 10/20... Training loss: 0.4406\n",
      "Epoch: 10/20... Training loss: 0.4331\n",
      "Epoch: 10/20... Training loss: 0.4360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Training loss: 0.4530\n",
      "Epoch: 10/20... Training loss: 0.4495\n",
      "Epoch: 10/20... Training loss: 0.4460\n",
      "Epoch: 10/20... Training loss: 0.4387\n",
      "Epoch: 10/20... Training loss: 0.4352\n",
      "Epoch: 10/20... Training loss: 0.4513\n",
      "Epoch: 10/20... Training loss: 0.4491\n",
      "Epoch: 10/20... Training loss: 0.4351\n",
      "Epoch: 10/20... Training loss: 0.4489\n",
      "Epoch: 10/20... Training loss: 0.4474\n",
      "Epoch: 10/20... Training loss: 0.4463\n",
      "Epoch: 10/20... Training loss: 0.4410\n",
      "Epoch: 10/20... Training loss: 0.4384\n",
      "Epoch: 10/20... Training loss: 0.4459\n",
      "Epoch: 10/20... Training loss: 0.4428\n",
      "Epoch: 10/20... Training loss: 0.4436\n",
      "Epoch: 10/20... Training loss: 0.4477\n",
      "Epoch: 10/20... Training loss: 0.4381\n",
      "Epoch: 10/20... Training loss: 0.4523\n",
      "Epoch: 10/20... Training loss: 0.4444\n",
      "Epoch: 10/20... Training loss: 0.4436\n",
      "Epoch: 10/20... Training loss: 0.4509\n",
      "Epoch: 10/20... Training loss: 0.4447\n",
      "Epoch: 10/20... Training loss: 0.4382\n",
      "Epoch: 10/20... Training loss: 0.4458\n",
      "Epoch: 10/20... Training loss: 0.4359\n",
      "Epoch: 10/20... Training loss: 0.4452\n",
      "Epoch: 10/20... Training loss: 0.4351\n",
      "Epoch: 10/20... Training loss: 0.4527\n",
      "Epoch: 10/20... Training loss: 0.4532\n",
      "Epoch: 10/20... Training loss: 0.4398\n",
      "Epoch: 10/20... Training loss: 0.4407\n",
      "Epoch: 10/20... Training loss: 0.4307\n",
      "Epoch: 10/20... Training loss: 0.4502\n",
      "Epoch: 10/20... Training loss: 0.4430\n",
      "Epoch: 10/20... Training loss: 0.4446\n",
      "Epoch: 10/20... Training loss: 0.4355\n",
      "Epoch: 10/20... Training loss: 0.4356\n",
      "Epoch: 10/20... Training loss: 0.4434\n",
      "Epoch: 10/20... Training loss: 0.4468\n",
      "Epoch: 10/20... Training loss: 0.4456\n",
      "Epoch: 10/20... Training loss: 0.4379\n",
      "Epoch: 10/20... Training loss: 0.4455\n",
      "Epoch: 10/20... Training loss: 0.4509\n",
      "Epoch: 10/20... Training loss: 0.4442\n",
      "Epoch: 10/20... Training loss: 0.4391\n",
      "Epoch: 10/20... Training loss: 0.4343\n",
      "Epoch: 10/20... Training loss: 0.4450\n",
      "Epoch: 11/20... Training loss: 0.4394\n",
      "Epoch: 11/20... Training loss: 0.4480\n",
      "Epoch: 11/20... Training loss: 0.4469\n",
      "Epoch: 11/20... Training loss: 0.4439\n",
      "Epoch: 11/20... Training loss: 0.4422\n",
      "Epoch: 11/20... Training loss: 0.4411\n",
      "Epoch: 11/20... Training loss: 0.4451\n",
      "Epoch: 11/20... Training loss: 0.4510\n",
      "Epoch: 11/20... Training loss: 0.4364\n",
      "Epoch: 11/20... Training loss: 0.4374\n",
      "Epoch: 11/20... Training loss: 0.4399\n",
      "Epoch: 11/20... Training loss: 0.4313\n",
      "Epoch: 11/20... Training loss: 0.4411\n",
      "Epoch: 11/20... Training loss: 0.4426\n",
      "Epoch: 11/20... Training loss: 0.4481\n",
      "Epoch: 11/20... Training loss: 0.4412\n",
      "Epoch: 11/20... Training loss: 0.4329\n",
      "Epoch: 11/20... Training loss: 0.4385\n",
      "Epoch: 11/20... Training loss: 0.4410\n",
      "Epoch: 11/20... Training loss: 0.4359\n",
      "Epoch: 11/20... Training loss: 0.4430\n",
      "Epoch: 11/20... Training loss: 0.4424\n",
      "Epoch: 11/20... Training loss: 0.4425\n",
      "Epoch: 11/20... Training loss: 0.4487\n",
      "Epoch: 11/20... Training loss: 0.4433\n",
      "Epoch: 11/20... Training loss: 0.4335\n",
      "Epoch: 11/20... Training loss: 0.4448\n",
      "Epoch: 11/20... Training loss: 0.4392\n",
      "Epoch: 11/20... Training loss: 0.4370\n",
      "Epoch: 11/20... Training loss: 0.4494\n",
      "Epoch: 11/20... Training loss: 0.4436\n",
      "Epoch: 11/20... Training loss: 0.4384\n",
      "Epoch: 11/20... Training loss: 0.4559\n",
      "Epoch: 11/20... Training loss: 0.4413\n",
      "Epoch: 11/20... Training loss: 0.4399\n",
      "Epoch: 11/20... Training loss: 0.4514\n",
      "Epoch: 11/20... Training loss: 0.4352\n",
      "Epoch: 11/20... Training loss: 0.4493\n",
      "Epoch: 11/20... Training loss: 0.4355\n",
      "Epoch: 11/20... Training loss: 0.4349\n",
      "Epoch: 11/20... Training loss: 0.4429\n",
      "Epoch: 11/20... Training loss: 0.4389\n",
      "Epoch: 11/20... Training loss: 0.4475\n",
      "Epoch: 11/20... Training loss: 0.4456\n",
      "Epoch: 11/20... Training loss: 0.4456\n",
      "Epoch: 11/20... Training loss: 0.4444\n",
      "Epoch: 11/20... Training loss: 0.4327\n",
      "Epoch: 11/20... Training loss: 0.4426\n",
      "Epoch: 11/20... Training loss: 0.4443\n",
      "Epoch: 11/20... Training loss: 0.4504\n",
      "Epoch: 11/20... Training loss: 0.4400\n",
      "Epoch: 11/20... Training loss: 0.4380\n",
      "Epoch: 11/20... Training loss: 0.4475\n",
      "Epoch: 11/20... Training loss: 0.4413\n",
      "Epoch: 11/20... Training loss: 0.4445\n",
      "Epoch: 11/20... Training loss: 0.4486\n",
      "Epoch: 11/20... Training loss: 0.4476\n",
      "Epoch: 11/20... Training loss: 0.4379\n",
      "Epoch: 11/20... Training loss: 0.4502\n",
      "Epoch: 11/20... Training loss: 0.4481\n",
      "Epoch: 11/20... Training loss: 0.4466\n",
      "Epoch: 11/20... Training loss: 0.4415\n",
      "Epoch: 11/20... Training loss: 0.4438\n",
      "Epoch: 11/20... Training loss: 0.4383\n",
      "Epoch: 11/20... Training loss: 0.4428\n",
      "Epoch: 11/20... Training loss: 0.4479\n",
      "Epoch: 11/20... Training loss: 0.4428\n",
      "Epoch: 11/20... Training loss: 0.4454\n",
      "Epoch: 11/20... Training loss: 0.4412\n",
      "Epoch: 11/20... Training loss: 0.4434\n",
      "Epoch: 11/20... Training loss: 0.4550\n",
      "Epoch: 11/20... Training loss: 0.4502\n",
      "Epoch: 11/20... Training loss: 0.4490\n",
      "Epoch: 11/20... Training loss: 0.4548\n",
      "Epoch: 11/20... Training loss: 0.4363\n",
      "Epoch: 11/20... Training loss: 0.4420\n",
      "Epoch: 11/20... Training loss: 0.4465\n",
      "Epoch: 11/20... Training loss: 0.4523\n",
      "Epoch: 11/20... Training loss: 0.4479\n",
      "Epoch: 11/20... Training loss: 0.4373\n",
      "Epoch: 11/20... Training loss: 0.4328\n",
      "Epoch: 11/20... Training loss: 0.4373\n",
      "Epoch: 11/20... Training loss: 0.4496\n",
      "Epoch: 11/20... Training loss: 0.4358\n",
      "Epoch: 11/20... Training loss: 0.4453\n",
      "Epoch: 11/20... Training loss: 0.4390\n",
      "Epoch: 11/20... Training loss: 0.4451\n",
      "Epoch: 11/20... Training loss: 0.4421\n",
      "Epoch: 11/20... Training loss: 0.4487\n",
      "Epoch: 11/20... Training loss: 0.4435\n",
      "Epoch: 11/20... Training loss: 0.4461\n",
      "Epoch: 11/20... Training loss: 0.4211\n",
      "Epoch: 11/20... Training loss: 0.4493\n",
      "Epoch: 11/20... Training loss: 0.4466\n",
      "Epoch: 11/20... Training loss: 0.4444\n",
      "Epoch: 11/20... Training loss: 0.4376\n",
      "Epoch: 11/20... Training loss: 0.4446\n",
      "Epoch: 11/20... Training loss: 0.4475\n",
      "Epoch: 11/20... Training loss: 0.4416\n",
      "Epoch: 11/20... Training loss: 0.4557\n",
      "Epoch: 11/20... Training loss: 0.4463\n",
      "Epoch: 11/20... Training loss: 0.4398\n",
      "Epoch: 11/20... Training loss: 0.4473\n",
      "Epoch: 11/20... Training loss: 0.4337\n",
      "Epoch: 11/20... Training loss: 0.4410\n",
      "Epoch: 11/20... Training loss: 0.4428\n",
      "Epoch: 11/20... Training loss: 0.4431\n",
      "Epoch: 11/20... Training loss: 0.4432\n",
      "Epoch: 11/20... Training loss: 0.4424\n",
      "Epoch: 11/20... Training loss: 0.4395\n",
      "Epoch: 11/20... Training loss: 0.4451\n",
      "Epoch: 11/20... Training loss: 0.4505\n",
      "Epoch: 11/20... Training loss: 0.4417\n",
      "Epoch: 11/20... Training loss: 0.4519\n",
      "Epoch: 11/20... Training loss: 0.4417\n",
      "Epoch: 11/20... Training loss: 0.4356\n",
      "Epoch: 11/20... Training loss: 0.4510\n",
      "Epoch: 11/20... Training loss: 0.4373\n",
      "Epoch: 11/20... Training loss: 0.4381\n",
      "Epoch: 11/20... Training loss: 0.4319\n",
      "Epoch: 11/20... Training loss: 0.4377\n",
      "Epoch: 11/20... Training loss: 0.4484\n",
      "Epoch: 11/20... Training loss: 0.4406\n",
      "Epoch: 11/20... Training loss: 0.4332\n",
      "Epoch: 11/20... Training loss: 0.4469\n",
      "Epoch: 11/20... Training loss: 0.4485\n",
      "Epoch: 11/20... Training loss: 0.4419\n",
      "Epoch: 11/20... Training loss: 0.4354\n",
      "Epoch: 11/20... Training loss: 0.4397\n",
      "Epoch: 11/20... Training loss: 0.4441\n",
      "Epoch: 11/20... Training loss: 0.4500\n",
      "Epoch: 11/20... Training loss: 0.4482\n",
      "Epoch: 11/20... Training loss: 0.4421\n",
      "Epoch: 11/20... Training loss: 0.4437\n",
      "Epoch: 11/20... Training loss: 0.4421\n",
      "Epoch: 11/20... Training loss: 0.4485\n",
      "Epoch: 11/20... Training loss: 0.4354\n",
      "Epoch: 11/20... Training loss: 0.4434\n",
      "Epoch: 11/20... Training loss: 0.4447\n",
      "Epoch: 11/20... Training loss: 0.4411\n",
      "Epoch: 11/20... Training loss: 0.4436\n",
      "Epoch: 11/20... Training loss: 0.4446\n",
      "Epoch: 11/20... Training loss: 0.4440\n",
      "Epoch: 11/20... Training loss: 0.4381\n",
      "Epoch: 11/20... Training loss: 0.4363\n",
      "Epoch: 11/20... Training loss: 0.4345\n",
      "Epoch: 11/20... Training loss: 0.4429\n",
      "Epoch: 11/20... Training loss: 0.4428\n",
      "Epoch: 11/20... Training loss: 0.4334\n",
      "Epoch: 11/20... Training loss: 0.4477\n",
      "Epoch: 11/20... Training loss: 0.4467\n",
      "Epoch: 11/20... Training loss: 0.4374\n",
      "Epoch: 11/20... Training loss: 0.4434\n",
      "Epoch: 11/20... Training loss: 0.4532\n",
      "Epoch: 11/20... Training loss: 0.4355\n",
      "Epoch: 11/20... Training loss: 0.4441\n",
      "Epoch: 11/20... Training loss: 0.4460\n",
      "Epoch: 11/20... Training loss: 0.4393\n",
      "Epoch: 11/20... Training loss: 0.4364\n",
      "Epoch: 11/20... Training loss: 0.4500\n",
      "Epoch: 11/20... Training loss: 0.4368\n",
      "Epoch: 11/20... Training loss: 0.4416\n",
      "Epoch: 11/20... Training loss: 0.4394\n",
      "Epoch: 11/20... Training loss: 0.4385\n",
      "Epoch: 11/20... Training loss: 0.4431\n",
      "Epoch: 11/20... Training loss: 0.4582\n",
      "Epoch: 11/20... Training loss: 0.4412\n",
      "Epoch: 11/20... Training loss: 0.4477\n",
      "Epoch: 11/20... Training loss: 0.4426\n",
      "Epoch: 11/20... Training loss: 0.4456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20... Training loss: 0.4497\n",
      "Epoch: 11/20... Training loss: 0.4516\n",
      "Epoch: 11/20... Training loss: 0.4280\n",
      "Epoch: 11/20... Training loss: 0.4472\n",
      "Epoch: 11/20... Training loss: 0.4408\n",
      "Epoch: 11/20... Training loss: 0.4379\n",
      "Epoch: 11/20... Training loss: 0.4499\n",
      "Epoch: 11/20... Training loss: 0.4485\n",
      "Epoch: 11/20... Training loss: 0.4387\n",
      "Epoch: 11/20... Training loss: 0.4322\n",
      "Epoch: 11/20... Training loss: 0.4493\n",
      "Epoch: 11/20... Training loss: 0.4402\n",
      "Epoch: 11/20... Training loss: 0.4494\n",
      "Epoch: 12/20... Training loss: 0.4396\n",
      "Epoch: 12/20... Training loss: 0.4417\n",
      "Epoch: 12/20... Training loss: 0.4418\n",
      "Epoch: 12/20... Training loss: 0.4366\n",
      "Epoch: 12/20... Training loss: 0.4349\n",
      "Epoch: 12/20... Training loss: 0.4418\n",
      "Epoch: 12/20... Training loss: 0.4417\n",
      "Epoch: 12/20... Training loss: 0.4419\n",
      "Epoch: 12/20... Training loss: 0.4551\n",
      "Epoch: 12/20... Training loss: 0.4420\n",
      "Epoch: 12/20... Training loss: 0.4485\n",
      "Epoch: 12/20... Training loss: 0.4349\n",
      "Epoch: 12/20... Training loss: 0.4424\n",
      "Epoch: 12/20... Training loss: 0.4418\n",
      "Epoch: 12/20... Training loss: 0.4512\n",
      "Epoch: 12/20... Training loss: 0.4492\n",
      "Epoch: 12/20... Training loss: 0.4431\n",
      "Epoch: 12/20... Training loss: 0.4419\n",
      "Epoch: 12/20... Training loss: 0.4376\n",
      "Epoch: 12/20... Training loss: 0.4464\n",
      "Epoch: 12/20... Training loss: 0.4405\n",
      "Epoch: 12/20... Training loss: 0.4453\n",
      "Epoch: 12/20... Training loss: 0.4499\n",
      "Epoch: 12/20... Training loss: 0.4483\n",
      "Epoch: 12/20... Training loss: 0.4475\n",
      "Epoch: 12/20... Training loss: 0.4450\n",
      "Epoch: 12/20... Training loss: 0.4509\n",
      "Epoch: 12/20... Training loss: 0.4412\n",
      "Epoch: 12/20... Training loss: 0.4365\n",
      "Epoch: 12/20... Training loss: 0.4367\n",
      "Epoch: 12/20... Training loss: 0.4435\n",
      "Epoch: 12/20... Training loss: 0.4481\n",
      "Epoch: 12/20... Training loss: 0.4449\n",
      "Epoch: 12/20... Training loss: 0.4454\n",
      "Epoch: 12/20... Training loss: 0.4463\n",
      "Epoch: 12/20... Training loss: 0.4462\n",
      "Epoch: 12/20... Training loss: 0.4444\n",
      "Epoch: 12/20... Training loss: 0.4469\n",
      "Epoch: 12/20... Training loss: 0.4406\n",
      "Epoch: 12/20... Training loss: 0.4501\n",
      "Epoch: 12/20... Training loss: 0.4473\n",
      "Epoch: 12/20... Training loss: 0.4423\n",
      "Epoch: 12/20... Training loss: 0.4411\n",
      "Epoch: 12/20... Training loss: 0.4442\n",
      "Epoch: 12/20... Training loss: 0.4444\n",
      "Epoch: 12/20... Training loss: 0.4430\n",
      "Epoch: 12/20... Training loss: 0.4396\n",
      "Epoch: 12/20... Training loss: 0.4436\n",
      "Epoch: 12/20... Training loss: 0.4310\n",
      "Epoch: 12/20... Training loss: 0.4463\n",
      "Epoch: 12/20... Training loss: 0.4426\n",
      "Epoch: 12/20... Training loss: 0.4469\n",
      "Epoch: 12/20... Training loss: 0.4424\n",
      "Epoch: 12/20... Training loss: 0.4397\n",
      "Epoch: 12/20... Training loss: 0.4388\n",
      "Epoch: 12/20... Training loss: 0.4476\n",
      "Epoch: 12/20... Training loss: 0.4336\n",
      "Epoch: 12/20... Training loss: 0.4380\n",
      "Epoch: 12/20... Training loss: 0.4345\n",
      "Epoch: 12/20... Training loss: 0.4437\n",
      "Epoch: 12/20... Training loss: 0.4346\n",
      "Epoch: 12/20... Training loss: 0.4481\n",
      "Epoch: 12/20... Training loss: 0.4449\n",
      "Epoch: 12/20... Training loss: 0.4447\n",
      "Epoch: 12/20... Training loss: 0.4369\n",
      "Epoch: 12/20... Training loss: 0.4392\n",
      "Epoch: 12/20... Training loss: 0.4427\n",
      "Epoch: 12/20... Training loss: 0.4326\n",
      "Epoch: 12/20... Training loss: 0.4449\n",
      "Epoch: 12/20... Training loss: 0.4464\n",
      "Epoch: 12/20... Training loss: 0.4467\n",
      "Epoch: 12/20... Training loss: 0.4395\n",
      "Epoch: 12/20... Training loss: 0.4431\n",
      "Epoch: 12/20... Training loss: 0.4366\n",
      "Epoch: 12/20... Training loss: 0.4383\n",
      "Epoch: 12/20... Training loss: 0.4429\n",
      "Epoch: 12/20... Training loss: 0.4473\n",
      "Epoch: 12/20... Training loss: 0.4480\n",
      "Epoch: 12/20... Training loss: 0.4399\n",
      "Epoch: 12/20... Training loss: 0.4371\n",
      "Epoch: 12/20... Training loss: 0.4561\n",
      "Epoch: 12/20... Training loss: 0.4338\n",
      "Epoch: 12/20... Training loss: 0.4460\n",
      "Epoch: 12/20... Training loss: 0.4347\n",
      "Epoch: 12/20... Training loss: 0.4502\n",
      "Epoch: 12/20... Training loss: 0.4343\n",
      "Epoch: 12/20... Training loss: 0.4363\n",
      "Epoch: 12/20... Training loss: 0.4455\n",
      "Epoch: 12/20... Training loss: 0.4297\n",
      "Epoch: 12/20... Training loss: 0.4389\n",
      "Epoch: 12/20... Training loss: 0.4363\n",
      "Epoch: 12/20... Training loss: 0.4471\n",
      "Epoch: 12/20... Training loss: 0.4430\n",
      "Epoch: 12/20... Training loss: 0.4412\n",
      "Epoch: 12/20... Training loss: 0.4297\n",
      "Epoch: 12/20... Training loss: 0.4301\n",
      "Epoch: 12/20... Training loss: 0.4437\n",
      "Epoch: 12/20... Training loss: 0.4391\n",
      "Epoch: 12/20... Training loss: 0.4474\n",
      "Epoch: 12/20... Training loss: 0.4480\n",
      "Epoch: 12/20... Training loss: 0.4429\n",
      "Epoch: 12/20... Training loss: 0.4454\n",
      "Epoch: 12/20... Training loss: 0.4417\n",
      "Epoch: 12/20... Training loss: 0.4474\n",
      "Epoch: 12/20... Training loss: 0.4352\n",
      "Epoch: 12/20... Training loss: 0.4405\n",
      "Epoch: 12/20... Training loss: 0.4430\n",
      "Epoch: 12/20... Training loss: 0.4397\n",
      "Epoch: 12/20... Training loss: 0.4461\n",
      "Epoch: 12/20... Training loss: 0.4419\n",
      "Epoch: 12/20... Training loss: 0.4428\n",
      "Epoch: 12/20... Training loss: 0.4444\n",
      "Epoch: 12/20... Training loss: 0.4590\n",
      "Epoch: 12/20... Training loss: 0.4448\n",
      "Epoch: 12/20... Training loss: 0.4446\n",
      "Epoch: 12/20... Training loss: 0.4382\n",
      "Epoch: 12/20... Training loss: 0.4469\n",
      "Epoch: 12/20... Training loss: 0.4365\n",
      "Epoch: 12/20... Training loss: 0.4355\n",
      "Epoch: 12/20... Training loss: 0.4341\n",
      "Epoch: 12/20... Training loss: 0.4432\n",
      "Epoch: 12/20... Training loss: 0.4468\n",
      "Epoch: 12/20... Training loss: 0.4492\n",
      "Epoch: 12/20... Training loss: 0.4413\n",
      "Epoch: 12/20... Training loss: 0.4411\n",
      "Epoch: 12/20... Training loss: 0.4502\n",
      "Epoch: 12/20... Training loss: 0.4353\n",
      "Epoch: 12/20... Training loss: 0.4429\n",
      "Epoch: 12/20... Training loss: 0.4402\n",
      "Epoch: 12/20... Training loss: 0.4459\n",
      "Epoch: 12/20... Training loss: 0.4352\n",
      "Epoch: 12/20... Training loss: 0.4436\n",
      "Epoch: 12/20... Training loss: 0.4526\n",
      "Epoch: 12/20... Training loss: 0.4488\n",
      "Epoch: 12/20... Training loss: 0.4445\n",
      "Epoch: 12/20... Training loss: 0.4277\n",
      "Epoch: 12/20... Training loss: 0.4427\n",
      "Epoch: 12/20... Training loss: 0.4456\n",
      "Epoch: 12/20... Training loss: 0.4397\n",
      "Epoch: 12/20... Training loss: 0.4461\n",
      "Epoch: 12/20... Training loss: 0.4414\n",
      "Epoch: 12/20... Training loss: 0.4372\n",
      "Epoch: 12/20... Training loss: 0.4322\n",
      "Epoch: 12/20... Training loss: 0.4415\n",
      "Epoch: 12/20... Training loss: 0.4395\n",
      "Epoch: 12/20... Training loss: 0.4427\n",
      "Epoch: 12/20... Training loss: 0.4480\n",
      "Epoch: 12/20... Training loss: 0.4420\n",
      "Epoch: 12/20... Training loss: 0.4373\n",
      "Epoch: 12/20... Training loss: 0.4513\n",
      "Epoch: 12/20... Training loss: 0.4307\n",
      "Epoch: 12/20... Training loss: 0.4457\n",
      "Epoch: 12/20... Training loss: 0.4473\n",
      "Epoch: 12/20... Training loss: 0.4399\n",
      "Epoch: 12/20... Training loss: 0.4491\n",
      "Epoch: 12/20... Training loss: 0.4329\n",
      "Epoch: 12/20... Training loss: 0.4511\n",
      "Epoch: 12/20... Training loss: 0.4327\n",
      "Epoch: 12/20... Training loss: 0.4394\n",
      "Epoch: 12/20... Training loss: 0.4380\n",
      "Epoch: 12/20... Training loss: 0.4472\n",
      "Epoch: 12/20... Training loss: 0.4370\n",
      "Epoch: 12/20... Training loss: 0.4376\n",
      "Epoch: 12/20... Training loss: 0.4405\n",
      "Epoch: 12/20... Training loss: 0.4470\n",
      "Epoch: 12/20... Training loss: 0.4384\n",
      "Epoch: 12/20... Training loss: 0.4424\n",
      "Epoch: 12/20... Training loss: 0.4587\n",
      "Epoch: 12/20... Training loss: 0.4453\n",
      "Epoch: 12/20... Training loss: 0.4421\n",
      "Epoch: 12/20... Training loss: 0.4386\n",
      "Epoch: 12/20... Training loss: 0.4429\n",
      "Epoch: 12/20... Training loss: 0.4375\n",
      "Epoch: 12/20... Training loss: 0.4491\n",
      "Epoch: 12/20... Training loss: 0.4440\n",
      "Epoch: 12/20... Training loss: 0.4400\n",
      "Epoch: 12/20... Training loss: 0.4382\n",
      "Epoch: 12/20... Training loss: 0.4420\n",
      "Epoch: 12/20... Training loss: 0.4483\n",
      "Epoch: 12/20... Training loss: 0.4377\n",
      "Epoch: 12/20... Training loss: 0.4349\n",
      "Epoch: 12/20... Training loss: 0.4435\n",
      "Epoch: 12/20... Training loss: 0.4481\n",
      "Epoch: 13/20... Training loss: 0.4404\n",
      "Epoch: 13/20... Training loss: 0.4607\n",
      "Epoch: 13/20... Training loss: 0.4468\n",
      "Epoch: 13/20... Training loss: 0.4486\n",
      "Epoch: 13/20... Training loss: 0.4390\n",
      "Epoch: 13/20... Training loss: 0.4369\n",
      "Epoch: 13/20... Training loss: 0.4364\n",
      "Epoch: 13/20... Training loss: 0.4447\n",
      "Epoch: 13/20... Training loss: 0.4480\n",
      "Epoch: 13/20... Training loss: 0.4501\n",
      "Epoch: 13/20... Training loss: 0.4445\n",
      "Epoch: 13/20... Training loss: 0.4298\n",
      "Epoch: 13/20... Training loss: 0.4516\n",
      "Epoch: 13/20... Training loss: 0.4369\n",
      "Epoch: 13/20... Training loss: 0.4390\n",
      "Epoch: 13/20... Training loss: 0.4453\n",
      "Epoch: 13/20... Training loss: 0.4468\n",
      "Epoch: 13/20... Training loss: 0.4420\n",
      "Epoch: 13/20... Training loss: 0.4464\n",
      "Epoch: 13/20... Training loss: 0.4471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20... Training loss: 0.4388\n",
      "Epoch: 13/20... Training loss: 0.4361\n",
      "Epoch: 13/20... Training loss: 0.4482\n",
      "Epoch: 13/20... Training loss: 0.4473\n",
      "Epoch: 13/20... Training loss: 0.4485\n",
      "Epoch: 13/20... Training loss: 0.4415\n",
      "Epoch: 13/20... Training loss: 0.4363\n",
      "Epoch: 13/20... Training loss: 0.4398\n",
      "Epoch: 13/20... Training loss: 0.4474\n",
      "Epoch: 13/20... Training loss: 0.4419\n",
      "Epoch: 13/20... Training loss: 0.4398\n",
      "Epoch: 13/20... Training loss: 0.4426\n",
      "Epoch: 13/20... Training loss: 0.4413\n",
      "Epoch: 13/20... Training loss: 0.4474\n",
      "Epoch: 13/20... Training loss: 0.4388\n",
      "Epoch: 13/20... Training loss: 0.4425\n",
      "Epoch: 13/20... Training loss: 0.4462\n",
      "Epoch: 13/20... Training loss: 0.4429\n",
      "Epoch: 13/20... Training loss: 0.4430\n",
      "Epoch: 13/20... Training loss: 0.4451\n",
      "Epoch: 13/20... Training loss: 0.4378\n",
      "Epoch: 13/20... Training loss: 0.4432\n",
      "Epoch: 13/20... Training loss: 0.4368\n",
      "Epoch: 13/20... Training loss: 0.4515\n",
      "Epoch: 13/20... Training loss: 0.4380\n",
      "Epoch: 13/20... Training loss: 0.4446\n",
      "Epoch: 13/20... Training loss: 0.4466\n",
      "Epoch: 13/20... Training loss: 0.4354\n",
      "Epoch: 13/20... Training loss: 0.4433\n",
      "Epoch: 13/20... Training loss: 0.4336\n",
      "Epoch: 13/20... Training loss: 0.4353\n",
      "Epoch: 13/20... Training loss: 0.4357\n",
      "Epoch: 13/20... Training loss: 0.4337\n",
      "Epoch: 13/20... Training loss: 0.4460\n",
      "Epoch: 13/20... Training loss: 0.4425\n",
      "Epoch: 13/20... Training loss: 0.4459\n",
      "Epoch: 13/20... Training loss: 0.4340\n",
      "Epoch: 13/20... Training loss: 0.4303\n",
      "Epoch: 13/20... Training loss: 0.4401\n",
      "Epoch: 13/20... Training loss: 0.4404\n",
      "Epoch: 13/20... Training loss: 0.4371\n",
      "Epoch: 13/20... Training loss: 0.4387\n",
      "Epoch: 13/20... Training loss: 0.4456\n",
      "Epoch: 13/20... Training loss: 0.4448\n",
      "Epoch: 13/20... Training loss: 0.4457\n",
      "Epoch: 13/20... Training loss: 0.4515\n",
      "Epoch: 13/20... Training loss: 0.4376\n",
      "Epoch: 13/20... Training loss: 0.4411\n",
      "Epoch: 13/20... Training loss: 0.4408\n",
      "Epoch: 13/20... Training loss: 0.4504\n",
      "Epoch: 13/20... Training loss: 0.4432\n",
      "Epoch: 13/20... Training loss: 0.4434\n",
      "Epoch: 13/20... Training loss: 0.4498\n",
      "Epoch: 13/20... Training loss: 0.4491\n",
      "Epoch: 13/20... Training loss: 0.4302\n",
      "Epoch: 13/20... Training loss: 0.4467\n",
      "Epoch: 13/20... Training loss: 0.4466\n",
      "Epoch: 13/20... Training loss: 0.4415\n",
      "Epoch: 13/20... Training loss: 0.4390\n",
      "Epoch: 13/20... Training loss: 0.4405\n",
      "Epoch: 13/20... Training loss: 0.4558\n",
      "Epoch: 13/20... Training loss: 0.4375\n",
      "Epoch: 13/20... Training loss: 0.4361\n",
      "Epoch: 13/20... Training loss: 0.4344\n",
      "Epoch: 13/20... Training loss: 0.4475\n",
      "Epoch: 13/20... Training loss: 0.4415\n",
      "Epoch: 13/20... Training loss: 0.4391\n",
      "Epoch: 13/20... Training loss: 0.4416\n",
      "Epoch: 13/20... Training loss: 0.4373\n",
      "Epoch: 13/20... Training loss: 0.4428\n",
      "Epoch: 13/20... Training loss: 0.4405\n",
      "Epoch: 13/20... Training loss: 0.4371\n",
      "Epoch: 13/20... Training loss: 0.4397\n",
      "Epoch: 13/20... Training loss: 0.4300\n",
      "Epoch: 13/20... Training loss: 0.4393\n",
      "Epoch: 13/20... Training loss: 0.4393\n",
      "Epoch: 13/20... Training loss: 0.4461\n",
      "Epoch: 13/20... Training loss: 0.4414\n",
      "Epoch: 13/20... Training loss: 0.4458\n",
      "Epoch: 13/20... Training loss: 0.4467\n",
      "Epoch: 13/20... Training loss: 0.4538\n",
      "Epoch: 13/20... Training loss: 0.4532\n",
      "Epoch: 13/20... Training loss: 0.4555\n",
      "Epoch: 13/20... Training loss: 0.4385\n",
      "Epoch: 13/20... Training loss: 0.4470\n",
      "Epoch: 13/20... Training loss: 0.4319\n",
      "Epoch: 13/20... Training loss: 0.4515\n",
      "Epoch: 13/20... Training loss: 0.4533\n",
      "Epoch: 13/20... Training loss: 0.4405\n",
      "Epoch: 13/20... Training loss: 0.4464\n",
      "Epoch: 13/20... Training loss: 0.4399\n",
      "Epoch: 13/20... Training loss: 0.4525\n",
      "Epoch: 13/20... Training loss: 0.4361\n",
      "Epoch: 13/20... Training loss: 0.4420\n",
      "Epoch: 13/20... Training loss: 0.4376\n",
      "Epoch: 13/20... Training loss: 0.4376\n",
      "Epoch: 13/20... Training loss: 0.4429\n",
      "Epoch: 13/20... Training loss: 0.4485\n",
      "Epoch: 13/20... Training loss: 0.4349\n",
      "Epoch: 13/20... Training loss: 0.4495\n",
      "Epoch: 13/20... Training loss: 0.4394\n",
      "Epoch: 13/20... Training loss: 0.4337\n",
      "Epoch: 13/20... Training loss: 0.4436\n",
      "Epoch: 13/20... Training loss: 0.4359\n",
      "Epoch: 13/20... Training loss: 0.4472\n",
      "Epoch: 13/20... Training loss: 0.4459\n",
      "Epoch: 13/20... Training loss: 0.4505\n",
      "Epoch: 13/20... Training loss: 0.4416\n",
      "Epoch: 13/20... Training loss: 0.4438\n",
      "Epoch: 13/20... Training loss: 0.4374\n",
      "Epoch: 13/20... Training loss: 0.4428\n",
      "Epoch: 13/20... Training loss: 0.4491\n",
      "Epoch: 13/20... Training loss: 0.4424\n",
      "Epoch: 13/20... Training loss: 0.4429\n",
      "Epoch: 13/20... Training loss: 0.4541\n",
      "Epoch: 13/20... Training loss: 0.4456\n",
      "Epoch: 13/20... Training loss: 0.4316\n",
      "Epoch: 13/20... Training loss: 0.4386\n",
      "Epoch: 13/20... Training loss: 0.4521\n",
      "Epoch: 13/20... Training loss: 0.4372\n",
      "Epoch: 13/20... Training loss: 0.4387\n",
      "Epoch: 13/20... Training loss: 0.4376\n",
      "Epoch: 13/20... Training loss: 0.4406\n",
      "Epoch: 13/20... Training loss: 0.4356\n",
      "Epoch: 13/20... Training loss: 0.4439\n",
      "Epoch: 13/20... Training loss: 0.4369\n",
      "Epoch: 13/20... Training loss: 0.4549\n",
      "Epoch: 13/20... Training loss: 0.4466\n",
      "Epoch: 13/20... Training loss: 0.4371\n",
      "Epoch: 13/20... Training loss: 0.4310\n",
      "Epoch: 13/20... Training loss: 0.4349\n",
      "Epoch: 13/20... Training loss: 0.4558\n",
      "Epoch: 13/20... Training loss: 0.4344\n",
      "Epoch: 13/20... Training loss: 0.4379\n",
      "Epoch: 13/20... Training loss: 0.4421\n",
      "Epoch: 13/20... Training loss: 0.4453\n",
      "Epoch: 13/20... Training loss: 0.4395\n",
      "Epoch: 13/20... Training loss: 0.4426\n",
      "Epoch: 13/20... Training loss: 0.4353\n",
      "Epoch: 13/20... Training loss: 0.4399\n",
      "Epoch: 13/20... Training loss: 0.4478\n",
      "Epoch: 13/20... Training loss: 0.4405\n",
      "Epoch: 13/20... Training loss: 0.4440\n",
      "Epoch: 13/20... Training loss: 0.4381\n",
      "Epoch: 13/20... Training loss: 0.4436\n",
      "Epoch: 13/20... Training loss: 0.4471\n",
      "Epoch: 13/20... Training loss: 0.4481\n",
      "Epoch: 13/20... Training loss: 0.4454\n",
      "Epoch: 13/20... Training loss: 0.4420\n",
      "Epoch: 13/20... Training loss: 0.4441\n",
      "Epoch: 13/20... Training loss: 0.4414\n",
      "Epoch: 13/20... Training loss: 0.4336\n",
      "Epoch: 13/20... Training loss: 0.4466\n",
      "Epoch: 13/20... Training loss: 0.4382\n",
      "Epoch: 13/20... Training loss: 0.4391\n",
      "Epoch: 13/20... Training loss: 0.4367\n",
      "Epoch: 13/20... Training loss: 0.4501\n",
      "Epoch: 13/20... Training loss: 0.4486\n",
      "Epoch: 13/20... Training loss: 0.4455\n",
      "Epoch: 13/20... Training loss: 0.4465\n",
      "Epoch: 13/20... Training loss: 0.4321\n",
      "Epoch: 13/20... Training loss: 0.4387\n",
      "Epoch: 13/20... Training loss: 0.4430\n",
      "Epoch: 14/20... Training loss: 0.4485\n",
      "Epoch: 14/20... Training loss: 0.4429\n",
      "Epoch: 14/20... Training loss: 0.4442\n",
      "Epoch: 14/20... Training loss: 0.4409\n",
      "Epoch: 14/20... Training loss: 0.4412\n",
      "Epoch: 14/20... Training loss: 0.4457\n",
      "Epoch: 14/20... Training loss: 0.4532\n",
      "Epoch: 14/20... Training loss: 0.4427\n",
      "Epoch: 14/20... Training loss: 0.4441\n",
      "Epoch: 14/20... Training loss: 0.4364\n",
      "Epoch: 14/20... Training loss: 0.4367\n",
      "Epoch: 14/20... Training loss: 0.4474\n",
      "Epoch: 14/20... Training loss: 0.4470\n",
      "Epoch: 14/20... Training loss: 0.4343\n",
      "Epoch: 14/20... Training loss: 0.4401\n",
      "Epoch: 14/20... Training loss: 0.4413\n",
      "Epoch: 14/20... Training loss: 0.4335\n",
      "Epoch: 14/20... Training loss: 0.4421\n",
      "Epoch: 14/20... Training loss: 0.4455\n",
      "Epoch: 14/20... Training loss: 0.4442\n",
      "Epoch: 14/20... Training loss: 0.4391\n",
      "Epoch: 14/20... Training loss: 0.4379\n",
      "Epoch: 14/20... Training loss: 0.4366\n",
      "Epoch: 14/20... Training loss: 0.4447\n",
      "Epoch: 14/20... Training loss: 0.4394\n",
      "Epoch: 14/20... Training loss: 0.4412\n",
      "Epoch: 14/20... Training loss: 0.4404\n",
      "Epoch: 14/20... Training loss: 0.4428\n",
      "Epoch: 14/20... Training loss: 0.4406\n",
      "Epoch: 14/20... Training loss: 0.4440\n",
      "Epoch: 14/20... Training loss: 0.4393\n",
      "Epoch: 14/20... Training loss: 0.4376\n",
      "Epoch: 14/20... Training loss: 0.4304\n",
      "Epoch: 14/20... Training loss: 0.4407\n",
      "Epoch: 14/20... Training loss: 0.4429\n",
      "Epoch: 14/20... Training loss: 0.4394\n",
      "Epoch: 14/20... Training loss: 0.4482\n",
      "Epoch: 14/20... Training loss: 0.4496\n",
      "Epoch: 14/20... Training loss: 0.4526\n",
      "Epoch: 14/20... Training loss: 0.4393\n",
      "Epoch: 14/20... Training loss: 0.4444\n",
      "Epoch: 14/20... Training loss: 0.4543\n",
      "Epoch: 14/20... Training loss: 0.4369\n",
      "Epoch: 14/20... Training loss: 0.4386\n",
      "Epoch: 14/20... Training loss: 0.4416\n",
      "Epoch: 14/20... Training loss: 0.4522\n",
      "Epoch: 14/20... Training loss: 0.4334\n",
      "Epoch: 14/20... Training loss: 0.4476\n",
      "Epoch: 14/20... Training loss: 0.4431\n",
      "Epoch: 14/20... Training loss: 0.4455\n",
      "Epoch: 14/20... Training loss: 0.4346\n",
      "Epoch: 14/20... Training loss: 0.4414\n",
      "Epoch: 14/20... Training loss: 0.4393\n",
      "Epoch: 14/20... Training loss: 0.4421\n",
      "Epoch: 14/20... Training loss: 0.4412\n",
      "Epoch: 14/20... Training loss: 0.4400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20... Training loss: 0.4360\n",
      "Epoch: 14/20... Training loss: 0.4341\n",
      "Epoch: 14/20... Training loss: 0.4335\n",
      "Epoch: 14/20... Training loss: 0.4445\n",
      "Epoch: 14/20... Training loss: 0.4414\n",
      "Epoch: 14/20... Training loss: 0.4448\n",
      "Epoch: 14/20... Training loss: 0.4515\n",
      "Epoch: 14/20... Training loss: 0.4329\n",
      "Epoch: 14/20... Training loss: 0.4428\n",
      "Epoch: 14/20... Training loss: 0.4409\n",
      "Epoch: 14/20... Training loss: 0.4497\n",
      "Epoch: 14/20... Training loss: 0.4393\n",
      "Epoch: 14/20... Training loss: 0.4445\n",
      "Epoch: 14/20... Training loss: 0.4529\n",
      "Epoch: 14/20... Training loss: 0.4397\n",
      "Epoch: 14/20... Training loss: 0.4449\n",
      "Epoch: 14/20... Training loss: 0.4524\n",
      "Epoch: 14/20... Training loss: 0.4387\n",
      "Epoch: 14/20... Training loss: 0.4352\n",
      "Epoch: 14/20... Training loss: 0.4424\n",
      "Epoch: 14/20... Training loss: 0.4421\n",
      "Epoch: 14/20... Training loss: 0.4548\n",
      "Epoch: 14/20... Training loss: 0.4456\n",
      "Epoch: 14/20... Training loss: 0.4539\n",
      "Epoch: 14/20... Training loss: 0.4445\n",
      "Epoch: 14/20... Training loss: 0.4524\n",
      "Epoch: 14/20... Training loss: 0.4410\n",
      "Epoch: 14/20... Training loss: 0.4415\n",
      "Epoch: 14/20... Training loss: 0.4471\n",
      "Epoch: 14/20... Training loss: 0.4530\n",
      "Epoch: 14/20... Training loss: 0.4379\n",
      "Epoch: 14/20... Training loss: 0.4392\n",
      "Epoch: 14/20... Training loss: 0.4475\n",
      "Epoch: 14/20... Training loss: 0.4454\n",
      "Epoch: 14/20... Training loss: 0.4431\n",
      "Epoch: 14/20... Training loss: 0.4376\n",
      "Epoch: 14/20... Training loss: 0.4455\n",
      "Epoch: 14/20... Training loss: 0.4423\n",
      "Epoch: 14/20... Training loss: 0.4360\n",
      "Epoch: 14/20... Training loss: 0.4428\n",
      "Epoch: 14/20... Training loss: 0.4388\n",
      "Epoch: 14/20... Training loss: 0.4359\n",
      "Epoch: 14/20... Training loss: 0.4428\n",
      "Epoch: 14/20... Training loss: 0.4388\n",
      "Epoch: 14/20... Training loss: 0.4346\n",
      "Epoch: 14/20... Training loss: 0.4543\n",
      "Epoch: 14/20... Training loss: 0.4335\n",
      "Epoch: 14/20... Training loss: 0.4404\n",
      "Epoch: 14/20... Training loss: 0.4492\n",
      "Epoch: 14/20... Training loss: 0.4425\n",
      "Epoch: 14/20... Training loss: 0.4512\n",
      "Epoch: 14/20... Training loss: 0.4364\n",
      "Epoch: 14/20... Training loss: 0.4337\n",
      "Epoch: 14/20... Training loss: 0.4413\n",
      "Epoch: 14/20... Training loss: 0.4450\n",
      "Epoch: 14/20... Training loss: 0.4455\n",
      "Epoch: 14/20... Training loss: 0.4351\n",
      "Epoch: 14/20... Training loss: 0.4461\n",
      "Epoch: 14/20... Training loss: 0.4450\n",
      "Epoch: 14/20... Training loss: 0.4320\n",
      "Epoch: 14/20... Training loss: 0.4419\n",
      "Epoch: 14/20... Training loss: 0.4425\n",
      "Epoch: 14/20... Training loss: 0.4435\n",
      "Epoch: 14/20... Training loss: 0.4407\n",
      "Epoch: 14/20... Training loss: 0.4509\n",
      "Epoch: 14/20... Training loss: 0.4402\n",
      "Epoch: 14/20... Training loss: 0.4418\n",
      "Epoch: 14/20... Training loss: 0.4459\n",
      "Epoch: 14/20... Training loss: 0.4490\n",
      "Epoch: 14/20... Training loss: 0.4372\n",
      "Epoch: 14/20... Training loss: 0.4327\n",
      "Epoch: 14/20... Training loss: 0.4372\n",
      "Epoch: 14/20... Training loss: 0.4389\n",
      "Epoch: 14/20... Training loss: 0.4420\n",
      "Epoch: 14/20... Training loss: 0.4242\n",
      "Epoch: 14/20... Training loss: 0.4433\n",
      "Epoch: 14/20... Training loss: 0.4420\n",
      "Epoch: 14/20... Training loss: 0.4423\n",
      "Epoch: 14/20... Training loss: 0.4327\n",
      "Epoch: 14/20... Training loss: 0.4404\n",
      "Epoch: 14/20... Training loss: 0.4496\n",
      "Epoch: 14/20... Training loss: 0.4340\n",
      "Epoch: 14/20... Training loss: 0.4397\n",
      "Epoch: 14/20... Training loss: 0.4450\n",
      "Epoch: 14/20... Training loss: 0.4437\n",
      "Epoch: 14/20... Training loss: 0.4425\n",
      "Epoch: 14/20... Training loss: 0.4441\n",
      "Epoch: 14/20... Training loss: 0.4394\n",
      "Epoch: 14/20... Training loss: 0.4441\n",
      "Epoch: 14/20... Training loss: 0.4467\n",
      "Epoch: 14/20... Training loss: 0.4491\n",
      "Epoch: 14/20... Training loss: 0.4414\n",
      "Epoch: 14/20... Training loss: 0.4376\n",
      "Epoch: 14/20... Training loss: 0.4376\n",
      "Epoch: 14/20... Training loss: 0.4379\n",
      "Epoch: 14/20... Training loss: 0.4399\n",
      "Epoch: 14/20... Training loss: 0.4460\n",
      "Epoch: 14/20... Training loss: 0.4447\n",
      "Epoch: 14/20... Training loss: 0.4469\n",
      "Epoch: 14/20... Training loss: 0.4355\n",
      "Epoch: 14/20... Training loss: 0.4416\n",
      "Epoch: 14/20... Training loss: 0.4517\n",
      "Epoch: 14/20... Training loss: 0.4428\n",
      "Epoch: 14/20... Training loss: 0.4451\n",
      "Epoch: 14/20... Training loss: 0.4322\n",
      "Epoch: 14/20... Training loss: 0.4463\n",
      "Epoch: 14/20... Training loss: 0.4445\n",
      "Epoch: 14/20... Training loss: 0.4378\n",
      "Epoch: 14/20... Training loss: 0.4397\n",
      "Epoch: 14/20... Training loss: 0.4368\n",
      "Epoch: 14/20... Training loss: 0.4538\n",
      "Epoch: 14/20... Training loss: 0.4389\n",
      "Epoch: 14/20... Training loss: 0.4422\n",
      "Epoch: 14/20... Training loss: 0.4369\n",
      "Epoch: 14/20... Training loss: 0.4435\n",
      "Epoch: 14/20... Training loss: 0.4468\n",
      "Epoch: 14/20... Training loss: 0.4383\n",
      "Epoch: 14/20... Training loss: 0.4398\n",
      "Epoch: 14/20... Training loss: 0.4421\n",
      "Epoch: 14/20... Training loss: 0.4374\n",
      "Epoch: 14/20... Training loss: 0.4553\n",
      "Epoch: 14/20... Training loss: 0.4369\n",
      "Epoch: 14/20... Training loss: 0.4548\n",
      "Epoch: 14/20... Training loss: 0.4390\n",
      "Epoch: 14/20... Training loss: 0.4345\n",
      "Epoch: 14/20... Training loss: 0.4359\n",
      "Epoch: 14/20... Training loss: 0.4305\n",
      "Epoch: 15/20... Training loss: 0.4347\n",
      "Epoch: 15/20... Training loss: 0.4346\n",
      "Epoch: 15/20... Training loss: 0.4598\n",
      "Epoch: 15/20... Training loss: 0.4281\n",
      "Epoch: 15/20... Training loss: 0.4406\n",
      "Epoch: 15/20... Training loss: 0.4472\n",
      "Epoch: 15/20... Training loss: 0.4433\n",
      "Epoch: 15/20... Training loss: 0.4375\n",
      "Epoch: 15/20... Training loss: 0.4364\n",
      "Epoch: 15/20... Training loss: 0.4376\n",
      "Epoch: 15/20... Training loss: 0.4339\n",
      "Epoch: 15/20... Training loss: 0.4467\n",
      "Epoch: 15/20... Training loss: 0.4454\n",
      "Epoch: 15/20... Training loss: 0.4488\n",
      "Epoch: 15/20... Training loss: 0.4454\n",
      "Epoch: 15/20... Training loss: 0.4371\n",
      "Epoch: 15/20... Training loss: 0.4443\n",
      "Epoch: 15/20... Training loss: 0.4498\n",
      "Epoch: 15/20... Training loss: 0.4420\n",
      "Epoch: 15/20... Training loss: 0.4343\n",
      "Epoch: 15/20... Training loss: 0.4334\n",
      "Epoch: 15/20... Training loss: 0.4384\n",
      "Epoch: 15/20... Training loss: 0.4417\n",
      "Epoch: 15/20... Training loss: 0.4313\n",
      "Epoch: 15/20... Training loss: 0.4458\n",
      "Epoch: 15/20... Training loss: 0.4400\n",
      "Epoch: 15/20... Training loss: 0.4374\n",
      "Epoch: 15/20... Training loss: 0.4371\n",
      "Epoch: 15/20... Training loss: 0.4395\n",
      "Epoch: 15/20... Training loss: 0.4304\n",
      "Epoch: 15/20... Training loss: 0.4360\n",
      "Epoch: 15/20... Training loss: 0.4291\n",
      "Epoch: 15/20... Training loss: 0.4438\n",
      "Epoch: 15/20... Training loss: 0.4481\n",
      "Epoch: 15/20... Training loss: 0.4308\n",
      "Epoch: 15/20... Training loss: 0.4447\n",
      "Epoch: 15/20... Training loss: 0.4467\n",
      "Epoch: 15/20... Training loss: 0.4436\n",
      "Epoch: 15/20... Training loss: 0.4559\n",
      "Epoch: 15/20... Training loss: 0.4431\n",
      "Epoch: 15/20... Training loss: 0.4500\n",
      "Epoch: 15/20... Training loss: 0.4343\n",
      "Epoch: 15/20... Training loss: 0.4355\n",
      "Epoch: 15/20... Training loss: 0.4427\n",
      "Epoch: 15/20... Training loss: 0.4310\n",
      "Epoch: 15/20... Training loss: 0.4449\n",
      "Epoch: 15/20... Training loss: 0.4476\n",
      "Epoch: 15/20... Training loss: 0.4327\n",
      "Epoch: 15/20... Training loss: 0.4490\n",
      "Epoch: 15/20... Training loss: 0.4417\n",
      "Epoch: 15/20... Training loss: 0.4407\n",
      "Epoch: 15/20... Training loss: 0.4463\n",
      "Epoch: 15/20... Training loss: 0.4357\n",
      "Epoch: 15/20... Training loss: 0.4423\n",
      "Epoch: 15/20... Training loss: 0.4384\n",
      "Epoch: 15/20... Training loss: 0.4418\n",
      "Epoch: 15/20... Training loss: 0.4360\n",
      "Epoch: 15/20... Training loss: 0.4378\n",
      "Epoch: 15/20... Training loss: 0.4427\n",
      "Epoch: 15/20... Training loss: 0.4531\n",
      "Epoch: 15/20... Training loss: 0.4400\n",
      "Epoch: 15/20... Training loss: 0.4295\n",
      "Epoch: 15/20... Training loss: 0.4413\n",
      "Epoch: 15/20... Training loss: 0.4355\n",
      "Epoch: 15/20... Training loss: 0.4438\n",
      "Epoch: 15/20... Training loss: 0.4474\n",
      "Epoch: 15/20... Training loss: 0.4427\n",
      "Epoch: 15/20... Training loss: 0.4467\n",
      "Epoch: 15/20... Training loss: 0.4546\n",
      "Epoch: 15/20... Training loss: 0.4464\n",
      "Epoch: 15/20... Training loss: 0.4354\n",
      "Epoch: 15/20... Training loss: 0.4428\n",
      "Epoch: 15/20... Training loss: 0.4415\n",
      "Epoch: 15/20... Training loss: 0.4429\n",
      "Epoch: 15/20... Training loss: 0.4409\n",
      "Epoch: 15/20... Training loss: 0.4384\n",
      "Epoch: 15/20... Training loss: 0.4395\n",
      "Epoch: 15/20... Training loss: 0.4435\n",
      "Epoch: 15/20... Training loss: 0.4420\n",
      "Epoch: 15/20... Training loss: 0.4357\n",
      "Epoch: 15/20... Training loss: 0.4493\n",
      "Epoch: 15/20... Training loss: 0.4347\n",
      "Epoch: 15/20... Training loss: 0.4403\n",
      "Epoch: 15/20... Training loss: 0.4467\n",
      "Epoch: 15/20... Training loss: 0.4386\n",
      "Epoch: 15/20... Training loss: 0.4472\n",
      "Epoch: 15/20... Training loss: 0.4463\n",
      "Epoch: 15/20... Training loss: 0.4396\n",
      "Epoch: 15/20... Training loss: 0.4453\n",
      "Epoch: 15/20... Training loss: 0.4487\n",
      "Epoch: 15/20... Training loss: 0.4400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20... Training loss: 0.4419\n",
      "Epoch: 15/20... Training loss: 0.4373\n",
      "Epoch: 15/20... Training loss: 0.4429\n",
      "Epoch: 15/20... Training loss: 0.4396\n",
      "Epoch: 15/20... Training loss: 0.4418\n",
      "Epoch: 15/20... Training loss: 0.4499\n",
      "Epoch: 15/20... Training loss: 0.4382\n",
      "Epoch: 15/20... Training loss: 0.4384\n",
      "Epoch: 15/20... Training loss: 0.4432\n",
      "Epoch: 15/20... Training loss: 0.4306\n",
      "Epoch: 15/20... Training loss: 0.4329\n",
      "Epoch: 15/20... Training loss: 0.4351\n",
      "Epoch: 15/20... Training loss: 0.4449\n",
      "Epoch: 15/20... Training loss: 0.4457\n",
      "Epoch: 15/20... Training loss: 0.4487\n",
      "Epoch: 15/20... Training loss: 0.4489\n",
      "Epoch: 15/20... Training loss: 0.4451\n",
      "Epoch: 15/20... Training loss: 0.4453\n",
      "Epoch: 15/20... Training loss: 0.4410\n",
      "Epoch: 15/20... Training loss: 0.4438\n",
      "Epoch: 15/20... Training loss: 0.4371\n",
      "Epoch: 15/20... Training loss: 0.4497\n",
      "Epoch: 15/20... Training loss: 0.4405\n",
      "Epoch: 15/20... Training loss: 0.4460\n",
      "Epoch: 15/20... Training loss: 0.4562\n",
      "Epoch: 15/20... Training loss: 0.4420\n",
      "Epoch: 15/20... Training loss: 0.4427\n",
      "Epoch: 15/20... Training loss: 0.4389\n",
      "Epoch: 15/20... Training loss: 0.4434\n",
      "Epoch: 15/20... Training loss: 0.4434\n",
      "Epoch: 15/20... Training loss: 0.4434\n",
      "Epoch: 15/20... Training loss: 0.4399\n",
      "Epoch: 15/20... Training loss: 0.4374\n",
      "Epoch: 15/20... Training loss: 0.4486\n",
      "Epoch: 15/20... Training loss: 0.4448\n",
      "Epoch: 15/20... Training loss: 0.4433\n",
      "Epoch: 15/20... Training loss: 0.4398\n",
      "Epoch: 15/20... Training loss: 0.4416\n",
      "Epoch: 15/20... Training loss: 0.4457\n",
      "Epoch: 15/20... Training loss: 0.4459\n",
      "Epoch: 15/20... Training loss: 0.4487\n",
      "Epoch: 15/20... Training loss: 0.4447\n",
      "Epoch: 15/20... Training loss: 0.4302\n",
      "Epoch: 15/20... Training loss: 0.4477\n",
      "Epoch: 15/20... Training loss: 0.4468\n",
      "Epoch: 15/20... Training loss: 0.4362\n",
      "Epoch: 15/20... Training loss: 0.4434\n",
      "Epoch: 15/20... Training loss: 0.4348\n",
      "Epoch: 15/20... Training loss: 0.4361\n",
      "Epoch: 15/20... Training loss: 0.4514\n",
      "Epoch: 15/20... Training loss: 0.4356\n",
      "Epoch: 15/20... Training loss: 0.4375\n",
      "Epoch: 15/20... Training loss: 0.4463\n",
      "Epoch: 15/20... Training loss: 0.4369\n",
      "Epoch: 15/20... Training loss: 0.4462\n",
      "Epoch: 15/20... Training loss: 0.4447\n",
      "Epoch: 15/20... Training loss: 0.4400\n",
      "Epoch: 15/20... Training loss: 0.4432\n",
      "Epoch: 15/20... Training loss: 0.4387\n",
      "Epoch: 15/20... Training loss: 0.4375\n",
      "Epoch: 15/20... Training loss: 0.4465\n",
      "Epoch: 15/20... Training loss: 0.4404\n",
      "Epoch: 15/20... Training loss: 0.4444\n",
      "Epoch: 15/20... Training loss: 0.4382\n",
      "Epoch: 15/20... Training loss: 0.4442\n",
      "Epoch: 15/20... Training loss: 0.4305\n",
      "Epoch: 15/20... Training loss: 0.4429\n",
      "Epoch: 15/20... Training loss: 0.4402\n",
      "Epoch: 15/20... Training loss: 0.4533\n",
      "Epoch: 15/20... Training loss: 0.4473\n",
      "Epoch: 15/20... Training loss: 0.4326\n",
      "Epoch: 15/20... Training loss: 0.4412\n",
      "Epoch: 15/20... Training loss: 0.4341\n",
      "Epoch: 15/20... Training loss: 0.4452\n",
      "Epoch: 15/20... Training loss: 0.4366\n",
      "Epoch: 15/20... Training loss: 0.4354\n",
      "Epoch: 15/20... Training loss: 0.4385\n",
      "Epoch: 15/20... Training loss: 0.4485\n",
      "Epoch: 15/20... Training loss: 0.4446\n",
      "Epoch: 15/20... Training loss: 0.4537\n",
      "Epoch: 15/20... Training loss: 0.4431\n",
      "Epoch: 15/20... Training loss: 0.4471\n",
      "Epoch: 15/20... Training loss: 0.4395\n",
      "Epoch: 15/20... Training loss: 0.4344\n",
      "Epoch: 15/20... Training loss: 0.4421\n",
      "Epoch: 15/20... Training loss: 0.4404\n",
      "Epoch: 15/20... Training loss: 0.4385\n",
      "Epoch: 15/20... Training loss: 0.4420\n",
      "Epoch: 15/20... Training loss: 0.4458\n",
      "Epoch: 15/20... Training loss: 0.4446\n",
      "Epoch: 15/20... Training loss: 0.4445\n",
      "Epoch: 15/20... Training loss: 0.4446\n",
      "Epoch: 16/20... Training loss: 0.4412\n",
      "Epoch: 16/20... Training loss: 0.4415\n",
      "Epoch: 16/20... Training loss: 0.4386\n",
      "Epoch: 16/20... Training loss: 0.4369\n",
      "Epoch: 16/20... Training loss: 0.4379\n",
      "Epoch: 16/20... Training loss: 0.4360\n",
      "Epoch: 16/20... Training loss: 0.4388\n",
      "Epoch: 16/20... Training loss: 0.4368\n",
      "Epoch: 16/20... Training loss: 0.4381\n",
      "Epoch: 16/20... Training loss: 0.4471\n",
      "Epoch: 16/20... Training loss: 0.4378\n",
      "Epoch: 16/20... Training loss: 0.4448\n",
      "Epoch: 16/20... Training loss: 0.4396\n",
      "Epoch: 16/20... Training loss: 0.4467\n",
      "Epoch: 16/20... Training loss: 0.4422\n",
      "Epoch: 16/20... Training loss: 0.4468\n",
      "Epoch: 16/20... Training loss: 0.4398\n",
      "Epoch: 16/20... Training loss: 0.4523\n",
      "Epoch: 16/20... Training loss: 0.4389\n",
      "Epoch: 16/20... Training loss: 0.4440\n",
      "Epoch: 16/20... Training loss: 0.4406\n",
      "Epoch: 16/20... Training loss: 0.4485\n",
      "Epoch: 16/20... Training loss: 0.4318\n",
      "Epoch: 16/20... Training loss: 0.4547\n",
      "Epoch: 16/20... Training loss: 0.4391\n",
      "Epoch: 16/20... Training loss: 0.4365\n",
      "Epoch: 16/20... Training loss: 0.4458\n",
      "Epoch: 16/20... Training loss: 0.4471\n",
      "Epoch: 16/20... Training loss: 0.4428\n",
      "Epoch: 16/20... Training loss: 0.4439\n",
      "Epoch: 16/20... Training loss: 0.4453\n",
      "Epoch: 16/20... Training loss: 0.4319\n",
      "Epoch: 16/20... Training loss: 0.4434\n",
      "Epoch: 16/20... Training loss: 0.4449\n",
      "Epoch: 16/20... Training loss: 0.4458\n",
      "Epoch: 16/20... Training loss: 0.4429\n",
      "Epoch: 16/20... Training loss: 0.4373\n",
      "Epoch: 16/20... Training loss: 0.4394\n",
      "Epoch: 16/20... Training loss: 0.4287\n",
      "Epoch: 16/20... Training loss: 0.4464\n",
      "Epoch: 16/20... Training loss: 0.4427\n",
      "Epoch: 16/20... Training loss: 0.4493\n",
      "Epoch: 16/20... Training loss: 0.4430\n",
      "Epoch: 16/20... Training loss: 0.4492\n",
      "Epoch: 16/20... Training loss: 0.4377\n",
      "Epoch: 16/20... Training loss: 0.4345\n",
      "Epoch: 16/20... Training loss: 0.4412\n",
      "Epoch: 16/20... Training loss: 0.4484\n",
      "Epoch: 16/20... Training loss: 0.4385\n",
      "Epoch: 16/20... Training loss: 0.4523\n",
      "Epoch: 16/20... Training loss: 0.4409\n",
      "Epoch: 16/20... Training loss: 0.4518\n",
      "Epoch: 16/20... Training loss: 0.4390\n",
      "Epoch: 16/20... Training loss: 0.4413\n",
      "Epoch: 16/20... Training loss: 0.4480\n",
      "Epoch: 16/20... Training loss: 0.4433\n",
      "Epoch: 16/20... Training loss: 0.4451\n",
      "Epoch: 16/20... Training loss: 0.4505\n",
      "Epoch: 16/20... Training loss: 0.4427\n",
      "Epoch: 16/20... Training loss: 0.4456\n",
      "Epoch: 16/20... Training loss: 0.4378\n",
      "Epoch: 16/20... Training loss: 0.4439\n",
      "Epoch: 16/20... Training loss: 0.4401\n",
      "Epoch: 16/20... Training loss: 0.4430\n",
      "Epoch: 16/20... Training loss: 0.4481\n",
      "Epoch: 16/20... Training loss: 0.4400\n",
      "Epoch: 16/20... Training loss: 0.4303\n",
      "Epoch: 16/20... Training loss: 0.4502\n",
      "Epoch: 16/20... Training loss: 0.4486\n",
      "Epoch: 16/20... Training loss: 0.4342\n",
      "Epoch: 16/20... Training loss: 0.4381\n",
      "Epoch: 16/20... Training loss: 0.4399\n",
      "Epoch: 16/20... Training loss: 0.4403\n",
      "Epoch: 16/20... Training loss: 0.4420\n",
      "Epoch: 16/20... Training loss: 0.4456\n",
      "Epoch: 16/20... Training loss: 0.4425\n",
      "Epoch: 16/20... Training loss: 0.4423\n",
      "Epoch: 16/20... Training loss: 0.4349\n",
      "Epoch: 16/20... Training loss: 0.4352\n",
      "Epoch: 16/20... Training loss: 0.4390\n",
      "Epoch: 16/20... Training loss: 0.4402\n",
      "Epoch: 16/20... Training loss: 0.4444\n",
      "Epoch: 16/20... Training loss: 0.4313\n",
      "Epoch: 16/20... Training loss: 0.4432\n",
      "Epoch: 16/20... Training loss: 0.4339\n",
      "Epoch: 16/20... Training loss: 0.4390\n",
      "Epoch: 16/20... Training loss: 0.4465\n",
      "Epoch: 16/20... Training loss: 0.4466\n",
      "Epoch: 16/20... Training loss: 0.4436\n",
      "Epoch: 16/20... Training loss: 0.4563\n",
      "Epoch: 16/20... Training loss: 0.4292\n",
      "Epoch: 16/20... Training loss: 0.4442\n",
      "Epoch: 16/20... Training loss: 0.4472\n",
      "Epoch: 16/20... Training loss: 0.4466\n",
      "Epoch: 16/20... Training loss: 0.4375\n",
      "Epoch: 16/20... Training loss: 0.4478\n",
      "Epoch: 16/20... Training loss: 0.4369\n",
      "Epoch: 16/20... Training loss: 0.4373\n",
      "Epoch: 16/20... Training loss: 0.4414\n",
      "Epoch: 16/20... Training loss: 0.4359\n",
      "Epoch: 16/20... Training loss: 0.4455\n",
      "Epoch: 16/20... Training loss: 0.4294\n",
      "Epoch: 16/20... Training loss: 0.4435\n",
      "Epoch: 16/20... Training loss: 0.4347\n",
      "Epoch: 16/20... Training loss: 0.4453\n",
      "Epoch: 16/20... Training loss: 0.4378\n",
      "Epoch: 16/20... Training loss: 0.4406\n",
      "Epoch: 16/20... Training loss: 0.4421\n",
      "Epoch: 16/20... Training loss: 0.4320\n",
      "Epoch: 16/20... Training loss: 0.4438\n",
      "Epoch: 16/20... Training loss: 0.4431\n",
      "Epoch: 16/20... Training loss: 0.4366\n",
      "Epoch: 16/20... Training loss: 0.4444\n",
      "Epoch: 16/20... Training loss: 0.4354\n",
      "Epoch: 16/20... Training loss: 0.4455\n",
      "Epoch: 16/20... Training loss: 0.4533\n",
      "Epoch: 16/20... Training loss: 0.4485\n",
      "Epoch: 16/20... Training loss: 0.4330\n",
      "Epoch: 16/20... Training loss: 0.4384\n",
      "Epoch: 16/20... Training loss: 0.4460\n",
      "Epoch: 16/20... Training loss: 0.4359\n",
      "Epoch: 16/20... Training loss: 0.4400\n",
      "Epoch: 16/20... Training loss: 0.4397\n",
      "Epoch: 16/20... Training loss: 0.4354\n",
      "Epoch: 16/20... Training loss: 0.4252\n",
      "Epoch: 16/20... Training loss: 0.4404\n",
      "Epoch: 16/20... Training loss: 0.4412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20... Training loss: 0.4438\n",
      "Epoch: 16/20... Training loss: 0.4458\n",
      "Epoch: 16/20... Training loss: 0.4444\n",
      "Epoch: 16/20... Training loss: 0.4365\n",
      "Epoch: 16/20... Training loss: 0.4492\n",
      "Epoch: 16/20... Training loss: 0.4461\n",
      "Epoch: 16/20... Training loss: 0.4473\n",
      "Epoch: 16/20... Training loss: 0.4267\n",
      "Epoch: 16/20... Training loss: 0.4381\n",
      "Epoch: 16/20... Training loss: 0.4471\n",
      "Epoch: 16/20... Training loss: 0.4448\n",
      "Epoch: 16/20... Training loss: 0.4388\n",
      "Epoch: 16/20... Training loss: 0.4398\n",
      "Epoch: 16/20... Training loss: 0.4517\n",
      "Epoch: 16/20... Training loss: 0.4367\n",
      "Epoch: 16/20... Training loss: 0.4428\n",
      "Epoch: 16/20... Training loss: 0.4357\n",
      "Epoch: 16/20... Training loss: 0.4419\n",
      "Epoch: 16/20... Training loss: 0.4306\n",
      "Epoch: 16/20... Training loss: 0.4426\n",
      "Epoch: 16/20... Training loss: 0.4365\n",
      "Epoch: 16/20... Training loss: 0.4435\n",
      "Epoch: 16/20... Training loss: 0.4429\n",
      "Epoch: 16/20... Training loss: 0.4412\n",
      "Epoch: 16/20... Training loss: 0.4315\n",
      "Epoch: 16/20... Training loss: 0.4504\n",
      "Epoch: 16/20... Training loss: 0.4550\n",
      "Epoch: 16/20... Training loss: 0.4373\n",
      "Epoch: 16/20... Training loss: 0.4384\n",
      "Epoch: 16/20... Training loss: 0.4318\n",
      "Epoch: 16/20... Training loss: 0.4436\n",
      "Epoch: 16/20... Training loss: 0.4435\n",
      "Epoch: 16/20... Training loss: 0.4393\n",
      "Epoch: 16/20... Training loss: 0.4483\n",
      "Epoch: 16/20... Training loss: 0.4463\n",
      "Epoch: 16/20... Training loss: 0.4453\n",
      "Epoch: 16/20... Training loss: 0.4374\n",
      "Epoch: 16/20... Training loss: 0.4385\n",
      "Epoch: 16/20... Training loss: 0.4497\n",
      "Epoch: 16/20... Training loss: 0.4486\n",
      "Epoch: 16/20... Training loss: 0.4330\n",
      "Epoch: 16/20... Training loss: 0.4333\n",
      "Epoch: 16/20... Training loss: 0.4414\n",
      "Epoch: 16/20... Training loss: 0.4357\n",
      "Epoch: 16/20... Training loss: 0.4474\n",
      "Epoch: 16/20... Training loss: 0.4519\n",
      "Epoch: 16/20... Training loss: 0.4252\n",
      "Epoch: 16/20... Training loss: 0.4452\n",
      "Epoch: 16/20... Training loss: 0.4402\n",
      "Epoch: 16/20... Training loss: 0.4371\n",
      "Epoch: 16/20... Training loss: 0.4413\n",
      "Epoch: 16/20... Training loss: 0.4348\n",
      "Epoch: 16/20... Training loss: 0.4539\n",
      "Epoch: 16/20... Training loss: 0.4408\n",
      "Epoch: 16/20... Training loss: 0.4424\n",
      "Epoch: 16/20... Training loss: 0.4308\n",
      "Epoch: 17/20... Training loss: 0.4449\n",
      "Epoch: 17/20... Training loss: 0.4423\n",
      "Epoch: 17/20... Training loss: 0.4350\n",
      "Epoch: 17/20... Training loss: 0.4439\n",
      "Epoch: 17/20... Training loss: 0.4532\n",
      "Epoch: 17/20... Training loss: 0.4435\n",
      "Epoch: 17/20... Training loss: 0.4333\n",
      "Epoch: 17/20... Training loss: 0.4436\n",
      "Epoch: 17/20... Training loss: 0.4545\n",
      "Epoch: 17/20... Training loss: 0.4441\n",
      "Epoch: 17/20... Training loss: 0.4382\n",
      "Epoch: 17/20... Training loss: 0.4341\n",
      "Epoch: 17/20... Training loss: 0.4373\n",
      "Epoch: 17/20... Training loss: 0.4442\n",
      "Epoch: 17/20... Training loss: 0.4348\n",
      "Epoch: 17/20... Training loss: 0.4289\n",
      "Epoch: 17/20... Training loss: 0.4424\n",
      "Epoch: 17/20... Training loss: 0.4399\n",
      "Epoch: 17/20... Training loss: 0.4509\n",
      "Epoch: 17/20... Training loss: 0.4376\n",
      "Epoch: 17/20... Training loss: 0.4496\n",
      "Epoch: 17/20... Training loss: 0.4386\n",
      "Epoch: 17/20... Training loss: 0.4413\n",
      "Epoch: 17/20... Training loss: 0.4428\n",
      "Epoch: 17/20... Training loss: 0.4505\n",
      "Epoch: 17/20... Training loss: 0.4596\n",
      "Epoch: 17/20... Training loss: 0.4454\n",
      "Epoch: 17/20... Training loss: 0.4386\n",
      "Epoch: 17/20... Training loss: 0.4417\n",
      "Epoch: 17/20... Training loss: 0.4343\n",
      "Epoch: 17/20... Training loss: 0.4421\n",
      "Epoch: 17/20... Training loss: 0.4455\n",
      "Epoch: 17/20... Training loss: 0.4441\n",
      "Epoch: 17/20... Training loss: 0.4489\n",
      "Epoch: 17/20... Training loss: 0.4431\n",
      "Epoch: 17/20... Training loss: 0.4391\n",
      "Epoch: 17/20... Training loss: 0.4455\n",
      "Epoch: 17/20... Training loss: 0.4401\n",
      "Epoch: 17/20... Training loss: 0.4327\n",
      "Epoch: 17/20... Training loss: 0.4445\n",
      "Epoch: 17/20... Training loss: 0.4426\n",
      "Epoch: 17/20... Training loss: 0.4369\n",
      "Epoch: 17/20... Training loss: 0.4445\n",
      "Epoch: 17/20... Training loss: 0.4410\n",
      "Epoch: 17/20... Training loss: 0.4320\n",
      "Epoch: 17/20... Training loss: 0.4423\n",
      "Epoch: 17/20... Training loss: 0.4329\n",
      "Epoch: 17/20... Training loss: 0.4419\n",
      "Epoch: 17/20... Training loss: 0.4401\n",
      "Epoch: 17/20... Training loss: 0.4347\n",
      "Epoch: 17/20... Training loss: 0.4469\n",
      "Epoch: 17/20... Training loss: 0.4385\n",
      "Epoch: 17/20... Training loss: 0.4427\n",
      "Epoch: 17/20... Training loss: 0.4372\n",
      "Epoch: 17/20... Training loss: 0.4394\n",
      "Epoch: 17/20... Training loss: 0.4455\n",
      "Epoch: 17/20... Training loss: 0.4355\n",
      "Epoch: 17/20... Training loss: 0.4496\n",
      "Epoch: 17/20... Training loss: 0.4417\n",
      "Epoch: 17/20... Training loss: 0.4486\n",
      "Epoch: 17/20... Training loss: 0.4440\n",
      "Epoch: 17/20... Training loss: 0.4278\n",
      "Epoch: 17/20... Training loss: 0.4372\n",
      "Epoch: 17/20... Training loss: 0.4441\n",
      "Epoch: 17/20... Training loss: 0.4421\n",
      "Epoch: 17/20... Training loss: 0.4372\n",
      "Epoch: 17/20... Training loss: 0.4460\n",
      "Epoch: 17/20... Training loss: 0.4402\n",
      "Epoch: 17/20... Training loss: 0.4342\n",
      "Epoch: 17/20... Training loss: 0.4354\n",
      "Epoch: 17/20... Training loss: 0.4410\n",
      "Epoch: 17/20... Training loss: 0.4449\n",
      "Epoch: 17/20... Training loss: 0.4369\n",
      "Epoch: 17/20... Training loss: 0.4346\n",
      "Epoch: 17/20... Training loss: 0.4324\n",
      "Epoch: 17/20... Training loss: 0.4459\n",
      "Epoch: 17/20... Training loss: 0.4498\n",
      "Epoch: 17/20... Training loss: 0.4401\n",
      "Epoch: 17/20... Training loss: 0.4485\n",
      "Epoch: 17/20... Training loss: 0.4459\n",
      "Epoch: 17/20... Training loss: 0.4351\n",
      "Epoch: 17/20... Training loss: 0.4486\n",
      "Epoch: 17/20... Training loss: 0.4397\n",
      "Epoch: 17/20... Training loss: 0.4472\n",
      "Epoch: 17/20... Training loss: 0.4375\n",
      "Epoch: 17/20... Training loss: 0.4401\n",
      "Epoch: 17/20... Training loss: 0.4481\n",
      "Epoch: 17/20... Training loss: 0.4530\n",
      "Epoch: 17/20... Training loss: 0.4347\n",
      "Epoch: 17/20... Training loss: 0.4514\n",
      "Epoch: 17/20... Training loss: 0.4364\n",
      "Epoch: 17/20... Training loss: 0.4332\n",
      "Epoch: 17/20... Training loss: 0.4386\n",
      "Epoch: 17/20... Training loss: 0.4414\n",
      "Epoch: 17/20... Training loss: 0.4360\n",
      "Epoch: 17/20... Training loss: 0.4450\n",
      "Epoch: 17/20... Training loss: 0.4379\n",
      "Epoch: 17/20... Training loss: 0.4288\n",
      "Epoch: 17/20... Training loss: 0.4426\n",
      "Epoch: 17/20... Training loss: 0.4396\n",
      "Epoch: 17/20... Training loss: 0.4536\n",
      "Epoch: 17/20... Training loss: 0.4365\n",
      "Epoch: 17/20... Training loss: 0.4449\n",
      "Epoch: 17/20... Training loss: 0.4378\n",
      "Epoch: 17/20... Training loss: 0.4325\n",
      "Epoch: 17/20... Training loss: 0.4365\n",
      "Epoch: 17/20... Training loss: 0.4229\n",
      "Epoch: 17/20... Training loss: 0.4431\n",
      "Epoch: 17/20... Training loss: 0.4399\n",
      "Epoch: 17/20... Training loss: 0.4330\n",
      "Epoch: 17/20... Training loss: 0.4516\n",
      "Epoch: 17/20... Training loss: 0.4416\n",
      "Epoch: 17/20... Training loss: 0.4365\n",
      "Epoch: 17/20... Training loss: 0.4393\n",
      "Epoch: 17/20... Training loss: 0.4437\n",
      "Epoch: 17/20... Training loss: 0.4460\n",
      "Epoch: 17/20... Training loss: 0.4473\n",
      "Epoch: 17/20... Training loss: 0.4404\n",
      "Epoch: 17/20... Training loss: 0.4475\n",
      "Epoch: 17/20... Training loss: 0.4384\n",
      "Epoch: 17/20... Training loss: 0.4381\n",
      "Epoch: 17/20... Training loss: 0.4440\n",
      "Epoch: 17/20... Training loss: 0.4414\n",
      "Epoch: 17/20... Training loss: 0.4356\n",
      "Epoch: 17/20... Training loss: 0.4370\n",
      "Epoch: 17/20... Training loss: 0.4299\n",
      "Epoch: 17/20... Training loss: 0.4497\n",
      "Epoch: 17/20... Training loss: 0.4375\n",
      "Epoch: 17/20... Training loss: 0.4375\n",
      "Epoch: 17/20... Training loss: 0.4406\n",
      "Epoch: 17/20... Training loss: 0.4384\n",
      "Epoch: 17/20... Training loss: 0.4432\n",
      "Epoch: 17/20... Training loss: 0.4472\n",
      "Epoch: 17/20... Training loss: 0.4544\n",
      "Epoch: 17/20... Training loss: 0.4354\n",
      "Epoch: 17/20... Training loss: 0.4474\n",
      "Epoch: 17/20... Training loss: 0.4537\n",
      "Epoch: 17/20... Training loss: 0.4383\n",
      "Epoch: 17/20... Training loss: 0.4412\n",
      "Epoch: 17/20... Training loss: 0.4327\n",
      "Epoch: 17/20... Training loss: 0.4382\n",
      "Epoch: 17/20... Training loss: 0.4357\n",
      "Epoch: 17/20... Training loss: 0.4442\n",
      "Epoch: 17/20... Training loss: 0.4398\n",
      "Epoch: 17/20... Training loss: 0.4378\n",
      "Epoch: 17/20... Training loss: 0.4446\n",
      "Epoch: 17/20... Training loss: 0.4348\n",
      "Epoch: 17/20... Training loss: 0.4365\n",
      "Epoch: 17/20... Training loss: 0.4413\n",
      "Epoch: 17/20... Training loss: 0.4375\n",
      "Epoch: 17/20... Training loss: 0.4310\n",
      "Epoch: 17/20... Training loss: 0.4490\n",
      "Epoch: 17/20... Training loss: 0.4441\n",
      "Epoch: 17/20... Training loss: 0.4346\n",
      "Epoch: 17/20... Training loss: 0.4367\n",
      "Epoch: 17/20... Training loss: 0.4434\n",
      "Epoch: 17/20... Training loss: 0.4404\n",
      "Epoch: 17/20... Training loss: 0.4455\n",
      "Epoch: 17/20... Training loss: 0.4470\n",
      "Epoch: 17/20... Training loss: 0.4434\n",
      "Epoch: 17/20... Training loss: 0.4395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20... Training loss: 0.4404\n",
      "Epoch: 17/20... Training loss: 0.4507\n",
      "Epoch: 17/20... Training loss: 0.4433\n",
      "Epoch: 17/20... Training loss: 0.4437\n",
      "Epoch: 17/20... Training loss: 0.4413\n",
      "Epoch: 17/20... Training loss: 0.4450\n",
      "Epoch: 17/20... Training loss: 0.4430\n",
      "Epoch: 17/20... Training loss: 0.4362\n",
      "Epoch: 17/20... Training loss: 0.4466\n",
      "Epoch: 17/20... Training loss: 0.4350\n",
      "Epoch: 17/20... Training loss: 0.4443\n",
      "Epoch: 17/20... Training loss: 0.4413\n",
      "Epoch: 17/20... Training loss: 0.4441\n",
      "Epoch: 17/20... Training loss: 0.4511\n",
      "Epoch: 17/20... Training loss: 0.4336\n",
      "Epoch: 17/20... Training loss: 0.4505\n",
      "Epoch: 17/20... Training loss: 0.4359\n",
      "Epoch: 17/20... Training loss: 0.4418\n",
      "Epoch: 17/20... Training loss: 0.4378\n",
      "Epoch: 17/20... Training loss: 0.4381\n",
      "Epoch: 17/20... Training loss: 0.4471\n",
      "Epoch: 17/20... Training loss: 0.4417\n",
      "Epoch: 18/20... Training loss: 0.4486\n",
      "Epoch: 18/20... Training loss: 0.4475\n",
      "Epoch: 18/20... Training loss: 0.4488\n",
      "Epoch: 18/20... Training loss: 0.4411\n",
      "Epoch: 18/20... Training loss: 0.4391\n",
      "Epoch: 18/20... Training loss: 0.4390\n",
      "Epoch: 18/20... Training loss: 0.4433\n",
      "Epoch: 18/20... Training loss: 0.4419\n",
      "Epoch: 18/20... Training loss: 0.4363\n",
      "Epoch: 18/20... Training loss: 0.4438\n",
      "Epoch: 18/20... Training loss: 0.4430\n",
      "Epoch: 18/20... Training loss: 0.4424\n",
      "Epoch: 18/20... Training loss: 0.4433\n",
      "Epoch: 18/20... Training loss: 0.4276\n",
      "Epoch: 18/20... Training loss: 0.4461\n",
      "Epoch: 18/20... Training loss: 0.4412\n",
      "Epoch: 18/20... Training loss: 0.4411\n",
      "Epoch: 18/20... Training loss: 0.4349\n",
      "Epoch: 18/20... Training loss: 0.4382\n",
      "Epoch: 18/20... Training loss: 0.4363\n",
      "Epoch: 18/20... Training loss: 0.4504\n",
      "Epoch: 18/20... Training loss: 0.4438\n",
      "Epoch: 18/20... Training loss: 0.4421\n",
      "Epoch: 18/20... Training loss: 0.4467\n",
      "Epoch: 18/20... Training loss: 0.4485\n",
      "Epoch: 18/20... Training loss: 0.4511\n",
      "Epoch: 18/20... Training loss: 0.4414\n",
      "Epoch: 18/20... Training loss: 0.4351\n",
      "Epoch: 18/20... Training loss: 0.4345\n",
      "Epoch: 18/20... Training loss: 0.4386\n",
      "Epoch: 18/20... Training loss: 0.4420\n",
      "Epoch: 18/20... Training loss: 0.4608\n",
      "Epoch: 18/20... Training loss: 0.4472\n",
      "Epoch: 18/20... Training loss: 0.4387\n",
      "Epoch: 18/20... Training loss: 0.4454\n",
      "Epoch: 18/20... Training loss: 0.4363\n",
      "Epoch: 18/20... Training loss: 0.4502\n",
      "Epoch: 18/20... Training loss: 0.4348\n",
      "Epoch: 18/20... Training loss: 0.4338\n",
      "Epoch: 18/20... Training loss: 0.4386\n",
      "Epoch: 18/20... Training loss: 0.4433\n",
      "Epoch: 18/20... Training loss: 0.4413\n",
      "Epoch: 18/20... Training loss: 0.4419\n",
      "Epoch: 18/20... Training loss: 0.4359\n",
      "Epoch: 18/20... Training loss: 0.4457\n",
      "Epoch: 18/20... Training loss: 0.4361\n",
      "Epoch: 18/20... Training loss: 0.4401\n",
      "Epoch: 18/20... Training loss: 0.4448\n",
      "Epoch: 18/20... Training loss: 0.4329\n",
      "Epoch: 18/20... Training loss: 0.4415\n",
      "Epoch: 18/20... Training loss: 0.4483\n",
      "Epoch: 18/20... Training loss: 0.4420\n",
      "Epoch: 18/20... Training loss: 0.4454\n",
      "Epoch: 18/20... Training loss: 0.4334\n",
      "Epoch: 18/20... Training loss: 0.4446\n",
      "Epoch: 18/20... Training loss: 0.4419\n",
      "Epoch: 18/20... Training loss: 0.4424\n",
      "Epoch: 18/20... Training loss: 0.4365\n",
      "Epoch: 18/20... Training loss: 0.4407\n",
      "Epoch: 18/20... Training loss: 0.4418\n",
      "Epoch: 18/20... Training loss: 0.4435\n",
      "Epoch: 18/20... Training loss: 0.4471\n",
      "Epoch: 18/20... Training loss: 0.4353\n",
      "Epoch: 18/20... Training loss: 0.4382\n",
      "Epoch: 18/20... Training loss: 0.4532\n",
      "Epoch: 18/20... Training loss: 0.4472\n",
      "Epoch: 18/20... Training loss: 0.4394\n",
      "Epoch: 18/20... Training loss: 0.4455\n",
      "Epoch: 18/20... Training loss: 0.4426\n",
      "Epoch: 18/20... Training loss: 0.4469\n",
      "Epoch: 18/20... Training loss: 0.4513\n",
      "Epoch: 18/20... Training loss: 0.4381\n",
      "Epoch: 18/20... Training loss: 0.4385\n",
      "Epoch: 18/20... Training loss: 0.4383\n",
      "Epoch: 18/20... Training loss: 0.4393\n",
      "Epoch: 18/20... Training loss: 0.4382\n",
      "Epoch: 18/20... Training loss: 0.4437\n",
      "Epoch: 18/20... Training loss: 0.4361\n",
      "Epoch: 18/20... Training loss: 0.4480\n",
      "Epoch: 18/20... Training loss: 0.4314\n",
      "Epoch: 18/20... Training loss: 0.4486\n",
      "Epoch: 18/20... Training loss: 0.4443\n",
      "Epoch: 18/20... Training loss: 0.4451\n",
      "Epoch: 18/20... Training loss: 0.4418\n",
      "Epoch: 18/20... Training loss: 0.4370\n",
      "Epoch: 18/20... Training loss: 0.4526\n",
      "Epoch: 18/20... Training loss: 0.4465\n",
      "Epoch: 18/20... Training loss: 0.4365\n",
      "Epoch: 18/20... Training loss: 0.4377\n",
      "Epoch: 18/20... Training loss: 0.4354\n",
      "Epoch: 18/20... Training loss: 0.4352\n",
      "Epoch: 18/20... Training loss: 0.4296\n",
      "Epoch: 18/20... Training loss: 0.4404\n",
      "Epoch: 18/20... Training loss: 0.4402\n",
      "Epoch: 18/20... Training loss: 0.4359\n",
      "Epoch: 18/20... Training loss: 0.4271\n",
      "Epoch: 18/20... Training loss: 0.4414\n",
      "Epoch: 18/20... Training loss: 0.4475\n",
      "Epoch: 18/20... Training loss: 0.4487\n",
      "Epoch: 18/20... Training loss: 0.4414\n",
      "Epoch: 18/20... Training loss: 0.4386\n",
      "Epoch: 18/20... Training loss: 0.4447\n",
      "Epoch: 18/20... Training loss: 0.4363\n",
      "Epoch: 18/20... Training loss: 0.4465\n",
      "Epoch: 18/20... Training loss: 0.4435\n",
      "Epoch: 18/20... Training loss: 0.4377\n",
      "Epoch: 18/20... Training loss: 0.4359\n",
      "Epoch: 18/20... Training loss: 0.4581\n",
      "Epoch: 18/20... Training loss: 0.4478\n",
      "Epoch: 18/20... Training loss: 0.4505\n",
      "Epoch: 18/20... Training loss: 0.4381\n",
      "Epoch: 18/20... Training loss: 0.4428\n",
      "Epoch: 18/20... Training loss: 0.4402\n",
      "Epoch: 18/20... Training loss: 0.4444\n",
      "Epoch: 18/20... Training loss: 0.4344\n",
      "Epoch: 18/20... Training loss: 0.4414\n",
      "Epoch: 18/20... Training loss: 0.4408\n",
      "Epoch: 18/20... Training loss: 0.4462\n",
      "Epoch: 18/20... Training loss: 0.4399\n",
      "Epoch: 18/20... Training loss: 0.4345\n",
      "Epoch: 18/20... Training loss: 0.4270\n",
      "Epoch: 18/20... Training loss: 0.4351\n",
      "Epoch: 18/20... Training loss: 0.4380\n",
      "Epoch: 18/20... Training loss: 0.4444\n",
      "Epoch: 18/20... Training loss: 0.4458\n",
      "Epoch: 18/20... Training loss: 0.4431\n",
      "Epoch: 18/20... Training loss: 0.4410\n",
      "Epoch: 18/20... Training loss: 0.4514\n",
      "Epoch: 18/20... Training loss: 0.4452\n",
      "Epoch: 18/20... Training loss: 0.4331\n",
      "Epoch: 18/20... Training loss: 0.4408\n",
      "Epoch: 18/20... Training loss: 0.4402\n",
      "Epoch: 18/20... Training loss: 0.4441\n",
      "Epoch: 18/20... Training loss: 0.4377\n",
      "Epoch: 18/20... Training loss: 0.4425\n",
      "Epoch: 18/20... Training loss: 0.4488\n",
      "Epoch: 18/20... Training loss: 0.4388\n",
      "Epoch: 18/20... Training loss: 0.4370\n",
      "Epoch: 18/20... Training loss: 0.4313\n",
      "Epoch: 18/20... Training loss: 0.4418\n",
      "Epoch: 18/20... Training loss: 0.4378\n",
      "Epoch: 18/20... Training loss: 0.4371\n",
      "Epoch: 18/20... Training loss: 0.4360\n",
      "Epoch: 18/20... Training loss: 0.4344\n",
      "Epoch: 18/20... Training loss: 0.4402\n",
      "Epoch: 18/20... Training loss: 0.4418\n",
      "Epoch: 18/20... Training loss: 0.4451\n",
      "Epoch: 18/20... Training loss: 0.4444\n",
      "Epoch: 18/20... Training loss: 0.4512\n",
      "Epoch: 18/20... Training loss: 0.4426\n",
      "Epoch: 18/20... Training loss: 0.4353\n",
      "Epoch: 18/20... Training loss: 0.4483\n",
      "Epoch: 18/20... Training loss: 0.4320\n",
      "Epoch: 18/20... Training loss: 0.4478\n",
      "Epoch: 18/20... Training loss: 0.4397\n",
      "Epoch: 18/20... Training loss: 0.4290\n",
      "Epoch: 18/20... Training loss: 0.4383\n",
      "Epoch: 18/20... Training loss: 0.4426\n",
      "Epoch: 18/20... Training loss: 0.4421\n",
      "Epoch: 18/20... Training loss: 0.4431\n",
      "Epoch: 18/20... Training loss: 0.4333\n",
      "Epoch: 18/20... Training loss: 0.4524\n",
      "Epoch: 18/20... Training loss: 0.4440\n",
      "Epoch: 18/20... Training loss: 0.4396\n",
      "Epoch: 18/20... Training loss: 0.4455\n",
      "Epoch: 18/20... Training loss: 0.4396\n",
      "Epoch: 18/20... Training loss: 0.4317\n",
      "Epoch: 18/20... Training loss: 0.4462\n",
      "Epoch: 18/20... Training loss: 0.4318\n",
      "Epoch: 18/20... Training loss: 0.4414\n",
      "Epoch: 18/20... Training loss: 0.4427\n",
      "Epoch: 18/20... Training loss: 0.4460\n",
      "Epoch: 18/20... Training loss: 0.4398\n",
      "Epoch: 18/20... Training loss: 0.4483\n",
      "Epoch: 18/20... Training loss: 0.4247\n",
      "Epoch: 18/20... Training loss: 0.4474\n",
      "Epoch: 18/20... Training loss: 0.4292\n",
      "Epoch: 18/20... Training loss: 0.4370\n",
      "Epoch: 18/20... Training loss: 0.4480\n",
      "Epoch: 18/20... Training loss: 0.4434\n",
      "Epoch: 18/20... Training loss: 0.4349\n",
      "Epoch: 18/20... Training loss: 0.4506\n",
      "Epoch: 18/20... Training loss: 0.4404\n",
      "Epoch: 19/20... Training loss: 0.4353\n",
      "Epoch: 19/20... Training loss: 0.4499\n",
      "Epoch: 19/20... Training loss: 0.4365\n",
      "Epoch: 19/20... Training loss: 0.4319\n",
      "Epoch: 19/20... Training loss: 0.4377\n",
      "Epoch: 19/20... Training loss: 0.4356\n",
      "Epoch: 19/20... Training loss: 0.4419\n",
      "Epoch: 19/20... Training loss: 0.4532\n",
      "Epoch: 19/20... Training loss: 0.4392\n",
      "Epoch: 19/20... Training loss: 0.4536\n",
      "Epoch: 19/20... Training loss: 0.4457\n",
      "Epoch: 19/20... Training loss: 0.4369\n",
      "Epoch: 19/20... Training loss: 0.4413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20... Training loss: 0.4436\n",
      "Epoch: 19/20... Training loss: 0.4432\n",
      "Epoch: 19/20... Training loss: 0.4455\n",
      "Epoch: 19/20... Training loss: 0.4489\n",
      "Epoch: 19/20... Training loss: 0.4427\n",
      "Epoch: 19/20... Training loss: 0.4322\n",
      "Epoch: 19/20... Training loss: 0.4337\n",
      "Epoch: 19/20... Training loss: 0.4437\n",
      "Epoch: 19/20... Training loss: 0.4402\n",
      "Epoch: 19/20... Training loss: 0.4295\n",
      "Epoch: 19/20... Training loss: 0.4468\n",
      "Epoch: 19/20... Training loss: 0.4481\n",
      "Epoch: 19/20... Training loss: 0.4434\n",
      "Epoch: 19/20... Training loss: 0.4433\n",
      "Epoch: 19/20... Training loss: 0.4398\n",
      "Epoch: 19/20... Training loss: 0.4342\n",
      "Epoch: 19/20... Training loss: 0.4482\n",
      "Epoch: 19/20... Training loss: 0.4331\n",
      "Epoch: 19/20... Training loss: 0.4365\n",
      "Epoch: 19/20... Training loss: 0.4386\n",
      "Epoch: 19/20... Training loss: 0.4397\n",
      "Epoch: 19/20... Training loss: 0.4428\n",
      "Epoch: 19/20... Training loss: 0.4416\n",
      "Epoch: 19/20... Training loss: 0.4385\n",
      "Epoch: 19/20... Training loss: 0.4447\n",
      "Epoch: 19/20... Training loss: 0.4462\n",
      "Epoch: 19/20... Training loss: 0.4606\n",
      "Epoch: 19/20... Training loss: 0.4423\n",
      "Epoch: 19/20... Training loss: 0.4403\n",
      "Epoch: 19/20... Training loss: 0.4435\n",
      "Epoch: 19/20... Training loss: 0.4303\n",
      "Epoch: 19/20... Training loss: 0.4518\n",
      "Epoch: 19/20... Training loss: 0.4449\n",
      "Epoch: 19/20... Training loss: 0.4500\n",
      "Epoch: 19/20... Training loss: 0.4530\n",
      "Epoch: 19/20... Training loss: 0.4347\n",
      "Epoch: 19/20... Training loss: 0.4410\n",
      "Epoch: 19/20... Training loss: 0.4353\n",
      "Epoch: 19/20... Training loss: 0.4428\n",
      "Epoch: 19/20... Training loss: 0.4428\n",
      "Epoch: 19/20... Training loss: 0.4535\n",
      "Epoch: 19/20... Training loss: 0.4373\n",
      "Epoch: 19/20... Training loss: 0.4431\n",
      "Epoch: 19/20... Training loss: 0.4364\n",
      "Epoch: 19/20... Training loss: 0.4402\n",
      "Epoch: 19/20... Training loss: 0.4388\n",
      "Epoch: 19/20... Training loss: 0.4538\n",
      "Epoch: 19/20... Training loss: 0.4365\n",
      "Epoch: 19/20... Training loss: 0.4311\n",
      "Epoch: 19/20... Training loss: 0.4336\n",
      "Epoch: 19/20... Training loss: 0.4404\n",
      "Epoch: 19/20... Training loss: 0.4351\n",
      "Epoch: 19/20... Training loss: 0.4357\n",
      "Epoch: 19/20... Training loss: 0.4443\n",
      "Epoch: 19/20... Training loss: 0.4399\n",
      "Epoch: 19/20... Training loss: 0.4392\n",
      "Epoch: 19/20... Training loss: 0.4334\n",
      "Epoch: 19/20... Training loss: 0.4443\n",
      "Epoch: 19/20... Training loss: 0.4289\n",
      "Epoch: 19/20... Training loss: 0.4489\n",
      "Epoch: 19/20... Training loss: 0.4385\n",
      "Epoch: 19/20... Training loss: 0.4395\n",
      "Epoch: 19/20... Training loss: 0.4355\n",
      "Epoch: 19/20... Training loss: 0.4486\n",
      "Epoch: 19/20... Training loss: 0.4459\n",
      "Epoch: 19/20... Training loss: 0.4389\n",
      "Epoch: 19/20... Training loss: 0.4390\n",
      "Epoch: 19/20... Training loss: 0.4286\n",
      "Epoch: 19/20... Training loss: 0.4373\n",
      "Epoch: 19/20... Training loss: 0.4385\n",
      "Epoch: 19/20... Training loss: 0.4409\n",
      "Epoch: 19/20... Training loss: 0.4461\n",
      "Epoch: 19/20... Training loss: 0.4300\n",
      "Epoch: 19/20... Training loss: 0.4390\n",
      "Epoch: 19/20... Training loss: 0.4481\n",
      "Epoch: 19/20... Training loss: 0.4358\n",
      "Epoch: 19/20... Training loss: 0.4333\n",
      "Epoch: 19/20... Training loss: 0.4377\n",
      "Epoch: 19/20... Training loss: 0.4343\n",
      "Epoch: 19/20... Training loss: 0.4403\n",
      "Epoch: 19/20... Training loss: 0.4396\n",
      "Epoch: 19/20... Training loss: 0.4440\n",
      "Epoch: 19/20... Training loss: 0.4401\n",
      "Epoch: 19/20... Training loss: 0.4385\n",
      "Epoch: 19/20... Training loss: 0.4467\n",
      "Epoch: 19/20... Training loss: 0.4366\n",
      "Epoch: 19/20... Training loss: 0.4344\n",
      "Epoch: 19/20... Training loss: 0.4398\n",
      "Epoch: 19/20... Training loss: 0.4436\n",
      "Epoch: 19/20... Training loss: 0.4413\n",
      "Epoch: 19/20... Training loss: 0.4447\n",
      "Epoch: 19/20... Training loss: 0.4466\n",
      "Epoch: 19/20... Training loss: 0.4482\n",
      "Epoch: 19/20... Training loss: 0.4289\n",
      "Epoch: 19/20... Training loss: 0.4343\n",
      "Epoch: 19/20... Training loss: 0.4416\n",
      "Epoch: 19/20... Training loss: 0.4497\n",
      "Epoch: 19/20... Training loss: 0.4419\n",
      "Epoch: 19/20... Training loss: 0.4344\n",
      "Epoch: 19/20... Training loss: 0.4403\n",
      "Epoch: 19/20... Training loss: 0.4541\n",
      "Epoch: 19/20... Training loss: 0.4339\n",
      "Epoch: 19/20... Training loss: 0.4404\n",
      "Epoch: 19/20... Training loss: 0.4465\n",
      "Epoch: 19/20... Training loss: 0.4366\n",
      "Epoch: 19/20... Training loss: 0.4395\n",
      "Epoch: 19/20... Training loss: 0.4474\n",
      "Epoch: 19/20... Training loss: 0.4379\n",
      "Epoch: 19/20... Training loss: 0.4446\n",
      "Epoch: 19/20... Training loss: 0.4372\n",
      "Epoch: 19/20... Training loss: 0.4460\n",
      "Epoch: 19/20... Training loss: 0.4410\n",
      "Epoch: 19/20... Training loss: 0.4425\n",
      "Epoch: 19/20... Training loss: 0.4417\n",
      "Epoch: 19/20... Training loss: 0.4442\n",
      "Epoch: 19/20... Training loss: 0.4403\n",
      "Epoch: 19/20... Training loss: 0.4493\n",
      "Epoch: 19/20... Training loss: 0.4361\n",
      "Epoch: 19/20... Training loss: 0.4310\n",
      "Epoch: 19/20... Training loss: 0.4354\n",
      "Epoch: 19/20... Training loss: 0.4501\n",
      "Epoch: 19/20... Training loss: 0.4313\n",
      "Epoch: 19/20... Training loss: 0.4457\n",
      "Epoch: 19/20... Training loss: 0.4399\n",
      "Epoch: 19/20... Training loss: 0.4412\n",
      "Epoch: 19/20... Training loss: 0.4326\n",
      "Epoch: 19/20... Training loss: 0.4392\n",
      "Epoch: 19/20... Training loss: 0.4402\n",
      "Epoch: 19/20... Training loss: 0.4412\n",
      "Epoch: 19/20... Training loss: 0.4412\n",
      "Epoch: 19/20... Training loss: 0.4380\n",
      "Epoch: 19/20... Training loss: 0.4523\n",
      "Epoch: 19/20... Training loss: 0.4378\n",
      "Epoch: 19/20... Training loss: 0.4430\n",
      "Epoch: 19/20... Training loss: 0.4414\n",
      "Epoch: 19/20... Training loss: 0.4266\n",
      "Epoch: 19/20... Training loss: 0.4447\n",
      "Epoch: 19/20... Training loss: 0.4430\n",
      "Epoch: 19/20... Training loss: 0.4422\n",
      "Epoch: 19/20... Training loss: 0.4485\n",
      "Epoch: 19/20... Training loss: 0.4447\n",
      "Epoch: 19/20... Training loss: 0.4437\n",
      "Epoch: 19/20... Training loss: 0.4372\n",
      "Epoch: 19/20... Training loss: 0.4370\n",
      "Epoch: 19/20... Training loss: 0.4409\n",
      "Epoch: 19/20... Training loss: 0.4436\n",
      "Epoch: 19/20... Training loss: 0.4381\n",
      "Epoch: 19/20... Training loss: 0.4346\n",
      "Epoch: 19/20... Training loss: 0.4419\n",
      "Epoch: 19/20... Training loss: 0.4477\n",
      "Epoch: 19/20... Training loss: 0.4411\n",
      "Epoch: 19/20... Training loss: 0.4418\n",
      "Epoch: 19/20... Training loss: 0.4454\n",
      "Epoch: 19/20... Training loss: 0.4465\n",
      "Epoch: 19/20... Training loss: 0.4418\n",
      "Epoch: 19/20... Training loss: 0.4389\n",
      "Epoch: 19/20... Training loss: 0.4295\n",
      "Epoch: 19/20... Training loss: 0.4352\n",
      "Epoch: 19/20... Training loss: 0.4411\n",
      "Epoch: 19/20... Training loss: 0.4452\n",
      "Epoch: 19/20... Training loss: 0.4421\n",
      "Epoch: 19/20... Training loss: 0.4332\n",
      "Epoch: 19/20... Training loss: 0.4403\n",
      "Epoch: 19/20... Training loss: 0.4398\n",
      "Epoch: 19/20... Training loss: 0.4456\n",
      "Epoch: 19/20... Training loss: 0.4331\n",
      "Epoch: 19/20... Training loss: 0.4385\n",
      "Epoch: 19/20... Training loss: 0.4387\n",
      "Epoch: 19/20... Training loss: 0.4467\n",
      "Epoch: 19/20... Training loss: 0.4341\n",
      "Epoch: 20/20... Training loss: 0.4434\n",
      "Epoch: 20/20... Training loss: 0.4333\n",
      "Epoch: 20/20... Training loss: 0.4379\n",
      "Epoch: 20/20... Training loss: 0.4380\n",
      "Epoch: 20/20... Training loss: 0.4392\n",
      "Epoch: 20/20... Training loss: 0.4487\n",
      "Epoch: 20/20... Training loss: 0.4441\n",
      "Epoch: 20/20... Training loss: 0.4472\n",
      "Epoch: 20/20... Training loss: 0.4445\n",
      "Epoch: 20/20... Training loss: 0.4517\n",
      "Epoch: 20/20... Training loss: 0.4355\n",
      "Epoch: 20/20... Training loss: 0.4397\n",
      "Epoch: 20/20... Training loss: 0.4478\n",
      "Epoch: 20/20... Training loss: 0.4430\n",
      "Epoch: 20/20... Training loss: 0.4383\n",
      "Epoch: 20/20... Training loss: 0.4470\n",
      "Epoch: 20/20... Training loss: 0.4364\n",
      "Epoch: 20/20... Training loss: 0.4368\n",
      "Epoch: 20/20... Training loss: 0.4353\n",
      "Epoch: 20/20... Training loss: 0.4420\n",
      "Epoch: 20/20... Training loss: 0.4359\n",
      "Epoch: 20/20... Training loss: 0.4426\n",
      "Epoch: 20/20... Training loss: 0.4345\n",
      "Epoch: 20/20... Training loss: 0.4382\n",
      "Epoch: 20/20... Training loss: 0.4336\n",
      "Epoch: 20/20... Training loss: 0.4236\n",
      "Epoch: 20/20... Training loss: 0.4457\n",
      "Epoch: 20/20... Training loss: 0.4375\n",
      "Epoch: 20/20... Training loss: 0.4392\n",
      "Epoch: 20/20... Training loss: 0.4370\n",
      "Epoch: 20/20... Training loss: 0.4360\n",
      "Epoch: 20/20... Training loss: 0.4399\n",
      "Epoch: 20/20... Training loss: 0.4370\n",
      "Epoch: 20/20... Training loss: 0.4397\n",
      "Epoch: 20/20... Training loss: 0.4369\n",
      "Epoch: 20/20... Training loss: 0.4421\n",
      "Epoch: 20/20... Training loss: 0.4399\n",
      "Epoch: 20/20... Training loss: 0.4405\n",
      "Epoch: 20/20... Training loss: 0.4458\n",
      "Epoch: 20/20... Training loss: 0.4388\n",
      "Epoch: 20/20... Training loss: 0.4362\n",
      "Epoch: 20/20... Training loss: 0.4348\n",
      "Epoch: 20/20... Training loss: 0.4443\n",
      "Epoch: 20/20... Training loss: 0.4406\n",
      "Epoch: 20/20... Training loss: 0.4413\n",
      "Epoch: 20/20... Training loss: 0.4436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Training loss: 0.4437\n",
      "Epoch: 20/20... Training loss: 0.4393\n",
      "Epoch: 20/20... Training loss: 0.4479\n",
      "Epoch: 20/20... Training loss: 0.4420\n",
      "Epoch: 20/20... Training loss: 0.4456\n",
      "Epoch: 20/20... Training loss: 0.4431\n",
      "Epoch: 20/20... Training loss: 0.4452\n",
      "Epoch: 20/20... Training loss: 0.4409\n",
      "Epoch: 20/20... Training loss: 0.4404\n",
      "Epoch: 20/20... Training loss: 0.4344\n",
      "Epoch: 20/20... Training loss: 0.4446\n",
      "Epoch: 20/20... Training loss: 0.4459\n",
      "Epoch: 20/20... Training loss: 0.4394\n",
      "Epoch: 20/20... Training loss: 0.4446\n",
      "Epoch: 20/20... Training loss: 0.4406\n",
      "Epoch: 20/20... Training loss: 0.4470\n",
      "Epoch: 20/20... Training loss: 0.4451\n",
      "Epoch: 20/20... Training loss: 0.4436\n",
      "Epoch: 20/20... Training loss: 0.4452\n",
      "Epoch: 20/20... Training loss: 0.4370\n",
      "Epoch: 20/20... Training loss: 0.4430\n",
      "Epoch: 20/20... Training loss: 0.4346\n",
      "Epoch: 20/20... Training loss: 0.4392\n",
      "Epoch: 20/20... Training loss: 0.4448\n",
      "Epoch: 20/20... Training loss: 0.4465\n",
      "Epoch: 20/20... Training loss: 0.4377\n",
      "Epoch: 20/20... Training loss: 0.4422\n",
      "Epoch: 20/20... Training loss: 0.4434\n",
      "Epoch: 20/20... Training loss: 0.4417\n",
      "Epoch: 20/20... Training loss: 0.4503\n",
      "Epoch: 20/20... Training loss: 0.4425\n",
      "Epoch: 20/20... Training loss: 0.4451\n",
      "Epoch: 20/20... Training loss: 0.4412\n",
      "Epoch: 20/20... Training loss: 0.4441\n",
      "Epoch: 20/20... Training loss: 0.4524\n",
      "Epoch: 20/20... Training loss: 0.4375\n",
      "Epoch: 20/20... Training loss: 0.4345\n",
      "Epoch: 20/20... Training loss: 0.4418\n",
      "Epoch: 20/20... Training loss: 0.4416\n",
      "Epoch: 20/20... Training loss: 0.4483\n",
      "Epoch: 20/20... Training loss: 0.4449\n",
      "Epoch: 20/20... Training loss: 0.4313\n",
      "Epoch: 20/20... Training loss: 0.4447\n",
      "Epoch: 20/20... Training loss: 0.4331\n",
      "Epoch: 20/20... Training loss: 0.4398\n",
      "Epoch: 20/20... Training loss: 0.4490\n",
      "Epoch: 20/20... Training loss: 0.4441\n",
      "Epoch: 20/20... Training loss: 0.4405\n",
      "Epoch: 20/20... Training loss: 0.4504\n",
      "Epoch: 20/20... Training loss: 0.4392\n",
      "Epoch: 20/20... Training loss: 0.4354\n",
      "Epoch: 20/20... Training loss: 0.4310\n",
      "Epoch: 20/20... Training loss: 0.4400\n",
      "Epoch: 20/20... Training loss: 0.4419\n",
      "Epoch: 20/20... Training loss: 0.4409\n",
      "Epoch: 20/20... Training loss: 0.4434\n",
      "Epoch: 20/20... Training loss: 0.4462\n",
      "Epoch: 20/20... Training loss: 0.4365\n",
      "Epoch: 20/20... Training loss: 0.4358\n",
      "Epoch: 20/20... Training loss: 0.4385\n",
      "Epoch: 20/20... Training loss: 0.4333\n",
      "Epoch: 20/20... Training loss: 0.4493\n",
      "Epoch: 20/20... Training loss: 0.4370\n",
      "Epoch: 20/20... Training loss: 0.4306\n",
      "Epoch: 20/20... Training loss: 0.4380\n",
      "Epoch: 20/20... Training loss: 0.4429\n",
      "Epoch: 20/20... Training loss: 0.4504\n",
      "Epoch: 20/20... Training loss: 0.4381\n",
      "Epoch: 20/20... Training loss: 0.4399\n",
      "Epoch: 20/20... Training loss: 0.4435\n",
      "Epoch: 20/20... Training loss: 0.4434\n",
      "Epoch: 20/20... Training loss: 0.4366\n",
      "Epoch: 20/20... Training loss: 0.4469\n",
      "Epoch: 20/20... Training loss: 0.4413\n",
      "Epoch: 20/20... Training loss: 0.4451\n",
      "Epoch: 20/20... Training loss: 0.4460\n",
      "Epoch: 20/20... Training loss: 0.4380\n",
      "Epoch: 20/20... Training loss: 0.4463\n",
      "Epoch: 20/20... Training loss: 0.4420\n",
      "Epoch: 20/20... Training loss: 0.4446\n",
      "Epoch: 20/20... Training loss: 0.4472\n",
      "Epoch: 20/20... Training loss: 0.4336\n",
      "Epoch: 20/20... Training loss: 0.4373\n",
      "Epoch: 20/20... Training loss: 0.4462\n",
      "Epoch: 20/20... Training loss: 0.4511\n",
      "Epoch: 20/20... Training loss: 0.4470\n",
      "Epoch: 20/20... Training loss: 0.4427\n",
      "Epoch: 20/20... Training loss: 0.4340\n",
      "Epoch: 20/20... Training loss: 0.4318\n",
      "Epoch: 20/20... Training loss: 0.4388\n",
      "Epoch: 20/20... Training loss: 0.4543\n",
      "Epoch: 20/20... Training loss: 0.4298\n",
      "Epoch: 20/20... Training loss: 0.4402\n",
      "Epoch: 20/20... Training loss: 0.4340\n",
      "Epoch: 20/20... Training loss: 0.4405\n",
      "Epoch: 20/20... Training loss: 0.4444\n",
      "Epoch: 20/20... Training loss: 0.4437\n",
      "Epoch: 20/20... Training loss: 0.4340\n",
      "Epoch: 20/20... Training loss: 0.4410\n",
      "Epoch: 20/20... Training loss: 0.4478\n",
      "Epoch: 20/20... Training loss: 0.4502\n",
      "Epoch: 20/20... Training loss: 0.4355\n",
      "Epoch: 20/20... Training loss: 0.4374\n",
      "Epoch: 20/20... Training loss: 0.4402\n",
      "Epoch: 20/20... Training loss: 0.4366\n",
      "Epoch: 20/20... Training loss: 0.4315\n",
      "Epoch: 20/20... Training loss: 0.4415\n",
      "Epoch: 20/20... Training loss: 0.4406\n",
      "Epoch: 20/20... Training loss: 0.4407\n",
      "Epoch: 20/20... Training loss: 0.4410\n",
      "Epoch: 20/20... Training loss: 0.4460\n",
      "Epoch: 20/20... Training loss: 0.4456\n",
      "Epoch: 20/20... Training loss: 0.4431\n",
      "Epoch: 20/20... Training loss: 0.4372\n",
      "Epoch: 20/20... Training loss: 0.4419\n",
      "Epoch: 20/20... Training loss: 0.4503\n",
      "Epoch: 20/20... Training loss: 0.4389\n",
      "Epoch: 20/20... Training loss: 0.4420\n",
      "Epoch: 20/20... Training loss: 0.4349\n",
      "Epoch: 20/20... Training loss: 0.4304\n",
      "Epoch: 20/20... Training loss: 0.4429\n",
      "Epoch: 20/20... Training loss: 0.4305\n",
      "Epoch: 20/20... Training loss: 0.4431\n",
      "Epoch: 20/20... Training loss: 0.4322\n",
      "Epoch: 20/20... Training loss: 0.4364\n",
      "Epoch: 20/20... Training loss: 0.4419\n",
      "Epoch: 20/20... Training loss: 0.4558\n",
      "Epoch: 20/20... Training loss: 0.4316\n",
      "Epoch: 20/20... Training loss: 0.4460\n",
      "Epoch: 20/20... Training loss: 0.4422\n",
      "Epoch: 20/20... Training loss: 0.4364\n",
      "Epoch: 20/20... Training loss: 0.4401\n",
      "Epoch: 20/20... Training loss: 0.4473\n",
      "Epoch: 20/20... Training loss: 0.4514\n",
      "Epoch: 20/20... Training loss: 0.4423\n",
      "Epoch: 20/20... Training loss: 0.4306\n",
      "Epoch: 20/20... Training loss: 0.4338\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 300\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for e in range(epochs):\n",
    "    for ii in range(len(data.train.images) // batch_size): \n",
    "        batch = data.train.next_batch(batch_size)\n",
    "        \n",
    "        # https://stackoverflow.com/questions/41848660/why-the-negative-reshape-1-in-mnist-tutorial\n",
    "        imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "        batch_cost, _= sess.run([cost, opt], feed_dict={inputs: imgs, targets: imgs})\n",
    "        \n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Training loss: {:.4f}\".format(batch_cost))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABawAAAEsCAYAAAAvofT2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fed6c7a7240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "in_imgs = data.test.images[:10]\n",
    "\n",
    "reconstructed = sess.run(decoded, feed_dict={inputs: in_imgs.reshape((10, 28, 28, 1))})\n",
    "\n",
    "for images, row in zip([in_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
